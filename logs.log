2024-02-04 19:05:19,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:05:19,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:05:19,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:05:19,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:05:19,901:INFO:PyCaret RegressionExperiment
2024-02-04 19:05:19,901:INFO:Logging name: houseprice1
2024-02-04 19:05:19,901:INFO:ML Usecase: MLUsecase.REGRESSION
2024-02-04 19:05:19,901:INFO:version 3.2.0
2024-02-04 19:05:19,901:INFO:Initializing setup()
2024-02-04 19:05:19,901:INFO:self.USI: 027c
2024-02-04 19:05:19,901:INFO:self._variable_keys: {'X_test', 'idx', 'gpu_n_jobs_param', 'y_test', 'memory', 'y_train', 'fold_generator', 'exp_id', 'y', 'fold_shuffle_param', 'data', 'pipeline', 'logging_param', 'n_jobs_param', 'gpu_param', 'seed', 'X_train', 'X', 'html_param', '_available_plots', 'target_param', 'transform_target_param', 'USI', 'fold_groups_param', 'exp_name_log', '_ml_usecase', 'log_plots_param'}
2024-02-04 19:05:19,901:INFO:Checking environment
2024-02-04 19:05:19,901:INFO:python_version: 3.9.9
2024-02-04 19:05:19,901:INFO:python_build: ('tags/v3.9.9:ccb0e6a', 'Nov 15 2021 18:08:50')
2024-02-04 19:05:19,901:INFO:machine: AMD64
2024-02-04 19:05:19,901:INFO:platform: Windows-10-10.0.22621-SP0
2024-02-04 19:05:19,901:INFO:Memory: svmem(total=16856203264, available=7707590656, percent=54.3, used=9148612608, free=7707590656)
2024-02-04 19:05:19,901:INFO:Physical Core: 4
2024-02-04 19:05:19,901:INFO:Logical Core: 8
2024-02-04 19:05:19,901:INFO:Checking libraries
2024-02-04 19:05:19,901:INFO:System:
2024-02-04 19:05:19,901:INFO:    python: 3.9.9 (tags/v3.9.9:ccb0e6a, Nov 15 2021, 18:08:50) [MSC v.1929 64 bit (AMD64)]
2024-02-04 19:05:19,901:INFO:executable: c:\Users\joseg\AppData\Local\Programs\Python\Python39\python.exe
2024-02-04 19:05:19,901:INFO:   machine: Windows-10-10.0.22621-SP0
2024-02-04 19:05:19,901:INFO:PyCaret required dependencies:
2024-02-04 19:05:19,951:INFO:                 pip: 21.2.4
2024-02-04 19:05:19,951:INFO:          setuptools: 58.1.0
2024-02-04 19:05:19,951:INFO:             pycaret: 3.2.0
2024-02-04 19:05:19,951:INFO:             IPython: 8.18.1
2024-02-04 19:05:19,951:INFO:          ipywidgets: 8.1.1
2024-02-04 19:05:19,951:INFO:                tqdm: 4.66.1
2024-02-04 19:05:19,951:INFO:               numpy: 1.23.5
2024-02-04 19:05:19,951:INFO:              pandas: 1.5.3
2024-02-04 19:05:19,951:INFO:              jinja2: 3.1.3
2024-02-04 19:05:19,951:INFO:               scipy: 1.10.1
2024-02-04 19:05:19,951:INFO:              joblib: 1.3.2
2024-02-04 19:05:19,951:INFO:             sklearn: 1.2.2
2024-02-04 19:05:19,951:INFO:                pyod: 1.1.2
2024-02-04 19:05:19,951:INFO:            imblearn: 0.12.0
2024-02-04 19:05:19,951:INFO:   category_encoders: 2.6.3
2024-02-04 19:05:19,951:INFO:            lightgbm: 4.3.0
2024-02-04 19:05:19,951:INFO:               numba: 0.59.0
2024-02-04 19:05:19,951:INFO:            requests: 2.31.0
2024-02-04 19:05:19,951:INFO:          matplotlib: 3.6.0
2024-02-04 19:05:19,951:INFO:          scikitplot: 0.3.7
2024-02-04 19:05:19,951:INFO:         yellowbrick: 1.5
2024-02-04 19:05:19,951:INFO:              plotly: 5.18.0
2024-02-04 19:05:19,951:INFO:    plotly-resampler: Not installed
2024-02-04 19:05:19,951:INFO:             kaleido: 0.2.1
2024-02-04 19:05:19,951:INFO:           schemdraw: 0.15
2024-02-04 19:05:19,951:INFO:         statsmodels: 0.14.1
2024-02-04 19:05:19,951:INFO:              sktime: 0.21.1
2024-02-04 19:05:19,951:INFO:               tbats: 1.1.3
2024-02-04 19:05:19,951:INFO:            pmdarima: 2.0.4
2024-02-04 19:05:19,951:INFO:              psutil: 5.9.8
2024-02-04 19:05:19,951:INFO:          markupsafe: 2.1.5
2024-02-04 19:05:19,951:INFO:             pickle5: Not installed
2024-02-04 19:05:19,951:INFO:         cloudpickle: 3.0.0
2024-02-04 19:05:19,951:INFO:         deprecation: 2.1.0
2024-02-04 19:05:19,951:INFO:              xxhash: 3.4.1
2024-02-04 19:05:19,951:INFO:           wurlitzer: Not installed
2024-02-04 19:05:19,951:INFO:PyCaret optional dependencies:
2024-02-04 19:05:21,017:INFO:                shap: 0.44.1
2024-02-04 19:05:21,017:INFO:           interpret: Not installed
2024-02-04 19:05:21,017:INFO:                umap: Not installed
2024-02-04 19:05:21,017:INFO:     ydata_profiling: Not installed
2024-02-04 19:05:21,017:INFO:  explainerdashboard: 0.4.5
2024-02-04 19:05:21,017:INFO:             autoviz: Not installed
2024-02-04 19:05:21,017:INFO:           fairlearn: Not installed
2024-02-04 19:05:21,017:INFO:          deepchecks: Not installed
2024-02-04 19:05:21,017:INFO:             xgboost: Not installed
2024-02-04 19:05:21,017:INFO:            catboost: Not installed
2024-02-04 19:05:21,017:INFO:              kmodes: Not installed
2024-02-04 19:05:21,017:INFO:             mlxtend: Not installed
2024-02-04 19:05:21,017:INFO:       statsforecast: Not installed
2024-02-04 19:05:21,017:INFO:        tune_sklearn: Not installed
2024-02-04 19:05:21,017:INFO:                 ray: Not installed
2024-02-04 19:05:21,017:INFO:            hyperopt: Not installed
2024-02-04 19:05:21,017:INFO:              optuna: Not installed
2024-02-04 19:05:21,017:INFO:               skopt: Not installed
2024-02-04 19:05:21,017:INFO:              mlflow: 2.10.0
2024-02-04 19:05:21,017:INFO:              gradio: 3.50.0
2024-02-04 19:05:21,017:INFO:             fastapi: 0.109.1
2024-02-04 19:05:21,017:INFO:             uvicorn: 0.27.0.post1
2024-02-04 19:05:21,017:INFO:              m2cgen: Not installed
2024-02-04 19:05:21,017:INFO:           evidently: Not installed
2024-02-04 19:05:21,017:INFO:               fugue: Not installed
2024-02-04 19:05:21,017:INFO:           streamlit: Not installed
2024-02-04 19:05:21,017:INFO:             prophet: Not installed
2024-02-04 19:05:21,017:INFO:None
2024-02-04 19:05:21,017:INFO:Set up data.
2024-02-04 19:05:21,034:INFO:Set up folding strategy.
2024-02-04 19:05:21,034:INFO:Set up train/test split.
2024-02-04 19:05:21,044:INFO:Set up index.
2024-02-04 19:05:21,044:INFO:Assigning column types.
2024-02-04 19:05:21,051:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-02-04 19:05:21,051:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,051:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,051:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,084:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,117:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,117:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,117:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,117:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,117:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,134:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,167:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,201:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,201:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,201:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,201:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-02-04 19:05:21,201:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,201:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,234:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,267:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,267:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,267:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,281:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,284:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,320:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,351:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,351:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,351:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,351:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-02-04 19:05:21,351:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,384:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,420:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,420:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,420:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,432:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,467:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,495:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,495:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,495:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,500:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-02-04 19:05:21,537:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,567:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,567:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,567:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,617:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,645:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,645:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,645:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,645:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-02-04 19:05:21,692:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,717:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,717:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,767:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,796:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,796:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,796:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-02-04 19:05:21,867:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,867:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,934:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,934:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,946:INFO:Preparing preprocessing pipeline...
2024-02-04 19:05:21,946:INFO:Set up simple imputation.
2024-02-04 19:05:21,950:INFO:Set up encoding of ordinal features.
2024-02-04 19:05:21,950:INFO:Set up encoding of categorical features.
2024-02-04 19:05:22,081:INFO:Finished creating preprocessing pipeline.
2024-02-04 19:05:22,097:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))])
2024-02-04 19:05:22,097:INFO:Creating final display dataframe.
2024-02-04 19:05:22,448:INFO:Setup _display_container:                     Description         Value
0                    Session id           123
1                        Target     SalePrice
2                   Target type    Regression
3           Original data shape    (1456, 56)
4        Transformed data shape   (1456, 134)
5   Transformed train set shape   (1019, 134)
6    Transformed test set shape    (437, 134)
7              Ordinal features             1
8              Numeric features            40
9          Categorical features            15
10     Rows with missing values         99.7%
11                   Preprocess          True
12              Imputation type        simple
13           Numeric imputation          mean
14       Categorical imputation          mode
15     Maximum one-hot encoding            25
16              Encoding method          None
17               Fold Generator         KFold
18                  Fold Number            10
19                     CPU Jobs            -1
20                      Use GPU         False
21               Log Experiment  MlflowLogger
22              Experiment Name   houseprice1
23                          USI          027c
2024-02-04 19:05:22,541:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:22,541:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:22,617:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:22,617:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:22,617:INFO:Logging experiment in loggers
2024-02-04 19:05:22,783:INFO:SubProcess save_model() called ==================================
2024-02-04 19:05:22,807:INFO:Initializing save_model()
2024-02-04 19:05:22,807:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))]), model_name=C:\Users\joseg\AppData\Local\Temp\tmpf6nvxbbn\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2024-02-04 19:05:22,807:INFO:Adding model into prep_pipe
2024-02-04 19:05:22,807:WARNING:Only Model saved as it was a pipeline.
2024-02-04 19:05:22,817:INFO:C:\Users\joseg\AppData\Local\Temp\tmpf6nvxbbn\Transformation Pipeline.pkl saved in current working directory
2024-02-04 19:05:22,831:INFO:Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))])
2024-02-04 19:05:22,831:INFO:save_model() successfully completed......................................
2024-02-04 19:05:22,918:INFO:SubProcess save_model() end ==================================
2024-02-04 19:05:22,918:INFO:setup() successfully completed in 2.72s...............
2024-02-04 19:05:59,243:INFO:Initializing compare_models()
2024-02-04 19:05:59,243:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, include=None, fold=5, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, 'include': None, 'exclude': None, 'fold': 5, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2024-02-04 19:05:59,243:INFO:Checking exceptions
2024-02-04 19:05:59,243:INFO:Preparing display monitor
2024-02-04 19:05:59,271:INFO:Initializing Linear Regression
2024-02-04 19:05:59,271:INFO:Total runtime is 0.0 minutes
2024-02-04 19:05:59,271:INFO:SubProcess create_model() called ==================================
2024-02-04 19:05:59,271:INFO:Initializing create_model()
2024-02-04 19:05:59,271:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lr, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:05:59,271:INFO:Checking exceptions
2024-02-04 19:05:59,271:INFO:Importing libraries
2024-02-04 19:05:59,271:INFO:Copying training dataset
2024-02-04 19:05:59,271:INFO:Defining folds
2024-02-04 19:05:59,271:INFO:Declaring metric variables
2024-02-04 19:05:59,279:INFO:Importing untrained model
2024-02-04 19:05:59,279:INFO:Linear Regression Imported successfully
2024-02-04 19:05:59,287:INFO:Starting cross validation
2024-02-04 19:05:59,296:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:01,994:INFO:Calculating mean and std
2024-02-04 19:06:01,996:INFO:Creating metrics dataframe
2024-02-04 19:06:01,996:INFO:Uploading results into container
2024-02-04 19:06:01,996:INFO:Uploading model into container now
2024-02-04 19:06:01,996:INFO:_master_model_container: 1
2024-02-04 19:06:01,996:INFO:_display_container: 2
2024-02-04 19:06:01,996:INFO:LinearRegression(n_jobs=-1)
2024-02-04 19:06:01,996:INFO:create_model() successfully completed......................................
2024-02-04 19:06:02,096:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:02,096:INFO:Creating metrics dataframe
2024-02-04 19:06:02,096:INFO:Initializing Lasso Regression
2024-02-04 19:06:02,096:INFO:Total runtime is 0.04709691603978475 minutes
2024-02-04 19:06:02,096:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:02,096:INFO:Initializing create_model()
2024-02-04 19:06:02,096:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lasso, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:02,096:INFO:Checking exceptions
2024-02-04 19:06:02,110:INFO:Importing libraries
2024-02-04 19:06:02,110:INFO:Copying training dataset
2024-02-04 19:06:02,113:INFO:Defining folds
2024-02-04 19:06:02,113:INFO:Declaring metric variables
2024-02-04 19:06:02,113:INFO:Importing untrained model
2024-02-04 19:06:02,120:INFO:Lasso Regression Imported successfully
2024-02-04 19:06:02,120:INFO:Starting cross validation
2024-02-04 19:06:02,120:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:02,380:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.451e+11, tolerance: 5.149e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:02,396:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.972e+11, tolerance: 4.496e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:03,813:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.206e+11, tolerance: 5.100e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:03,846:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+11, tolerance: 5.182e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:03,846:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.138e+11, tolerance: 4.925e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:03,928:INFO:Calculating mean and std
2024-02-04 19:06:03,929:INFO:Creating metrics dataframe
2024-02-04 19:06:03,935:INFO:Uploading results into container
2024-02-04 19:06:03,935:INFO:Uploading model into container now
2024-02-04 19:06:03,937:INFO:_master_model_container: 2
2024-02-04 19:06:03,937:INFO:_display_container: 2
2024-02-04 19:06:03,937:INFO:Lasso(random_state=123)
2024-02-04 19:06:03,937:INFO:create_model() successfully completed......................................
2024-02-04 19:06:04,029:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:04,029:INFO:Creating metrics dataframe
2024-02-04 19:06:04,034:INFO:Initializing Ridge Regression
2024-02-04 19:06:04,034:INFO:Total runtime is 0.07938300768534343 minutes
2024-02-04 19:06:04,042:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:04,042:INFO:Initializing create_model()
2024-02-04 19:06:04,042:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=ridge, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:04,042:INFO:Checking exceptions
2024-02-04 19:06:04,042:INFO:Importing libraries
2024-02-04 19:06:04,042:INFO:Copying training dataset
2024-02-04 19:06:04,046:INFO:Defining folds
2024-02-04 19:06:04,046:INFO:Declaring metric variables
2024-02-04 19:06:04,051:INFO:Importing untrained model
2024-02-04 19:06:04,051:INFO:Ridge Regression Imported successfully
2024-02-04 19:06:04,051:INFO:Starting cross validation
2024-02-04 19:06:04,051:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:04,359:INFO:Calculating mean and std
2024-02-04 19:06:04,359:INFO:Creating metrics dataframe
2024-02-04 19:06:04,362:INFO:Uploading results into container
2024-02-04 19:06:04,362:INFO:Uploading model into container now
2024-02-04 19:06:04,362:INFO:_master_model_container: 3
2024-02-04 19:06:04,362:INFO:_display_container: 2
2024-02-04 19:06:04,362:INFO:Ridge(random_state=123)
2024-02-04 19:06:04,362:INFO:create_model() successfully completed......................................
2024-02-04 19:06:04,446:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:04,446:INFO:Creating metrics dataframe
2024-02-04 19:06:04,458:INFO:Initializing Elastic Net
2024-02-04 19:06:04,458:INFO:Total runtime is 0.08645121256510417 minutes
2024-02-04 19:06:04,464:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:04,464:INFO:Initializing create_model()
2024-02-04 19:06:04,464:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=en, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:04,464:INFO:Checking exceptions
2024-02-04 19:06:04,464:INFO:Importing libraries
2024-02-04 19:06:04,464:INFO:Copying training dataset
2024-02-04 19:06:04,464:INFO:Defining folds
2024-02-04 19:06:04,464:INFO:Declaring metric variables
2024-02-04 19:06:04,473:INFO:Importing untrained model
2024-02-04 19:06:04,477:INFO:Elastic Net Imported successfully
2024-02-04 19:06:04,481:INFO:Starting cross validation
2024-02-04 19:06:04,481:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:04,806:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.303e+11, tolerance: 5.100e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:04,813:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.375e+11, tolerance: 4.925e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:04,826:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.038e+11, tolerance: 4.496e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:04,827:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.592e+11, tolerance: 5.182e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:04,829:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.659e+11, tolerance: 5.149e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:04,913:INFO:Calculating mean and std
2024-02-04 19:06:04,914:INFO:Creating metrics dataframe
2024-02-04 19:06:04,918:INFO:Uploading results into container
2024-02-04 19:06:04,918:INFO:Uploading model into container now
2024-02-04 19:06:04,918:INFO:_master_model_container: 4
2024-02-04 19:06:04,918:INFO:_display_container: 2
2024-02-04 19:06:04,918:INFO:ElasticNet(random_state=123)
2024-02-04 19:06:04,918:INFO:create_model() successfully completed......................................
2024-02-04 19:06:05,008:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:05,008:INFO:Creating metrics dataframe
2024-02-04 19:06:05,016:INFO:Initializing Least Angle Regression
2024-02-04 19:06:05,016:INFO:Total runtime is 0.09575716257095337 minutes
2024-02-04 19:06:05,016:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:05,016:INFO:Initializing create_model()
2024-02-04 19:06:05,016:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lar, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:05,016:INFO:Checking exceptions
2024-02-04 19:06:05,016:INFO:Importing libraries
2024-02-04 19:06:05,016:INFO:Copying training dataset
2024-02-04 19:06:05,016:INFO:Defining folds
2024-02-04 19:06:05,016:INFO:Declaring metric variables
2024-02-04 19:06:05,029:INFO:Importing untrained model
2024-02-04 19:06:05,033:INFO:Least Angle Regression Imported successfully
2024-02-04 19:06:05,034:INFO:Starting cross validation
2024-02-04 19:06:05,034:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.120e+02, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.077e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=2.921e+02, with an active set of 49 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.866e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=7.747e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.845e+02, with an active set of 53 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.239e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.379e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=1.536e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.070e+02, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=8.829e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=7.866e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=1.273e+02, with an active set of 75 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=9.683e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=6.627e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=5.541e+01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=5.236e+01, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=5.124e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=4.716e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=4.493e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=4.481e+05, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.188e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=1.683e+02, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=2.478e+06, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 167 iterations, i.e. alpha=2.516e+06, with an active set of 131 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=9.821e+05, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=7.083e+05, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=6.203e+05, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=5.911e+05, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 170 iterations, i.e. alpha=3.301e+09, with an active set of 129 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=2.670e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 170 iterations, i.e. alpha=2.294e+09, with an active set of 129 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=5.158e+05, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=3.391e+05, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=3.311e+05, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 177 iterations, i.e. alpha=1.130e+04, with an active set of 130 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 177 iterations, i.e. alpha=2.523e+03, with an active set of 130 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=3.302e+05, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=6.480e+05, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,279:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 173 iterations, i.e. alpha=3.011e+08, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,279:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=4.104e+06, with an active set of 130 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,279:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=3.638e+06, with an active set of 130 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,358:INFO:Calculating mean and std
2024-02-04 19:06:05,358:INFO:Creating metrics dataframe
2024-02-04 19:06:05,358:INFO:Uploading results into container
2024-02-04 19:06:05,358:INFO:Uploading model into container now
2024-02-04 19:06:05,358:INFO:_master_model_container: 5
2024-02-04 19:06:05,358:INFO:_display_container: 2
2024-02-04 19:06:05,358:INFO:Lars(random_state=123)
2024-02-04 19:06:05,358:INFO:create_model() successfully completed......................................
2024-02-04 19:06:05,449:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:05,449:INFO:Creating metrics dataframe
2024-02-04 19:06:05,449:INFO:Initializing Lasso Least Angle Regression
2024-02-04 19:06:05,449:INFO:Total runtime is 0.10297880570093791 minutes
2024-02-04 19:06:05,465:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:05,465:INFO:Initializing create_model()
2024-02-04 19:06:05,465:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=llar, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:05,466:INFO:Checking exceptions
2024-02-04 19:06:05,466:INFO:Importing libraries
2024-02-04 19:06:05,466:INFO:Copying training dataset
2024-02-04 19:06:05,466:INFO:Defining folds
2024-02-04 19:06:05,466:INFO:Declaring metric variables
2024-02-04 19:06:05,473:INFO:Importing untrained model
2024-02-04 19:06:05,473:INFO:Lasso Least Angle Regression Imported successfully
2024-02-04 19:06:05,483:INFO:Starting cross validation
2024-02-04 19:06:05,483:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:05,717:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=3.367e+02, with an active set of 47 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,717:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.695e+02, with an active set of 65 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,717:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=5.136e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,727:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.452e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,729:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 79 iterations, alpha=1.116e+03, previous alpha=1.723e+02, with an active set of 60 regressors.
  warnings.warn(

2024-02-04 19:06:05,729:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.697e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,729:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=2.465e+01, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=1.866e+01, with an active set of 106 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 138 iterations, alpha=1.717e+01, previous alpha=1.717e+01, with an active set of 107 regressors.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.971e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=4.937e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=8.321e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.873e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=2.455e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=4.349e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.392e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.392e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=1.199e+02, with an active set of 71 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=2.372e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=9.636e+01, with an active set of 75 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,750:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 103 iterations, alpha=6.958e+01, previous alpha=6.958e+01, with an active set of 82 regressors.
  warnings.warn(

2024-02-04 19:06:05,750:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=1.063e+02, with an active set of 78 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,750:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 135 iterations, alpha=6.986e+01, previous alpha=2.488e+01, with an active set of 100 regressors.
  warnings.warn(

2024-02-04 19:06:05,750:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=6.564e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,750:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 140 iterations, alpha=2.593e+01, previous alpha=2.593e+01, with an active set of 97 regressors.
  warnings.warn(

2024-02-04 19:06:05,828:INFO:Calculating mean and std
2024-02-04 19:06:05,828:INFO:Creating metrics dataframe
2024-02-04 19:06:05,833:INFO:Uploading results into container
2024-02-04 19:06:05,833:INFO:Uploading model into container now
2024-02-04 19:06:05,833:INFO:_master_model_container: 6
2024-02-04 19:06:05,833:INFO:_display_container: 2
2024-02-04 19:06:05,833:INFO:LassoLars(random_state=123)
2024-02-04 19:06:05,833:INFO:create_model() successfully completed......................................
2024-02-04 19:06:05,929:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:05,929:INFO:Creating metrics dataframe
2024-02-04 19:06:05,938:INFO:Initializing Orthogonal Matching Pursuit
2024-02-04 19:06:05,938:INFO:Total runtime is 0.11112848122914633 minutes
2024-02-04 19:06:05,941:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:05,941:INFO:Initializing create_model()
2024-02-04 19:06:05,942:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=omp, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:05,942:INFO:Checking exceptions
2024-02-04 19:06:05,942:INFO:Importing libraries
2024-02-04 19:06:05,942:INFO:Copying training dataset
2024-02-04 19:06:05,946:INFO:Defining folds
2024-02-04 19:06:05,947:INFO:Declaring metric variables
2024-02-04 19:06:05,951:INFO:Importing untrained model
2024-02-04 19:06:05,953:INFO:Orthogonal Matching Pursuit Imported successfully
2024-02-04 19:06:05,962:INFO:Starting cross validation
2024-02-04 19:06:05,969:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:06,278:INFO:Calculating mean and std
2024-02-04 19:06:06,278:INFO:Creating metrics dataframe
2024-02-04 19:06:06,281:INFO:Uploading results into container
2024-02-04 19:06:06,281:INFO:Uploading model into container now
2024-02-04 19:06:06,281:INFO:_master_model_container: 7
2024-02-04 19:06:06,281:INFO:_display_container: 2
2024-02-04 19:06:06,285:INFO:OrthogonalMatchingPursuit()
2024-02-04 19:06:06,285:INFO:create_model() successfully completed......................................
2024-02-04 19:06:06,379:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:06,379:INFO:Creating metrics dataframe
2024-02-04 19:06:06,379:INFO:Initializing Bayesian Ridge
2024-02-04 19:06:06,379:INFO:Total runtime is 0.11848035256067913 minutes
2024-02-04 19:06:06,379:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:06,379:INFO:Initializing create_model()
2024-02-04 19:06:06,379:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=br, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:06,379:INFO:Checking exceptions
2024-02-04 19:06:06,379:INFO:Importing libraries
2024-02-04 19:06:06,379:INFO:Copying training dataset
2024-02-04 19:06:06,396:INFO:Defining folds
2024-02-04 19:06:06,396:INFO:Declaring metric variables
2024-02-04 19:06:06,396:INFO:Importing untrained model
2024-02-04 19:06:06,396:INFO:Bayesian Ridge Imported successfully
2024-02-04 19:06:06,396:INFO:Starting cross validation
2024-02-04 19:06:06,410:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:06,759:INFO:Calculating mean and std
2024-02-04 19:06:06,759:INFO:Creating metrics dataframe
2024-02-04 19:06:06,762:INFO:Uploading results into container
2024-02-04 19:06:06,762:INFO:Uploading model into container now
2024-02-04 19:06:06,762:INFO:_master_model_container: 8
2024-02-04 19:06:06,762:INFO:_display_container: 2
2024-02-04 19:06:06,764:INFO:BayesianRidge()
2024-02-04 19:06:06,764:INFO:create_model() successfully completed......................................
2024-02-04 19:06:06,849:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:06,859:INFO:Creating metrics dataframe
2024-02-04 19:06:06,867:INFO:Initializing Passive Aggressive Regressor
2024-02-04 19:06:06,867:INFO:Total runtime is 0.12661214272181193 minutes
2024-02-04 19:06:06,867:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:06,867:INFO:Initializing create_model()
2024-02-04 19:06:06,867:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=par, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:06,867:INFO:Checking exceptions
2024-02-04 19:06:06,867:INFO:Importing libraries
2024-02-04 19:06:06,867:INFO:Copying training dataset
2024-02-04 19:06:06,875:INFO:Defining folds
2024-02-04 19:06:06,875:INFO:Declaring metric variables
2024-02-04 19:06:06,880:INFO:Importing untrained model
2024-02-04 19:06:06,884:INFO:Passive Aggressive Regressor Imported successfully
2024-02-04 19:06:06,884:INFO:Starting cross validation
2024-02-04 19:06:06,884:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:07,194:INFO:Calculating mean and std
2024-02-04 19:06:07,194:INFO:Creating metrics dataframe
2024-02-04 19:06:07,199:INFO:Uploading results into container
2024-02-04 19:06:07,199:INFO:Uploading model into container now
2024-02-04 19:06:07,199:INFO:_master_model_container: 9
2024-02-04 19:06:07,199:INFO:_display_container: 2
2024-02-04 19:06:07,199:INFO:PassiveAggressiveRegressor(random_state=123)
2024-02-04 19:06:07,199:INFO:create_model() successfully completed......................................
2024-02-04 19:06:07,279:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:07,279:INFO:Creating metrics dataframe
2024-02-04 19:06:07,298:INFO:Initializing Huber Regressor
2024-02-04 19:06:07,300:INFO:Total runtime is 0.13379119237263998 minutes
2024-02-04 19:06:07,300:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:07,300:INFO:Initializing create_model()
2024-02-04 19:06:07,300:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=huber, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:07,300:INFO:Checking exceptions
2024-02-04 19:06:07,300:INFO:Importing libraries
2024-02-04 19:06:07,300:INFO:Copying training dataset
2024-02-04 19:06:07,300:INFO:Defining folds
2024-02-04 19:06:07,300:INFO:Declaring metric variables
2024-02-04 19:06:07,300:INFO:Importing untrained model
2024-02-04 19:06:07,313:INFO:Huber Regressor Imported successfully
2024-02-04 19:06:07,317:INFO:Starting cross validation
2024-02-04 19:06:07,317:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:07,603:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:06:07,616:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:06:07,618:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:06:07,634:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:06:07,651:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:06:07,728:INFO:Calculating mean and std
2024-02-04 19:06:07,731:INFO:Creating metrics dataframe
2024-02-04 19:06:07,734:INFO:Uploading results into container
2024-02-04 19:06:07,735:INFO:Uploading model into container now
2024-02-04 19:06:07,735:INFO:_master_model_container: 10
2024-02-04 19:06:07,735:INFO:_display_container: 2
2024-02-04 19:06:07,735:INFO:HuberRegressor()
2024-02-04 19:06:07,735:INFO:create_model() successfully completed......................................
2024-02-04 19:06:07,834:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:07,834:INFO:Creating metrics dataframe
2024-02-04 19:06:07,834:INFO:Initializing K Neighbors Regressor
2024-02-04 19:06:07,834:INFO:Total runtime is 0.1427184502283732 minutes
2024-02-04 19:06:07,834:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:07,844:INFO:Initializing create_model()
2024-02-04 19:06:07,844:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=knn, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:07,844:INFO:Checking exceptions
2024-02-04 19:06:07,844:INFO:Importing libraries
2024-02-04 19:06:07,844:INFO:Copying training dataset
2024-02-04 19:06:07,851:INFO:Defining folds
2024-02-04 19:06:07,851:INFO:Declaring metric variables
2024-02-04 19:06:07,851:INFO:Importing untrained model
2024-02-04 19:06:07,857:INFO:K Neighbors Regressor Imported successfully
2024-02-04 19:06:07,864:INFO:Starting cross validation
2024-02-04 19:06:07,864:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:08,291:INFO:Calculating mean and std
2024-02-04 19:06:08,291:INFO:Creating metrics dataframe
2024-02-04 19:06:08,291:INFO:Uploading results into container
2024-02-04 19:06:08,291:INFO:Uploading model into container now
2024-02-04 19:06:08,291:INFO:_master_model_container: 11
2024-02-04 19:06:08,291:INFO:_display_container: 2
2024-02-04 19:06:08,296:INFO:KNeighborsRegressor(n_jobs=-1)
2024-02-04 19:06:08,296:INFO:create_model() successfully completed......................................
2024-02-04 19:06:08,379:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:08,379:INFO:Creating metrics dataframe
2024-02-04 19:06:08,392:INFO:Initializing Decision Tree Regressor
2024-02-04 19:06:08,396:INFO:Total runtime is 0.1520889639854431 minutes
2024-02-04 19:06:08,399:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:08,399:INFO:Initializing create_model()
2024-02-04 19:06:08,399:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=dt, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:08,399:INFO:Checking exceptions
2024-02-04 19:06:08,399:INFO:Importing libraries
2024-02-04 19:06:08,399:INFO:Copying training dataset
2024-02-04 19:06:08,399:INFO:Defining folds
2024-02-04 19:06:08,399:INFO:Declaring metric variables
2024-02-04 19:06:08,399:INFO:Importing untrained model
2024-02-04 19:06:08,410:INFO:Decision Tree Regressor Imported successfully
2024-02-04 19:06:08,416:INFO:Starting cross validation
2024-02-04 19:06:08,416:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:08,777:INFO:Calculating mean and std
2024-02-04 19:06:08,777:INFO:Creating metrics dataframe
2024-02-04 19:06:08,779:INFO:Uploading results into container
2024-02-04 19:06:08,779:INFO:Uploading model into container now
2024-02-04 19:06:08,779:INFO:_master_model_container: 12
2024-02-04 19:06:08,779:INFO:_display_container: 2
2024-02-04 19:06:08,779:INFO:DecisionTreeRegressor(random_state=123)
2024-02-04 19:06:08,779:INFO:create_model() successfully completed......................................
2024-02-04 19:06:08,865:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:08,865:INFO:Creating metrics dataframe
2024-02-04 19:06:08,865:INFO:Initializing Random Forest Regressor
2024-02-04 19:06:08,865:INFO:Total runtime is 0.1599098483721415 minutes
2024-02-04 19:06:08,881:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:08,881:INFO:Initializing create_model()
2024-02-04 19:06:08,881:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=rf, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:08,881:INFO:Checking exceptions
2024-02-04 19:06:08,881:INFO:Importing libraries
2024-02-04 19:06:08,881:INFO:Copying training dataset
2024-02-04 19:06:08,881:INFO:Defining folds
2024-02-04 19:06:08,881:INFO:Declaring metric variables
2024-02-04 19:06:08,881:INFO:Importing untrained model
2024-02-04 19:06:08,892:INFO:Random Forest Regressor Imported successfully
2024-02-04 19:06:08,898:INFO:Starting cross validation
2024-02-04 19:06:08,903:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:10,654:INFO:Calculating mean and std
2024-02-04 19:06:10,654:INFO:Creating metrics dataframe
2024-02-04 19:06:10,657:INFO:Uploading results into container
2024-02-04 19:06:10,657:INFO:Uploading model into container now
2024-02-04 19:06:10,657:INFO:_master_model_container: 13
2024-02-04 19:06:10,657:INFO:_display_container: 2
2024-02-04 19:06:10,657:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:06:10,657:INFO:create_model() successfully completed......................................
2024-02-04 19:06:10,750:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:10,750:INFO:Creating metrics dataframe
2024-02-04 19:06:10,750:INFO:Initializing Extra Trees Regressor
2024-02-04 19:06:10,750:INFO:Total runtime is 0.19132637580235798 minutes
2024-02-04 19:06:10,763:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:10,763:INFO:Initializing create_model()
2024-02-04 19:06:10,763:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=et, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:10,763:INFO:Checking exceptions
2024-02-04 19:06:10,763:INFO:Importing libraries
2024-02-04 19:06:10,763:INFO:Copying training dataset
2024-02-04 19:06:10,768:INFO:Defining folds
2024-02-04 19:06:10,768:INFO:Declaring metric variables
2024-02-04 19:06:10,768:INFO:Importing untrained model
2024-02-04 19:06:10,774:INFO:Extra Trees Regressor Imported successfully
2024-02-04 19:06:10,782:INFO:Starting cross validation
2024-02-04 19:06:10,782:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:12,025:INFO:Calculating mean and std
2024-02-04 19:06:12,025:INFO:Creating metrics dataframe
2024-02-04 19:06:12,028:INFO:Uploading results into container
2024-02-04 19:06:12,029:INFO:Uploading model into container now
2024-02-04 19:06:12,029:INFO:_master_model_container: 14
2024-02-04 19:06:12,029:INFO:_display_container: 2
2024-02-04 19:06:12,029:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:06:12,029:INFO:create_model() successfully completed......................................
2024-02-04 19:06:12,117:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:12,117:INFO:Creating metrics dataframe
2024-02-04 19:06:12,120:INFO:Initializing AdaBoost Regressor
2024-02-04 19:06:12,120:INFO:Total runtime is 0.21416183312733966 minutes
2024-02-04 19:06:12,120:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:12,120:INFO:Initializing create_model()
2024-02-04 19:06:12,120:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=ada, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:12,120:INFO:Checking exceptions
2024-02-04 19:06:12,120:INFO:Importing libraries
2024-02-04 19:06:12,129:INFO:Copying training dataset
2024-02-04 19:06:12,129:INFO:Defining folds
2024-02-04 19:06:12,129:INFO:Declaring metric variables
2024-02-04 19:06:12,137:INFO:Importing untrained model
2024-02-04 19:06:12,140:INFO:AdaBoost Regressor Imported successfully
2024-02-04 19:06:12,146:INFO:Starting cross validation
2024-02-04 19:06:12,149:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:12,675:INFO:Calculating mean and std
2024-02-04 19:06:12,675:INFO:Creating metrics dataframe
2024-02-04 19:06:12,675:INFO:Uploading results into container
2024-02-04 19:06:12,675:INFO:Uploading model into container now
2024-02-04 19:06:12,675:INFO:_master_model_container: 15
2024-02-04 19:06:12,675:INFO:_display_container: 2
2024-02-04 19:06:12,675:INFO:AdaBoostRegressor(random_state=123)
2024-02-04 19:06:12,675:INFO:create_model() successfully completed......................................
2024-02-04 19:06:12,764:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:12,764:INFO:Creating metrics dataframe
2024-02-04 19:06:12,772:INFO:Initializing Gradient Boosting Regressor
2024-02-04 19:06:12,772:INFO:Total runtime is 0.22501918474833169 minutes
2024-02-04 19:06:12,779:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:12,782:INFO:Initializing create_model()
2024-02-04 19:06:12,782:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=gbr, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:12,782:INFO:Checking exceptions
2024-02-04 19:06:12,782:INFO:Importing libraries
2024-02-04 19:06:12,782:INFO:Copying training dataset
2024-02-04 19:06:12,782:INFO:Defining folds
2024-02-04 19:06:12,782:INFO:Declaring metric variables
2024-02-04 19:06:12,787:INFO:Importing untrained model
2024-02-04 19:06:12,791:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:06:12,796:INFO:Starting cross validation
2024-02-04 19:06:12,801:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:13,558:INFO:Calculating mean and std
2024-02-04 19:06:13,558:INFO:Creating metrics dataframe
2024-02-04 19:06:13,558:INFO:Uploading results into container
2024-02-04 19:06:13,558:INFO:Uploading model into container now
2024-02-04 19:06:13,558:INFO:_master_model_container: 16
2024-02-04 19:06:13,558:INFO:_display_container: 2
2024-02-04 19:06:13,558:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:06:13,558:INFO:create_model() successfully completed......................................
2024-02-04 19:06:13,644:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:13,644:INFO:Creating metrics dataframe
2024-02-04 19:06:13,664:INFO:Initializing Light Gradient Boosting Machine
2024-02-04 19:06:13,664:INFO:Total runtime is 0.23988552888234455 minutes
2024-02-04 19:06:13,666:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:13,666:INFO:Initializing create_model()
2024-02-04 19:06:13,666:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lightgbm, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:13,666:INFO:Checking exceptions
2024-02-04 19:06:13,666:INFO:Importing libraries
2024-02-04 19:06:13,666:INFO:Copying training dataset
2024-02-04 19:06:13,666:INFO:Defining folds
2024-02-04 19:06:13,666:INFO:Declaring metric variables
2024-02-04 19:06:13,675:INFO:Importing untrained model
2024-02-04 19:06:13,679:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-04 19:06:13,683:INFO:Starting cross validation
2024-02-04 19:06:13,683:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:14,462:INFO:Calculating mean and std
2024-02-04 19:06:14,462:INFO:Creating metrics dataframe
2024-02-04 19:06:14,468:INFO:Uploading results into container
2024-02-04 19:06:14,468:INFO:Uploading model into container now
2024-02-04 19:06:14,468:INFO:_master_model_container: 17
2024-02-04 19:06:14,468:INFO:_display_container: 2
2024-02-04 19:06:14,468:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:06:14,468:INFO:create_model() successfully completed......................................
2024-02-04 19:06:14,584:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:14,584:INFO:Creating metrics dataframe
2024-02-04 19:06:14,584:INFO:Initializing Dummy Regressor
2024-02-04 19:06:14,584:INFO:Total runtime is 0.25522610743840535 minutes
2024-02-04 19:06:14,584:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:14,584:INFO:Initializing create_model()
2024-02-04 19:06:14,584:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=dummy, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:14,584:INFO:Checking exceptions
2024-02-04 19:06:14,596:INFO:Importing libraries
2024-02-04 19:06:14,596:INFO:Copying training dataset
2024-02-04 19:06:14,601:INFO:Defining folds
2024-02-04 19:06:14,601:INFO:Declaring metric variables
2024-02-04 19:06:14,606:INFO:Importing untrained model
2024-02-04 19:06:14,606:INFO:Dummy Regressor Imported successfully
2024-02-04 19:06:14,615:INFO:Starting cross validation
2024-02-04 19:06:14,615:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:14,928:INFO:Calculating mean and std
2024-02-04 19:06:14,930:INFO:Creating metrics dataframe
2024-02-04 19:06:14,933:INFO:Uploading results into container
2024-02-04 19:06:14,934:INFO:Uploading model into container now
2024-02-04 19:06:14,934:INFO:_master_model_container: 18
2024-02-04 19:06:14,934:INFO:_display_container: 2
2024-02-04 19:06:14,934:INFO:DummyRegressor()
2024-02-04 19:06:14,934:INFO:create_model() successfully completed......................................
2024-02-04 19:06:15,024:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:15,024:INFO:Creating metrics dataframe
2024-02-04 19:06:15,048:INFO:Initializing create_model()
2024-02-04 19:06:15,048:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:15,048:INFO:Checking exceptions
2024-02-04 19:06:15,050:INFO:Importing libraries
2024-02-04 19:06:15,050:INFO:Copying training dataset
2024-02-04 19:06:15,050:INFO:Defining folds
2024-02-04 19:06:15,050:INFO:Declaring metric variables
2024-02-04 19:06:15,050:INFO:Importing untrained model
2024-02-04 19:06:15,050:INFO:Declaring custom model
2024-02-04 19:06:15,050:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:06:15,050:INFO:Cross validation set to False
2024-02-04 19:06:15,050:INFO:Fitting Model
2024-02-04 19:06:15,610:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:06:15,610:INFO:create_model() successfully completed......................................
2024-02-04 19:06:15,712:INFO:Creating Dashboard logs
2024-02-04 19:06:15,715:INFO:Model: Gradient Boosting Regressor
2024-02-04 19:06:15,734:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-04 19:06:15,804:INFO:Initializing predict_model()
2024-02-04 19:06:15,804:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECCF0FC040>)
2024-02-04 19:06:15,804:INFO:Checking exceptions
2024-02-04 19:06:15,804:INFO:Preloading libraries
2024-02-04 19:06:17,026:INFO:Creating Dashboard logs
2024-02-04 19:06:17,042:INFO:Model: Light Gradient Boosting Machine
2024-02-04 19:06:17,058:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2024-02-04 19:06:17,229:INFO:Creating Dashboard logs
2024-02-04 19:06:17,229:INFO:Model: Extra Trees Regressor
2024-02-04 19:06:17,256:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-02-04 19:06:17,426:INFO:Creating Dashboard logs
2024-02-04 19:06:17,426:INFO:Model: Random Forest Regressor
2024-02-04 19:06:17,450:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-02-04 19:06:17,613:INFO:Creating Dashboard logs
2024-02-04 19:06:17,613:INFO:Model: Ridge Regression
2024-02-04 19:06:17,628:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}
2024-02-04 19:06:17,813:INFO:Creating Dashboard logs
2024-02-04 19:06:17,813:INFO:Model: Lasso Least Angle Regression
2024-02-04 19:06:17,829:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'max_iter': 500, 'normalize': 'deprecated', 'positive': False, 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2024-02-04 19:06:17,996:INFO:Creating Dashboard logs
2024-02-04 19:06:18,001:INFO:Model: Lasso Regression
2024-02-04 19:06:18,022:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2024-02-04 19:06:18,194:INFO:Creating Dashboard logs
2024-02-04 19:06:18,197:INFO:Model: Elastic Net
2024-02-04 19:06:18,210:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2024-02-04 19:06:18,397:INFO:Creating Dashboard logs
2024-02-04 19:06:18,397:INFO:Model: Bayesian Ridge
2024-02-04 19:06:18,419:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'n_iter': 300, 'tol': 0.001, 'verbose': False}
2024-02-04 19:06:18,571:INFO:Creating Dashboard logs
2024-02-04 19:06:18,571:INFO:Model: AdaBoost Regressor
2024-02-04 19:06:18,599:INFO:Logged params: {'base_estimator': 'deprecated', 'estimator': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50, 'random_state': 123}
2024-02-04 19:06:18,762:INFO:Creating Dashboard logs
2024-02-04 19:06:18,770:INFO:Model: Orthogonal Matching Pursuit
2024-02-04 19:06:18,789:INFO:Logged params: {'fit_intercept': True, 'n_nonzero_coefs': None, 'normalize': 'deprecated', 'precompute': 'auto', 'tol': None}
2024-02-04 19:06:18,944:INFO:Creating Dashboard logs
2024-02-04 19:06:18,944:INFO:Model: Decision Tree Regressor
2024-02-04 19:06:18,976:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 123, 'splitter': 'best'}
2024-02-04 19:06:19,141:INFO:Creating Dashboard logs
2024-02-04 19:06:19,141:INFO:Model: Huber Regressor
2024-02-04 19:06:19,157:INFO:Logged params: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': True, 'max_iter': 100, 'tol': 1e-05, 'warm_start': False}
2024-02-04 19:06:19,308:INFO:Creating Dashboard logs
2024-02-04 19:06:19,324:INFO:Model: K Neighbors Regressor
2024-02-04 19:06:19,333:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2024-02-04 19:06:19,507:INFO:Creating Dashboard logs
2024-02-04 19:06:19,507:INFO:Model: Passive Aggressive Regressor
2024-02-04 19:06:19,523:INFO:Logged params: {'C': 1.0, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'fit_intercept': True, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'n_iter_no_change': 5, 'random_state': 123, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-04 19:06:19,694:INFO:Creating Dashboard logs
2024-02-04 19:06:19,711:INFO:Model: Dummy Regressor
2024-02-04 19:06:19,729:INFO:Logged params: {'constant': None, 'quantile': None, 'strategy': 'mean'}
2024-02-04 19:06:19,881:INFO:Creating Dashboard logs
2024-02-04 19:06:19,881:INFO:Model: Linear Regression
2024-02-04 19:06:19,893:INFO:Logged params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'positive': False}
2024-02-04 19:06:20,060:INFO:Creating Dashboard logs
2024-02-04 19:06:20,066:INFO:Model: Least Angle Regression
2024-02-04 19:06:20,076:INFO:Logged params: {'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'n_nonzero_coefs': 500, 'normalize': 'deprecated', 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2024-02-04 19:06:20,246:INFO:_master_model_container: 18
2024-02-04 19:06:20,246:INFO:_display_container: 2
2024-02-04 19:06:20,246:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:06:20,246:INFO:compare_models() successfully completed......................................
2024-02-04 19:06:32,022:INFO:Initializing plot_model()
2024-02-04 19:06:32,022:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, system=True)
2024-02-04 19:06:32,022:INFO:Checking exceptions
2024-02-04 19:06:32,026:INFO:Preloading libraries
2024-02-04 19:06:32,034:INFO:Copying training dataset
2024-02-04 19:06:32,034:INFO:Plot type: residuals
2024-02-04 19:06:32,428:INFO:Fitting Model
2024-02-04 19:06:32,428:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names
  warnings.warn(

2024-02-04 19:06:32,455:INFO:Scoring test/hold-out set
2024-02-04 19:06:32,755:INFO:Visual Rendered Successfully
2024-02-04 19:06:32,855:INFO:plot_model() successfully completed......................................
2024-02-04 19:06:35,517:INFO:Initializing plot_model()
2024-02-04 19:06:35,517:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, system=True)
2024-02-04 19:06:35,517:INFO:Checking exceptions
2024-02-04 19:06:35,517:INFO:Preloading libraries
2024-02-04 19:06:35,523:INFO:Copying training dataset
2024-02-04 19:06:35,523:INFO:Plot type: error
2024-02-04 19:06:35,906:INFO:Fitting Model
2024-02-04 19:06:35,906:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names
  warnings.warn(

2024-02-04 19:06:35,906:INFO:Scoring test/hold-out set
2024-02-04 19:06:36,055:INFO:Visual Rendered Successfully
2024-02-04 19:06:36,156:INFO:plot_model() successfully completed......................................
2024-02-04 19:06:38,457:INFO:Initializing plot_model()
2024-02-04 19:06:38,457:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, system=True)
2024-02-04 19:06:38,457:INFO:Checking exceptions
2024-02-04 19:06:38,457:INFO:Preloading libraries
2024-02-04 19:06:38,465:INFO:Copying training dataset
2024-02-04 19:06:38,465:INFO:Plot type: feature
2024-02-04 19:06:38,465:WARNING:No coef_ found. Trying feature_importances_
2024-02-04 19:06:38,609:INFO:Visual Rendered Successfully
2024-02-04 19:06:38,706:INFO:plot_model() successfully completed......................................
2024-02-04 19:06:48,738:INFO:Initializing create_model()
2024-02-04 19:06:48,745:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:48,746:INFO:Checking exceptions
2024-02-04 19:06:48,754:INFO:Importing libraries
2024-02-04 19:06:48,754:INFO:Copying training dataset
2024-02-04 19:06:48,763:INFO:Defining folds
2024-02-04 19:06:48,763:INFO:Declaring metric variables
2024-02-04 19:06:48,763:INFO:Importing untrained model
2024-02-04 19:06:48,763:INFO:Declaring custom model
2024-02-04 19:06:48,763:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:06:48,779:INFO:Starting cross validation
2024-02-04 19:06:48,779:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:50,626:INFO:Calculating mean and std
2024-02-04 19:06:50,626:INFO:Creating metrics dataframe
2024-02-04 19:06:50,629:INFO:Finalizing model
2024-02-04 19:06:51,183:INFO:Creating Dashboard logs
2024-02-04 19:06:51,183:INFO:Model: Gradient Boosting Regressor
2024-02-04 19:06:51,212:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-04 19:06:51,261:INFO:Initializing predict_model()
2024-02-04 19:06:51,261:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECCED87040>)
2024-02-04 19:06:51,261:INFO:Checking exceptions
2024-02-04 19:06:51,261:INFO:Preloading libraries
2024-02-04 19:06:51,655:INFO:Uploading results into container
2024-02-04 19:06:51,655:INFO:Uploading model into container now
2024-02-04 19:06:51,664:INFO:_master_model_container: 19
2024-02-04 19:06:51,664:INFO:_display_container: 3
2024-02-04 19:06:51,664:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:06:51,664:INFO:create_model() successfully completed......................................
2024-02-04 19:06:55,874:INFO:Initializing create_model()
2024-02-04 19:06:55,874:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lightgbm, fold=3, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:55,874:INFO:Checking exceptions
2024-02-04 19:06:55,883:INFO:Importing libraries
2024-02-04 19:06:55,883:INFO:Copying training dataset
2024-02-04 19:06:55,883:INFO:Defining folds
2024-02-04 19:06:55,883:INFO:Declaring metric variables
2024-02-04 19:06:55,883:INFO:Importing untrained model
2024-02-04 19:06:55,898:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-04 19:06:55,900:INFO:Starting cross validation
2024-02-04 19:06:55,900:INFO:Cross validating with KFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:56,497:INFO:Calculating mean and std
2024-02-04 19:06:56,497:INFO:Creating metrics dataframe
2024-02-04 19:06:56,499:INFO:Finalizing model
2024-02-04 19:06:56,662:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-02-04 19:06:56,662:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000441 seconds.
2024-02-04 19:06:56,662:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-04 19:06:56,662:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-04 19:06:56,662:INFO:[LightGBM] [Info] Total Bins 2881
2024-02-04 19:06:56,662:INFO:[LightGBM] [Info] Number of data points in the train set: 1019, number of used features: 93
2024-02-04 19:06:56,662:INFO:[LightGBM] [Info] Start training from score 180679.625123
2024-02-04 19:06:56,736:INFO:Creating Dashboard logs
2024-02-04 19:06:56,753:INFO:Model: Light Gradient Boosting Machine
2024-02-04 19:06:56,781:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2024-02-04 19:06:56,852:INFO:Initializing predict_model()
2024-02-04 19:06:56,852:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECCED87040>)
2024-02-04 19:06:56,852:INFO:Checking exceptions
2024-02-04 19:06:56,852:INFO:Preloading libraries
2024-02-04 19:06:57,312:INFO:Uploading results into container
2024-02-04 19:06:57,316:INFO:Uploading model into container now
2024-02-04 19:06:57,317:INFO:_master_model_container: 20
2024-02-04 19:06:57,317:INFO:_display_container: 4
2024-02-04 19:06:57,317:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:06:57,317:INFO:create_model() successfully completed......................................
2024-02-04 19:07:02,697:INFO:Initializing compare_models()
2024-02-04 19:07:02,697:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, include=['lightgbm', 'gbr', 'br'], fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, 'include': ['lightgbm', 'gbr', 'br'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2024-02-04 19:07:02,697:INFO:Checking exceptions
2024-02-04 19:07:02,705:INFO:Preparing display monitor
2024-02-04 19:07:02,721:INFO:Initializing Light Gradient Boosting Machine
2024-02-04 19:07:02,721:INFO:Total runtime is 0.0 minutes
2024-02-04 19:07:02,721:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:02,729:INFO:Initializing create_model()
2024-02-04 19:07:02,729:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECC8890700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:02,729:INFO:Checking exceptions
2024-02-04 19:07:02,729:INFO:Importing libraries
2024-02-04 19:07:02,729:INFO:Copying training dataset
2024-02-04 19:07:02,729:INFO:Defining folds
2024-02-04 19:07:02,729:INFO:Declaring metric variables
2024-02-04 19:07:02,729:INFO:Importing untrained model
2024-02-04 19:07:02,738:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-04 19:07:02,745:INFO:Starting cross validation
2024-02-04 19:07:02,745:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:04,499:INFO:Calculating mean and std
2024-02-04 19:07:04,499:INFO:Creating metrics dataframe
2024-02-04 19:07:04,505:INFO:Uploading results into container
2024-02-04 19:07:04,505:INFO:Uploading model into container now
2024-02-04 19:07:04,505:INFO:_master_model_container: 21
2024-02-04 19:07:04,505:INFO:_display_container: 5
2024-02-04 19:07:04,505:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:07:04,505:INFO:create_model() successfully completed......................................
2024-02-04 19:07:04,606:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:04,606:INFO:Creating metrics dataframe
2024-02-04 19:07:04,617:INFO:Initializing Gradient Boosting Regressor
2024-02-04 19:07:04,617:INFO:Total runtime is 0.03159751097361247 minutes
2024-02-04 19:07:04,617:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:04,617:INFO:Initializing create_model()
2024-02-04 19:07:04,617:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECC8890700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:04,617:INFO:Checking exceptions
2024-02-04 19:07:04,617:INFO:Importing libraries
2024-02-04 19:07:04,617:INFO:Copying training dataset
2024-02-04 19:07:04,617:INFO:Defining folds
2024-02-04 19:07:04,617:INFO:Declaring metric variables
2024-02-04 19:07:04,617:INFO:Importing untrained model
2024-02-04 19:07:04,635:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:07:04,643:INFO:Starting cross validation
2024-02-04 19:07:04,643:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:06,468:INFO:Calculating mean and std
2024-02-04 19:07:06,468:INFO:Creating metrics dataframe
2024-02-04 19:07:06,472:INFO:Uploading results into container
2024-02-04 19:07:06,472:INFO:Uploading model into container now
2024-02-04 19:07:06,472:INFO:_master_model_container: 22
2024-02-04 19:07:06,472:INFO:_display_container: 5
2024-02-04 19:07:06,472:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:07:06,472:INFO:create_model() successfully completed......................................
2024-02-04 19:07:06,561:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:06,561:INFO:Creating metrics dataframe
2024-02-04 19:07:06,580:INFO:Initializing Bayesian Ridge
2024-02-04 19:07:06,580:INFO:Total runtime is 0.0643048405647278 minutes
2024-02-04 19:07:06,580:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:06,580:INFO:Initializing create_model()
2024-02-04 19:07:06,580:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECC8890700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:06,580:INFO:Checking exceptions
2024-02-04 19:07:06,580:INFO:Importing libraries
2024-02-04 19:07:06,580:INFO:Copying training dataset
2024-02-04 19:07:06,588:INFO:Defining folds
2024-02-04 19:07:06,588:INFO:Declaring metric variables
2024-02-04 19:07:06,594:INFO:Importing untrained model
2024-02-04 19:07:06,594:INFO:Bayesian Ridge Imported successfully
2024-02-04 19:07:06,594:INFO:Starting cross validation
2024-02-04 19:07:06,605:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:07,271:INFO:Calculating mean and std
2024-02-04 19:07:07,271:INFO:Creating metrics dataframe
2024-02-04 19:07:07,271:INFO:Uploading results into container
2024-02-04 19:07:07,271:INFO:Uploading model into container now
2024-02-04 19:07:07,271:INFO:_master_model_container: 23
2024-02-04 19:07:07,271:INFO:_display_container: 5
2024-02-04 19:07:07,271:INFO:BayesianRidge()
2024-02-04 19:07:07,271:INFO:create_model() successfully completed......................................
2024-02-04 19:07:07,362:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:07,362:INFO:Creating metrics dataframe
2024-02-04 19:07:07,368:INFO:Initializing create_model()
2024-02-04 19:07:07,368:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:07,368:INFO:Checking exceptions
2024-02-04 19:07:07,368:INFO:Importing libraries
2024-02-04 19:07:07,368:INFO:Copying training dataset
2024-02-04 19:07:07,384:INFO:Defining folds
2024-02-04 19:07:07,384:INFO:Declaring metric variables
2024-02-04 19:07:07,384:INFO:Importing untrained model
2024-02-04 19:07:07,384:INFO:Declaring custom model
2024-02-04 19:07:07,384:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:07:07,384:INFO:Cross validation set to False
2024-02-04 19:07:07,384:INFO:Fitting Model
2024-02-04 19:07:07,918:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:07:07,918:INFO:create_model() successfully completed......................................
2024-02-04 19:07:08,012:INFO:Creating Dashboard logs
2024-02-04 19:07:08,014:INFO:Model: Gradient Boosting Regressor
2024-02-04 19:07:08,038:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-04 19:07:08,095:INFO:Initializing predict_model()
2024-02-04 19:07:08,095:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECCED87430>)
2024-02-04 19:07:08,095:INFO:Checking exceptions
2024-02-04 19:07:08,095:INFO:Preloading libraries
2024-02-04 19:07:08,475:INFO:Creating Dashboard logs
2024-02-04 19:07:08,475:INFO:Model: Light Gradient Boosting Machine
2024-02-04 19:07:08,506:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2024-02-04 19:07:08,664:INFO:Creating Dashboard logs
2024-02-04 19:07:08,664:INFO:Model: Bayesian Ridge
2024-02-04 19:07:08,693:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'n_iter': 300, 'tol': 0.001, 'verbose': False}
2024-02-04 19:07:08,870:INFO:_master_model_container: 23
2024-02-04 19:07:08,870:INFO:_display_container: 5
2024-02-04 19:07:08,870:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:07:08,870:INFO:compare_models() successfully completed......................................
2024-02-04 19:07:19,861:INFO:Initializing compare_models()
2024-02-04 19:07:19,861:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, include=None, fold=None, round=4, cross_validation=True, sort=MAE, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MAE', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2024-02-04 19:07:19,861:INFO:Checking exceptions
2024-02-04 19:07:19,865:INFO:Preparing display monitor
2024-02-04 19:07:19,884:INFO:Initializing Linear Regression
2024-02-04 19:07:19,884:INFO:Total runtime is 0.0 minutes
2024-02-04 19:07:19,884:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:19,884:INFO:Initializing create_model()
2024-02-04 19:07:19,884:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:19,884:INFO:Checking exceptions
2024-02-04 19:07:19,884:INFO:Importing libraries
2024-02-04 19:07:19,884:INFO:Copying training dataset
2024-02-04 19:07:19,889:INFO:Defining folds
2024-02-04 19:07:19,889:INFO:Declaring metric variables
2024-02-04 19:07:19,889:INFO:Importing untrained model
2024-02-04 19:07:19,896:INFO:Linear Regression Imported successfully
2024-02-04 19:07:19,905:INFO:Starting cross validation
2024-02-04 19:07:19,908:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:20,588:INFO:Calculating mean and std
2024-02-04 19:07:20,588:INFO:Creating metrics dataframe
2024-02-04 19:07:20,590:INFO:Uploading results into container
2024-02-04 19:07:20,590:INFO:Uploading model into container now
2024-02-04 19:07:20,590:INFO:_master_model_container: 24
2024-02-04 19:07:20,590:INFO:_display_container: 6
2024-02-04 19:07:20,590:INFO:LinearRegression(n_jobs=-1)
2024-02-04 19:07:20,590:INFO:create_model() successfully completed......................................
2024-02-04 19:07:20,697:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:20,697:INFO:Creating metrics dataframe
2024-02-04 19:07:20,697:INFO:Initializing Lasso Regression
2024-02-04 19:07:20,697:INFO:Total runtime is 0.013545691967010498 minutes
2024-02-04 19:07:20,706:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:20,707:INFO:Initializing create_model()
2024-02-04 19:07:20,707:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:20,707:INFO:Checking exceptions
2024-02-04 19:07:20,707:INFO:Importing libraries
2024-02-04 19:07:20,707:INFO:Copying training dataset
2024-02-04 19:07:20,707:INFO:Defining folds
2024-02-04 19:07:20,707:INFO:Declaring metric variables
2024-02-04 19:07:20,707:INFO:Importing untrained model
2024-02-04 19:07:20,707:INFO:Lasso Regression Imported successfully
2024-02-04 19:07:20,723:INFO:Starting cross validation
2024-02-04 19:07:20,723:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:21,123:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.799e+11, tolerance: 5.738e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,140:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.569e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,156:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.692e+11, tolerance: 5.599e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,157:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.241e+11, tolerance: 5.445e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,157:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.563e+11, tolerance: 5.378e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,186:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.631e+11, tolerance: 5.696e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,186:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.657e+11, tolerance: 5.662e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,208:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.375e+11, tolerance: 5.336e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,423:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.674e+11, tolerance: 5.647e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,423:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.671e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,473:INFO:Calculating mean and std
2024-02-04 19:07:21,473:INFO:Creating metrics dataframe
2024-02-04 19:07:21,473:INFO:Uploading results into container
2024-02-04 19:07:21,473:INFO:Uploading model into container now
2024-02-04 19:07:21,473:INFO:_master_model_container: 25
2024-02-04 19:07:21,473:INFO:_display_container: 6
2024-02-04 19:07:21,473:INFO:Lasso(random_state=123)
2024-02-04 19:07:21,473:INFO:create_model() successfully completed......................................
2024-02-04 19:07:21,557:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:21,557:INFO:Creating metrics dataframe
2024-02-04 19:07:21,576:INFO:Initializing Ridge Regression
2024-02-04 19:07:21,576:INFO:Total runtime is 0.02819267908732096 minutes
2024-02-04 19:07:21,576:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:21,576:INFO:Initializing create_model()
2024-02-04 19:07:21,576:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:21,576:INFO:Checking exceptions
2024-02-04 19:07:21,576:INFO:Importing libraries
2024-02-04 19:07:21,576:INFO:Copying training dataset
2024-02-04 19:07:21,576:INFO:Defining folds
2024-02-04 19:07:21,576:INFO:Declaring metric variables
2024-02-04 19:07:21,586:INFO:Importing untrained model
2024-02-04 19:07:21,590:INFO:Ridge Regression Imported successfully
2024-02-04 19:07:21,592:INFO:Starting cross validation
2024-02-04 19:07:21,592:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:22,169:INFO:Calculating mean and std
2024-02-04 19:07:22,169:INFO:Creating metrics dataframe
2024-02-04 19:07:22,173:INFO:Uploading results into container
2024-02-04 19:07:22,173:INFO:Uploading model into container now
2024-02-04 19:07:22,173:INFO:_master_model_container: 26
2024-02-04 19:07:22,173:INFO:_display_container: 6
2024-02-04 19:07:22,173:INFO:Ridge(random_state=123)
2024-02-04 19:07:22,173:INFO:create_model() successfully completed......................................
2024-02-04 19:07:22,257:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:22,257:INFO:Creating metrics dataframe
2024-02-04 19:07:22,257:INFO:Initializing Elastic Net
2024-02-04 19:07:22,270:INFO:Total runtime is 0.03976488510767619 minutes
2024-02-04 19:07:22,273:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:22,273:INFO:Initializing create_model()
2024-02-04 19:07:22,273:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:22,273:INFO:Checking exceptions
2024-02-04 19:07:22,273:INFO:Importing libraries
2024-02-04 19:07:22,273:INFO:Copying training dataset
2024-02-04 19:07:22,279:INFO:Defining folds
2024-02-04 19:07:22,279:INFO:Declaring metric variables
2024-02-04 19:07:22,279:INFO:Importing untrained model
2024-02-04 19:07:22,287:INFO:Elastic Net Imported successfully
2024-02-04 19:07:22,289:INFO:Starting cross validation
2024-02-04 19:07:22,289:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:22,687:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.761e+11, tolerance: 5.445e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,702:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.858e+11, tolerance: 5.662e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,718:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.863e+11, tolerance: 5.599e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,718:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.013e+11, tolerance: 5.738e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,718:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.889e+11, tolerance: 5.696e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,718:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.602e+11, tolerance: 5.336e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,733:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.721e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,749:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+11, tolerance: 5.378e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,987:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.964e+11, tolerance: 5.647e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,987:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.975e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:23,065:INFO:Calculating mean and std
2024-02-04 19:07:23,065:INFO:Creating metrics dataframe
2024-02-04 19:07:23,065:INFO:Uploading results into container
2024-02-04 19:07:23,065:INFO:Uploading model into container now
2024-02-04 19:07:23,065:INFO:_master_model_container: 27
2024-02-04 19:07:23,065:INFO:_display_container: 6
2024-02-04 19:07:23,065:INFO:ElasticNet(random_state=123)
2024-02-04 19:07:23,065:INFO:create_model() successfully completed......................................
2024-02-04 19:07:23,154:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:23,154:INFO:Creating metrics dataframe
2024-02-04 19:07:23,154:INFO:Initializing Least Angle Regression
2024-02-04 19:07:23,154:INFO:Total runtime is 0.05449879964192708 minutes
2024-02-04 19:07:23,167:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:23,167:INFO:Initializing create_model()
2024-02-04 19:07:23,167:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:23,167:INFO:Checking exceptions
2024-02-04 19:07:23,167:INFO:Importing libraries
2024-02-04 19:07:23,167:INFO:Copying training dataset
2024-02-04 19:07:23,167:INFO:Defining folds
2024-02-04 19:07:23,167:INFO:Declaring metric variables
2024-02-04 19:07:23,176:INFO:Importing untrained model
2024-02-04 19:07:23,176:INFO:Least Angle Regression Imported successfully
2024-02-04 19:07:23,176:INFO:Starting cross validation
2024-02-04 19:07:23,187:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:23,456:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.718e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,456:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.851e+02, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,467:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.911e+02, with an active set of 99 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,467:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=3.621e+02, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,467:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.149e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,467:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=1.924e+05, with an active set of 109 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,467:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 160 iterations, i.e. alpha=1.780e+05, with an active set of 112 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,467:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.157e+03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,467:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.555e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,483:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.731e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,483:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.519e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=1.562e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=3.774e+05, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=3.759e+05, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.496e+09, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=8.153e+04, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=5.167e+07, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=3.422e+07, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=2.687e+07, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=2.606e+07, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=2.275e+07, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.908e+07, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=1.562e+07, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=3.936e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.264e+07, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.442e+07, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.173e+07, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.137e+07, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.059e+07, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.412e+02, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=1.029e+07, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=9.833e+06, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=9.103e+06, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=8.950e+06, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=7.986e+06, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=7.476e+06, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=7.475e+06, with an active set of 91 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.243e+06, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.242e+06, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=6.703e+06, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=5.425e+06, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=5.332e+06, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=4.831e+06, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=4.529e+06, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=3.694e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=3.221e+06, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=2.801e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 115 iterations, i.e. alpha=2.370e+06, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.244e+12, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=1.750e+11, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.543e+11, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.062e+11, with an active set of 107 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.027e+11, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=7.904e+10, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=7.308e+10, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=6.501e+10, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=5.857e+10, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=5.713e+10, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=5.810e+10, with an active set of 113 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=5.809e+10, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=3.100e+10, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,514:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=9.162e+13, with an active set of 131 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,514:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=7.581e+13, with an active set of 131 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,514:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.051e+02, with an active set of 72 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,530:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=2.116e+02, with an active set of 116 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,530:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=1.924e+02, with an active set of 116 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,539:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 146 iterations, i.e. alpha=1.674e+02, with an active set of 121 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,539:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=1.569e+02, with an active set of 124 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,544:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=1.388e+03, with an active set of 128 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,544:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=1.232e+03, with an active set of 128 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,544:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=1.900e+02, with an active set of 128 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,544:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=7.168e+00, with an active set of 128 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=3.745e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.029e+02, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.739e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=2.546e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.190e+02, with an active set of 73 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=6.923e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=5.772e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.716e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=5.669e+03, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=2.536e+03, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=1.472e+02, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=3.202e+01, with an active set of 98 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.832e+01, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.695e+01, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=1.943e+01, with an active set of 107 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=1.431e+01, with an active set of 108 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=1.773e+01, with an active set of 111 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=1.219e+01, with an active set of 116 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=1.102e+01, with an active set of 116 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,736:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 144 iterations, i.e. alpha=4.280e+01, with an active set of 125 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,736:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=3.768e+02, with an active set of 128 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,736:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=1.298e+04, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,736:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=7.540e+01, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,792:INFO:Calculating mean and std
2024-02-04 19:07:23,792:INFO:Creating metrics dataframe
2024-02-04 19:07:23,792:INFO:Uploading results into container
2024-02-04 19:07:23,792:INFO:Uploading model into container now
2024-02-04 19:07:23,792:INFO:_master_model_container: 28
2024-02-04 19:07:23,792:INFO:_display_container: 6
2024-02-04 19:07:23,792:INFO:Lars(random_state=123)
2024-02-04 19:07:23,792:INFO:create_model() successfully completed......................................
2024-02-04 19:07:23,871:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:23,871:INFO:Creating metrics dataframe
2024-02-04 19:07:23,887:INFO:Initializing Lasso Least Angle Regression
2024-02-04 19:07:23,887:INFO:Total runtime is 0.0667116363843282 minutes
2024-02-04 19:07:23,895:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:23,895:INFO:Initializing create_model()
2024-02-04 19:07:23,895:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:23,895:INFO:Checking exceptions
2024-02-04 19:07:23,895:INFO:Importing libraries
2024-02-04 19:07:23,895:INFO:Copying training dataset
2024-02-04 19:07:23,895:INFO:Defining folds
2024-02-04 19:07:23,895:INFO:Declaring metric variables
2024-02-04 19:07:23,895:INFO:Importing untrained model
2024-02-04 19:07:23,907:INFO:Lasso Least Angle Regression Imported successfully
2024-02-04 19:07:23,912:INFO:Starting cross validation
2024-02-04 19:07:23,912:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:24,218:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=7.153e+03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,218:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.274e+03, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,218:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.353e+03, previous alpha=1.302e+03, with an active set of 29 regressors.
  warnings.warn(

2024-02-04 19:07:24,234:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=3.340e+02, with an active set of 46 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,234:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.955e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,234:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.603e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,234:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 70 iterations, alpha=2.179e+02, previous alpha=2.179e+02, with an active set of 57 regressors.
  warnings.warn(

2024-02-04 19:07:24,234:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 86 iterations, alpha=1.479e+02, previous alpha=1.479e+02, with an active set of 71 regressors.
  warnings.warn(

2024-02-04 19:07:24,260:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=4.102e+02, with an active set of 45 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,260:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=4.005e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,260:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=3.697e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,260:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.087e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,260:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=1.416e+02, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=4.274e+00, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 146 iterations, i.e. alpha=1.586e+00, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 147 iterations, i.e. alpha=1.586e+00, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 151 iterations, alpha=1.419e+00, previous alpha=1.419e+00, with an active set of 118 regressors.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=8.289e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=8.289e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 101 iterations, alpha=8.102e+01, previous alpha=8.102e+01, with an active set of 80 regressors.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 111 iterations, alpha=7.889e+01, previous alpha=5.351e+01, with an active set of 88 regressors.
  warnings.warn(

2024-02-04 19:07:24,290:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.426e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,290:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.234e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,290:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.522e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,290:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.795e+02, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,306:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=6.685e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,306:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=6.615e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,306:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 103 iterations, alpha=5.927e+01, previous alpha=5.849e+01, with an active set of 82 regressors.
  warnings.warn(

2024-02-04 19:07:24,306:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 140 iterations, alpha=2.414e+01, previous alpha=2.414e+01, with an active set of 95 regressors.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.325e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.167e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.168e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=4.459e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.380e+02, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=9.583e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=9.092e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 89 iterations, alpha=8.936e+01, previous alpha=8.854e+01, with an active set of 78 regressors.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.956e+02, with an active set of 57 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,488:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 129 iterations, alpha=4.342e+01, previous alpha=3.885e+01, with an active set of 96 regressors.
  warnings.warn(

2024-02-04 19:07:24,535:INFO:Calculating mean and std
2024-02-04 19:07:24,535:INFO:Creating metrics dataframe
2024-02-04 19:07:24,535:INFO:Uploading results into container
2024-02-04 19:07:24,535:INFO:Uploading model into container now
2024-02-04 19:07:24,535:INFO:_master_model_container: 29
2024-02-04 19:07:24,535:INFO:_display_container: 6
2024-02-04 19:07:24,535:INFO:LassoLars(random_state=123)
2024-02-04 19:07:24,535:INFO:create_model() successfully completed......................................
2024-02-04 19:07:24,622:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:24,622:INFO:Creating metrics dataframe
2024-02-04 19:07:24,632:INFO:Initializing Orthogonal Matching Pursuit
2024-02-04 19:07:24,632:INFO:Total runtime is 0.0791207194328308 minutes
2024-02-04 19:07:24,632:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:24,632:INFO:Initializing create_model()
2024-02-04 19:07:24,632:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:24,639:INFO:Checking exceptions
2024-02-04 19:07:24,639:INFO:Importing libraries
2024-02-04 19:07:24,639:INFO:Copying training dataset
2024-02-04 19:07:24,642:INFO:Defining folds
2024-02-04 19:07:24,642:INFO:Declaring metric variables
2024-02-04 19:07:24,642:INFO:Importing untrained model
2024-02-04 19:07:24,642:INFO:Orthogonal Matching Pursuit Imported successfully
2024-02-04 19:07:24,656:INFO:Starting cross validation
2024-02-04 19:07:24,656:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:25,232:INFO:Calculating mean and std
2024-02-04 19:07:25,232:INFO:Creating metrics dataframe
2024-02-04 19:07:25,232:INFO:Uploading results into container
2024-02-04 19:07:25,232:INFO:Uploading model into container now
2024-02-04 19:07:25,232:INFO:_master_model_container: 30
2024-02-04 19:07:25,232:INFO:_display_container: 6
2024-02-04 19:07:25,232:INFO:OrthogonalMatchingPursuit()
2024-02-04 19:07:25,232:INFO:create_model() successfully completed......................................
2024-02-04 19:07:25,319:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:25,319:INFO:Creating metrics dataframe
2024-02-04 19:07:25,329:INFO:Initializing Bayesian Ridge
2024-02-04 19:07:25,329:INFO:Total runtime is 0.09075031280517577 minutes
2024-02-04 19:07:25,329:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:25,329:INFO:Initializing create_model()
2024-02-04 19:07:25,329:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:25,329:INFO:Checking exceptions
2024-02-04 19:07:25,329:INFO:Importing libraries
2024-02-04 19:07:25,329:INFO:Copying training dataset
2024-02-04 19:07:25,342:INFO:Defining folds
2024-02-04 19:07:25,342:INFO:Declaring metric variables
2024-02-04 19:07:25,342:INFO:Importing untrained model
2024-02-04 19:07:25,342:INFO:Bayesian Ridge Imported successfully
2024-02-04 19:07:25,342:INFO:Starting cross validation
2024-02-04 19:07:25,356:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:25,975:INFO:Calculating mean and std
2024-02-04 19:07:25,975:INFO:Creating metrics dataframe
2024-02-04 19:07:25,975:INFO:Uploading results into container
2024-02-04 19:07:25,975:INFO:Uploading model into container now
2024-02-04 19:07:25,975:INFO:_master_model_container: 31
2024-02-04 19:07:25,975:INFO:_display_container: 6
2024-02-04 19:07:25,975:INFO:BayesianRidge()
2024-02-04 19:07:25,975:INFO:create_model() successfully completed......................................
2024-02-04 19:07:26,062:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:26,062:INFO:Creating metrics dataframe
2024-02-04 19:07:26,073:INFO:Initializing Passive Aggressive Regressor
2024-02-04 19:07:26,073:INFO:Total runtime is 0.10313917795817057 minutes
2024-02-04 19:07:26,081:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:26,081:INFO:Initializing create_model()
2024-02-04 19:07:26,081:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:26,081:INFO:Checking exceptions
2024-02-04 19:07:26,081:INFO:Importing libraries
2024-02-04 19:07:26,081:INFO:Copying training dataset
2024-02-04 19:07:26,081:INFO:Defining folds
2024-02-04 19:07:26,081:INFO:Declaring metric variables
2024-02-04 19:07:26,090:INFO:Importing untrained model
2024-02-04 19:07:26,091:INFO:Passive Aggressive Regressor Imported successfully
2024-02-04 19:07:26,091:INFO:Starting cross validation
2024-02-04 19:07:26,091:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:26,675:INFO:Calculating mean and std
2024-02-04 19:07:26,675:INFO:Creating metrics dataframe
2024-02-04 19:07:26,675:INFO:Uploading results into container
2024-02-04 19:07:26,675:INFO:Uploading model into container now
2024-02-04 19:07:26,675:INFO:_master_model_container: 32
2024-02-04 19:07:26,675:INFO:_display_container: 6
2024-02-04 19:07:26,675:INFO:PassiveAggressiveRegressor(random_state=123)
2024-02-04 19:07:26,675:INFO:create_model() successfully completed......................................
2024-02-04 19:07:26,759:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:26,759:INFO:Creating metrics dataframe
2024-02-04 19:07:26,778:INFO:Initializing Huber Regressor
2024-02-04 19:07:26,778:INFO:Total runtime is 0.11488995552062987 minutes
2024-02-04 19:07:26,780:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:26,780:INFO:Initializing create_model()
2024-02-04 19:07:26,780:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:26,780:INFO:Checking exceptions
2024-02-04 19:07:26,780:INFO:Importing libraries
2024-02-04 19:07:26,780:INFO:Copying training dataset
2024-02-04 19:07:26,780:INFO:Defining folds
2024-02-04 19:07:26,780:INFO:Declaring metric variables
2024-02-04 19:07:26,780:INFO:Importing untrained model
2024-02-04 19:07:26,792:INFO:Huber Regressor Imported successfully
2024-02-04 19:07:26,792:INFO:Starting cross validation
2024-02-04 19:07:26,792:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:27,152:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,173:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,207:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,223:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,223:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,238:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,254:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,254:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,466:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,466:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,522:INFO:Calculating mean and std
2024-02-04 19:07:27,522:INFO:Creating metrics dataframe
2024-02-04 19:07:27,528:INFO:Uploading results into container
2024-02-04 19:07:27,528:INFO:Uploading model into container now
2024-02-04 19:07:27,528:INFO:_master_model_container: 33
2024-02-04 19:07:27,528:INFO:_display_container: 6
2024-02-04 19:07:27,528:INFO:HuberRegressor()
2024-02-04 19:07:27,528:INFO:create_model() successfully completed......................................
2024-02-04 19:07:27,618:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:27,618:INFO:Creating metrics dataframe
2024-02-04 19:07:27,618:INFO:Initializing K Neighbors Regressor
2024-02-04 19:07:27,618:INFO:Total runtime is 0.1288971940676371 minutes
2024-02-04 19:07:27,618:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:27,618:INFO:Initializing create_model()
2024-02-04 19:07:27,618:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:27,618:INFO:Checking exceptions
2024-02-04 19:07:27,618:INFO:Importing libraries
2024-02-04 19:07:27,618:INFO:Copying training dataset
2024-02-04 19:07:27,633:INFO:Defining folds
2024-02-04 19:07:27,633:INFO:Declaring metric variables
2024-02-04 19:07:27,637:INFO:Importing untrained model
2024-02-04 19:07:27,637:INFO:K Neighbors Regressor Imported successfully
2024-02-04 19:07:27,643:INFO:Starting cross validation
2024-02-04 19:07:27,643:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:28,230:INFO:Calculating mean and std
2024-02-04 19:07:28,230:INFO:Creating metrics dataframe
2024-02-04 19:07:28,230:INFO:Uploading results into container
2024-02-04 19:07:28,230:INFO:Uploading model into container now
2024-02-04 19:07:28,230:INFO:_master_model_container: 34
2024-02-04 19:07:28,230:INFO:_display_container: 6
2024-02-04 19:07:28,230:INFO:KNeighborsRegressor(n_jobs=-1)
2024-02-04 19:07:28,230:INFO:create_model() successfully completed......................................
2024-02-04 19:07:28,319:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:28,319:INFO:Creating metrics dataframe
2024-02-04 19:07:28,329:INFO:Initializing Decision Tree Regressor
2024-02-04 19:07:28,329:INFO:Total runtime is 0.14074541727701823 minutes
2024-02-04 19:07:28,329:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:28,329:INFO:Initializing create_model()
2024-02-04 19:07:28,329:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:28,329:INFO:Checking exceptions
2024-02-04 19:07:28,329:INFO:Importing libraries
2024-02-04 19:07:28,329:INFO:Copying training dataset
2024-02-04 19:07:28,344:INFO:Defining folds
2024-02-04 19:07:28,345:INFO:Declaring metric variables
2024-02-04 19:07:28,345:INFO:Importing untrained model
2024-02-04 19:07:28,345:INFO:Decision Tree Regressor Imported successfully
2024-02-04 19:07:28,356:INFO:Starting cross validation
2024-02-04 19:07:28,356:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:28,951:INFO:Calculating mean and std
2024-02-04 19:07:28,951:INFO:Creating metrics dataframe
2024-02-04 19:07:28,951:INFO:Uploading results into container
2024-02-04 19:07:28,951:INFO:Uploading model into container now
2024-02-04 19:07:28,951:INFO:_master_model_container: 35
2024-02-04 19:07:28,951:INFO:_display_container: 6
2024-02-04 19:07:28,951:INFO:DecisionTreeRegressor(random_state=123)
2024-02-04 19:07:28,951:INFO:create_model() successfully completed......................................
2024-02-04 19:07:29,038:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:29,038:INFO:Creating metrics dataframe
2024-02-04 19:07:29,038:INFO:Initializing Random Forest Regressor
2024-02-04 19:07:29,038:INFO:Total runtime is 0.15255572001139323 minutes
2024-02-04 19:07:29,055:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:29,055:INFO:Initializing create_model()
2024-02-04 19:07:29,055:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:29,055:INFO:Checking exceptions
2024-02-04 19:07:29,055:INFO:Importing libraries
2024-02-04 19:07:29,056:INFO:Copying training dataset
2024-02-04 19:07:29,062:INFO:Defining folds
2024-02-04 19:07:29,062:INFO:Declaring metric variables
2024-02-04 19:07:29,062:INFO:Importing untrained model
2024-02-04 19:07:29,062:INFO:Random Forest Regressor Imported successfully
2024-02-04 19:07:29,072:INFO:Starting cross validation
2024-02-04 19:07:29,076:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:32,585:INFO:Calculating mean and std
2024-02-04 19:07:32,585:INFO:Creating metrics dataframe
2024-02-04 19:07:32,585:INFO:Uploading results into container
2024-02-04 19:07:32,585:INFO:Uploading model into container now
2024-02-04 19:07:32,585:INFO:_master_model_container: 36
2024-02-04 19:07:32,585:INFO:_display_container: 6
2024-02-04 19:07:32,585:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:07:32,585:INFO:create_model() successfully completed......................................
2024-02-04 19:07:32,672:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:32,672:INFO:Creating metrics dataframe
2024-02-04 19:07:32,684:INFO:Initializing Extra Trees Regressor
2024-02-04 19:07:32,684:INFO:Total runtime is 0.21332013607025146 minutes
2024-02-04 19:07:32,693:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:32,693:INFO:Initializing create_model()
2024-02-04 19:07:32,693:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:32,693:INFO:Checking exceptions
2024-02-04 19:07:32,693:INFO:Importing libraries
2024-02-04 19:07:32,693:INFO:Copying training dataset
2024-02-04 19:07:32,695:INFO:Defining folds
2024-02-04 19:07:32,695:INFO:Declaring metric variables
2024-02-04 19:07:32,695:INFO:Importing untrained model
2024-02-04 19:07:32,695:INFO:Extra Trees Regressor Imported successfully
2024-02-04 19:07:32,711:INFO:Starting cross validation
2024-02-04 19:07:32,711:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:35,182:INFO:Calculating mean and std
2024-02-04 19:07:35,182:INFO:Creating metrics dataframe
2024-02-04 19:07:35,189:INFO:Uploading results into container
2024-02-04 19:07:35,190:INFO:Uploading model into container now
2024-02-04 19:07:35,190:INFO:_master_model_container: 37
2024-02-04 19:07:35,190:INFO:_display_container: 6
2024-02-04 19:07:35,190:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:07:35,190:INFO:create_model() successfully completed......................................
2024-02-04 19:07:35,269:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:35,269:INFO:Creating metrics dataframe
2024-02-04 19:07:35,284:INFO:Initializing AdaBoost Regressor
2024-02-04 19:07:35,284:INFO:Total runtime is 0.2566667358080546 minutes
2024-02-04 19:07:35,293:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:35,293:INFO:Initializing create_model()
2024-02-04 19:07:35,293:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:35,293:INFO:Checking exceptions
2024-02-04 19:07:35,293:INFO:Importing libraries
2024-02-04 19:07:35,293:INFO:Copying training dataset
2024-02-04 19:07:35,298:INFO:Defining folds
2024-02-04 19:07:35,298:INFO:Declaring metric variables
2024-02-04 19:07:35,298:INFO:Importing untrained model
2024-02-04 19:07:35,298:INFO:AdaBoost Regressor Imported successfully
2024-02-04 19:07:35,306:INFO:Starting cross validation
2024-02-04 19:07:35,315:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:36,452:INFO:Calculating mean and std
2024-02-04 19:07:36,452:INFO:Creating metrics dataframe
2024-02-04 19:07:36,452:INFO:Uploading results into container
2024-02-04 19:07:36,452:INFO:Uploading model into container now
2024-02-04 19:07:36,452:INFO:_master_model_container: 38
2024-02-04 19:07:36,452:INFO:_display_container: 6
2024-02-04 19:07:36,452:INFO:AdaBoostRegressor(random_state=123)
2024-02-04 19:07:36,452:INFO:create_model() successfully completed......................................
2024-02-04 19:07:36,537:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:36,537:INFO:Creating metrics dataframe
2024-02-04 19:07:36,550:INFO:Initializing Gradient Boosting Regressor
2024-02-04 19:07:36,550:INFO:Total runtime is 0.2777679204940796 minutes
2024-02-04 19:07:36,560:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:36,560:INFO:Initializing create_model()
2024-02-04 19:07:36,560:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:36,560:INFO:Checking exceptions
2024-02-04 19:07:36,560:INFO:Importing libraries
2024-02-04 19:07:36,560:INFO:Copying training dataset
2024-02-04 19:07:36,560:INFO:Defining folds
2024-02-04 19:07:36,560:INFO:Declaring metric variables
2024-02-04 19:07:36,560:INFO:Importing untrained model
2024-02-04 19:07:36,572:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:07:36,582:INFO:Starting cross validation
2024-02-04 19:07:36,587:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:38,301:INFO:Calculating mean and std
2024-02-04 19:07:38,301:INFO:Creating metrics dataframe
2024-02-04 19:07:38,301:INFO:Uploading results into container
2024-02-04 19:07:38,301:INFO:Uploading model into container now
2024-02-04 19:07:38,301:INFO:_master_model_container: 39
2024-02-04 19:07:38,301:INFO:_display_container: 6
2024-02-04 19:07:38,301:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:07:38,301:INFO:create_model() successfully completed......................................
2024-02-04 19:07:38,386:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:38,386:INFO:Creating metrics dataframe
2024-02-04 19:07:38,402:INFO:Initializing Light Gradient Boosting Machine
2024-02-04 19:07:38,402:INFO:Total runtime is 0.30862969160079956 minutes
2024-02-04 19:07:38,402:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:38,402:INFO:Initializing create_model()
2024-02-04 19:07:38,402:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:38,402:INFO:Checking exceptions
2024-02-04 19:07:38,402:INFO:Importing libraries
2024-02-04 19:07:38,402:INFO:Copying training dataset
2024-02-04 19:07:38,410:INFO:Defining folds
2024-02-04 19:07:38,410:INFO:Declaring metric variables
2024-02-04 19:07:38,410:INFO:Importing untrained model
2024-02-04 19:07:38,417:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-04 19:07:38,422:INFO:Starting cross validation
2024-02-04 19:07:38,422:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:39,867:INFO:Calculating mean and std
2024-02-04 19:07:39,867:INFO:Creating metrics dataframe
2024-02-04 19:07:39,872:INFO:Uploading results into container
2024-02-04 19:07:39,872:INFO:Uploading model into container now
2024-02-04 19:07:39,872:INFO:_master_model_container: 40
2024-02-04 19:07:39,872:INFO:_display_container: 6
2024-02-04 19:07:39,872:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:07:39,872:INFO:create_model() successfully completed......................................
2024-02-04 19:07:39,979:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:39,979:INFO:Creating metrics dataframe
2024-02-04 19:07:39,992:INFO:Initializing Dummy Regressor
2024-02-04 19:07:39,992:INFO:Total runtime is 0.33513397773106895 minutes
2024-02-04 19:07:39,992:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:39,992:INFO:Initializing create_model()
2024-02-04 19:07:39,992:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:39,992:INFO:Checking exceptions
2024-02-04 19:07:39,992:INFO:Importing libraries
2024-02-04 19:07:39,992:INFO:Copying training dataset
2024-02-04 19:07:40,002:INFO:Defining folds
2024-02-04 19:07:40,002:INFO:Declaring metric variables
2024-02-04 19:07:40,002:INFO:Importing untrained model
2024-02-04 19:07:40,008:INFO:Dummy Regressor Imported successfully
2024-02-04 19:07:40,008:INFO:Starting cross validation
2024-02-04 19:07:40,008:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:40,586:INFO:Calculating mean and std
2024-02-04 19:07:40,586:INFO:Creating metrics dataframe
2024-02-04 19:07:40,586:INFO:Uploading results into container
2024-02-04 19:07:40,586:INFO:Uploading model into container now
2024-02-04 19:07:40,586:INFO:_master_model_container: 41
2024-02-04 19:07:40,586:INFO:_display_container: 6
2024-02-04 19:07:40,586:INFO:DummyRegressor()
2024-02-04 19:07:40,586:INFO:create_model() successfully completed......................................
2024-02-04 19:07:40,672:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:40,672:INFO:Creating metrics dataframe
2024-02-04 19:07:40,692:INFO:Initializing create_model()
2024-02-04 19:07:40,692:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:40,692:INFO:Checking exceptions
2024-02-04 19:07:40,692:INFO:Importing libraries
2024-02-04 19:07:40,692:INFO:Copying training dataset
2024-02-04 19:07:40,692:INFO:Defining folds
2024-02-04 19:07:40,692:INFO:Declaring metric variables
2024-02-04 19:07:40,692:INFO:Importing untrained model
2024-02-04 19:07:40,692:INFO:Declaring custom model
2024-02-04 19:07:40,692:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:07:40,703:INFO:Cross validation set to False
2024-02-04 19:07:40,703:INFO:Fitting Model
2024-02-04 19:07:41,234:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:07:41,234:INFO:create_model() successfully completed......................................
2024-02-04 19:07:41,324:INFO:Creating Dashboard logs
2024-02-04 19:07:41,324:INFO:Model: Gradient Boosting Regressor
2024-02-04 19:07:41,354:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-04 19:07:41,405:INFO:Initializing predict_model()
2024-02-04 19:07:41,405:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECCF1FD790>)
2024-02-04 19:07:41,405:INFO:Checking exceptions
2024-02-04 19:07:41,405:INFO:Preloading libraries
2024-02-04 19:07:41,785:INFO:Initializing create_model()
2024-02-04 19:07:41,785:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:41,785:INFO:Checking exceptions
2024-02-04 19:07:41,785:INFO:Importing libraries
2024-02-04 19:07:41,785:INFO:Copying training dataset
2024-02-04 19:07:41,800:INFO:Defining folds
2024-02-04 19:07:41,800:INFO:Declaring metric variables
2024-02-04 19:07:41,800:INFO:Importing untrained model
2024-02-04 19:07:41,800:INFO:Declaring custom model
2024-02-04 19:07:41,800:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-04 19:07:41,805:INFO:Cross validation set to False
2024-02-04 19:07:41,805:INFO:Fitting Model
2024-02-04 19:07:41,923:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-02-04 19:07:41,936:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000851 seconds.
2024-02-04 19:07:41,936:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-04 19:07:41,936:INFO:[LightGBM] [Info] Total Bins 2881
2024-02-04 19:07:41,936:INFO:[LightGBM] [Info] Number of data points in the train set: 1019, number of used features: 93
2024-02-04 19:07:41,936:INFO:[LightGBM] [Info] Start training from score 180679.625123
2024-02-04 19:07:42,005:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:07:42,005:INFO:create_model() successfully completed......................................
2024-02-04 19:07:42,108:INFO:Creating Dashboard logs
2024-02-04 19:07:42,124:INFO:Model: Light Gradient Boosting Machine
2024-02-04 19:07:42,141:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2024-02-04 19:07:42,188:INFO:Initializing predict_model()
2024-02-04 19:07:42,188:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECCF1FD790>)
2024-02-04 19:07:42,188:INFO:Checking exceptions
2024-02-04 19:07:42,188:INFO:Preloading libraries
2024-02-04 19:07:42,623:INFO:Initializing create_model()
2024-02-04 19:07:42,623:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:42,623:INFO:Checking exceptions
2024-02-04 19:07:42,623:INFO:Importing libraries
2024-02-04 19:07:42,623:INFO:Copying training dataset
2024-02-04 19:07:42,639:INFO:Defining folds
2024-02-04 19:07:42,639:INFO:Declaring metric variables
2024-02-04 19:07:42,639:INFO:Importing untrained model
2024-02-04 19:07:42,639:INFO:Declaring custom model
2024-02-04 19:07:42,639:INFO:Extra Trees Regressor Imported successfully
2024-02-04 19:07:42,639:INFO:Cross validation set to False
2024-02-04 19:07:42,639:INFO:Fitting Model
2024-02-04 19:07:43,004:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:07:43,004:INFO:create_model() successfully completed......................................
2024-02-04 19:07:43,083:INFO:Creating Dashboard logs
2024-02-04 19:07:43,101:INFO:Model: Extra Trees Regressor
2024-02-04 19:07:43,125:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-02-04 19:07:43,179:INFO:Initializing predict_model()
2024-02-04 19:07:43,179:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECCF4783A0>)
2024-02-04 19:07:43,179:INFO:Checking exceptions
2024-02-04 19:07:43,179:INFO:Preloading libraries
2024-02-04 19:07:43,606:INFO:Creating Dashboard logs
2024-02-04 19:07:43,606:INFO:Model: Random Forest Regressor
2024-02-04 19:07:43,620:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-02-04 19:07:43,785:INFO:Creating Dashboard logs
2024-02-04 19:07:43,785:INFO:Model: Bayesian Ridge
2024-02-04 19:07:43,817:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'n_iter': 300, 'tol': 0.001, 'verbose': False}
2024-02-04 19:07:43,973:INFO:Creating Dashboard logs
2024-02-04 19:07:43,973:INFO:Model: Lasso Least Angle Regression
2024-02-04 19:07:43,995:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'max_iter': 500, 'normalize': 'deprecated', 'positive': False, 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2024-02-04 19:07:44,206:INFO:Creating Dashboard logs
2024-02-04 19:07:44,208:INFO:Model: Ridge Regression
2024-02-04 19:07:44,222:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}
2024-02-04 19:07:44,389:INFO:Creating Dashboard logs
2024-02-04 19:07:44,389:INFO:Model: Elastic Net
2024-02-04 19:07:44,410:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2024-02-04 19:07:44,567:INFO:Creating Dashboard logs
2024-02-04 19:07:44,567:INFO:Model: Lasso Regression
2024-02-04 19:07:44,597:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2024-02-04 19:07:44,740:INFO:Creating Dashboard logs
2024-02-04 19:07:44,756:INFO:Model: AdaBoost Regressor
2024-02-04 19:07:44,772:INFO:Logged params: {'base_estimator': 'deprecated', 'estimator': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50, 'random_state': 123}
2024-02-04 19:07:44,929:INFO:Creating Dashboard logs
2024-02-04 19:07:44,929:INFO:Model: Orthogonal Matching Pursuit
2024-02-04 19:07:44,943:INFO:Logged params: {'fit_intercept': True, 'n_nonzero_coefs': None, 'normalize': 'deprecated', 'precompute': 'auto', 'tol': None}
2024-02-04 19:07:45,104:INFO:Creating Dashboard logs
2024-02-04 19:07:45,108:INFO:Model: Decision Tree Regressor
2024-02-04 19:07:45,120:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 123, 'splitter': 'best'}
2024-02-04 19:07:45,285:INFO:Creating Dashboard logs
2024-02-04 19:07:45,285:INFO:Model: Huber Regressor
2024-02-04 19:07:45,308:INFO:Logged params: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': True, 'max_iter': 100, 'tol': 1e-05, 'warm_start': False}
2024-02-04 19:07:45,460:INFO:Creating Dashboard logs
2024-02-04 19:07:45,460:INFO:Model: K Neighbors Regressor
2024-02-04 19:07:45,489:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2024-02-04 19:07:45,638:INFO:Creating Dashboard logs
2024-02-04 19:07:45,638:INFO:Model: Passive Aggressive Regressor
2024-02-04 19:07:45,669:INFO:Logged params: {'C': 1.0, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'fit_intercept': True, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'n_iter_no_change': 5, 'random_state': 123, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-04 19:07:45,821:INFO:Creating Dashboard logs
2024-02-04 19:07:45,837:INFO:Model: Dummy Regressor
2024-02-04 19:07:45,852:INFO:Logged params: {'constant': None, 'quantile': None, 'strategy': 'mean'}
2024-02-04 19:07:46,009:INFO:Creating Dashboard logs
2024-02-04 19:07:46,009:INFO:Model: Linear Regression
2024-02-04 19:07:46,034:INFO:Logged params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'positive': False}
2024-02-04 19:07:46,194:INFO:Creating Dashboard logs
2024-02-04 19:07:46,194:INFO:Model: Least Angle Regression
2024-02-04 19:07:46,224:INFO:Logged params: {'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'n_nonzero_coefs': 500, 'normalize': 'deprecated', 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2024-02-04 19:07:46,420:INFO:_master_model_container: 41
2024-02-04 19:07:46,423:INFO:_display_container: 6
2024-02-04 19:07:46,423:INFO:[GradientBoostingRegressor(random_state=123), LGBMRegressor(n_jobs=-1, random_state=123), ExtraTreesRegressor(n_jobs=-1, random_state=123)]
2024-02-04 19:07:46,423:INFO:compare_models() successfully completed......................................
2024-02-04 19:09:26,980:INFO:Initializing tune_model()
2024-02-04 19:09:26,980:INFO:tune_model(estimator=GradientBoostingRegressor(random_state=123), fold=None, round=4, n_iter=10, custom_grid=None, optimize=R2, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>)
2024-02-04 19:09:26,980:INFO:Checking exceptions
2024-02-04 19:09:26,996:INFO:Copying training dataset
2024-02-04 19:09:26,999:INFO:Checking base model
2024-02-04 19:09:26,999:INFO:Base model : Gradient Boosting Regressor
2024-02-04 19:09:27,004:INFO:Declaring metric variables
2024-02-04 19:09:27,004:INFO:Defining Hyperparameters
2024-02-04 19:09:27,127:INFO:Tuning with n_jobs=-1
2024-02-04 19:09:27,127:INFO:Initializing RandomizedSearchCV
2024-02-04 19:09:40,626:INFO:best_params: {'actual_estimator__subsample': 0.85, 'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 5, 'actual_estimator__min_samples_leaf': 5, 'actual_estimator__min_impurity_decrease': 0.02, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 7, 'actual_estimator__learning_rate': 0.15}
2024-02-04 19:09:40,626:INFO:Hyperparameter search completed
2024-02-04 19:09:40,626:INFO:SubProcess create_model() called ==================================
2024-02-04 19:09:40,626:INFO:Initializing create_model()
2024-02-04 19:09:40,626:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECCF4DDD30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.85, 'n_estimators': 230, 'min_samples_split': 5, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.02, 'max_features': 1.0, 'max_depth': 7, 'learning_rate': 0.15})
2024-02-04 19:09:40,626:INFO:Checking exceptions
2024-02-04 19:09:40,626:INFO:Importing libraries
2024-02-04 19:09:40,626:INFO:Copying training dataset
2024-02-04 19:09:40,632:INFO:Defining folds
2024-02-04 19:09:40,632:INFO:Declaring metric variables
2024-02-04 19:09:40,636:INFO:Importing untrained model
2024-02-04 19:09:40,636:INFO:Declaring custom model
2024-02-04 19:09:40,636:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:09:40,636:INFO:Starting cross validation
2024-02-04 19:09:40,636:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:09:45,894:INFO:Calculating mean and std
2024-02-04 19:09:45,894:INFO:Creating metrics dataframe
2024-02-04 19:09:45,898:INFO:Finalizing model
2024-02-04 19:09:47,828:INFO:Uploading results into container
2024-02-04 19:09:47,838:INFO:Uploading model into container now
2024-02-04 19:09:47,838:INFO:_master_model_container: 42
2024-02-04 19:09:47,838:INFO:_display_container: 7
2024-02-04 19:09:47,840:INFO:GradientBoostingRegressor(learning_rate=0.15, max_depth=7, max_features=1.0,
                          min_impurity_decrease=0.02, min_samples_leaf=5,
                          min_samples_split=5, n_estimators=230,
                          random_state=123, subsample=0.85)
2024-02-04 19:09:47,840:INFO:create_model() successfully completed......................................
2024-02-04 19:09:47,933:INFO:SubProcess create_model() end ==================================
2024-02-04 19:09:47,933:INFO:choose_better activated
2024-02-04 19:09:47,933:INFO:SubProcess create_model() called ==================================
2024-02-04 19:09:47,945:INFO:Initializing create_model()
2024-02-04 19:09:47,945:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:09:47,945:INFO:Checking exceptions
2024-02-04 19:09:47,945:INFO:Importing libraries
2024-02-04 19:09:47,945:INFO:Copying training dataset
2024-02-04 19:09:47,945:INFO:Defining folds
2024-02-04 19:09:47,945:INFO:Declaring metric variables
2024-02-04 19:09:47,945:INFO:Importing untrained model
2024-02-04 19:09:47,945:INFO:Declaring custom model
2024-02-04 19:09:47,945:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:09:47,945:INFO:Starting cross validation
2024-02-04 19:09:47,945:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:09:49,546:INFO:Calculating mean and std
2024-02-04 19:09:49,546:INFO:Creating metrics dataframe
2024-02-04 19:09:49,546:INFO:Finalizing model
2024-02-04 19:09:50,064:INFO:Uploading results into container
2024-02-04 19:09:50,064:INFO:Uploading model into container now
2024-02-04 19:09:50,064:INFO:_master_model_container: 43
2024-02-04 19:09:50,064:INFO:_display_container: 8
2024-02-04 19:09:50,064:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:09:50,064:INFO:create_model() successfully completed......................................
2024-02-04 19:09:50,150:INFO:SubProcess create_model() end ==================================
2024-02-04 19:09:50,150:INFO:GradientBoostingRegressor(random_state=123) result for R2 is 0.8969
2024-02-04 19:09:50,150:INFO:GradientBoostingRegressor(learning_rate=0.15, max_depth=7, max_features=1.0,
                          min_impurity_decrease=0.02, min_samples_leaf=5,
                          min_samples_split=5, n_estimators=230,
                          random_state=123, subsample=0.85) result for R2 is 0.8931
2024-02-04 19:09:50,150:INFO:GradientBoostingRegressor(random_state=123) is best model
2024-02-04 19:09:50,150:INFO:choose_better completed
2024-02-04 19:09:50,150:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-02-04 19:09:50,150:INFO:Creating Dashboard logs
2024-02-04 19:09:50,150:INFO:Model: Gradient Boosting Regressor
2024-02-04 19:09:50,166:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-04 19:09:50,240:INFO:Initializing predict_model()
2024-02-04 19:09:50,240:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECD072F0D0>)
2024-02-04 19:09:50,240:INFO:Checking exceptions
2024-02-04 19:09:50,240:INFO:Preloading libraries
2024-02-04 19:09:50,637:INFO:_master_model_container: 43
2024-02-04 19:09:50,637:INFO:_display_container: 7
2024-02-04 19:09:50,637:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:09:50,637:INFO:tune_model() successfully completed......................................
2024-02-04 19:10:07,528:INFO:Initializing ensemble_model()
2024-02-04 19:10:07,528:INFO:ensemble_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), method=Bagging, fold=None, n_estimators=50, round=4, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-04 19:10:07,528:INFO:Checking exceptions
2024-02-04 19:10:07,544:INFO:Importing libraries
2024-02-04 19:10:07,544:INFO:Copying training dataset
2024-02-04 19:10:07,544:INFO:Checking base model
2024-02-04 19:10:07,544:INFO:Base model : Gradient Boosting Regressor
2024-02-04 19:10:07,549:INFO:Importing untrained ensembler
2024-02-04 19:10:07,549:INFO:Ensemble method set to Bagging
2024-02-04 19:10:07,549:INFO:SubProcess create_model() called ==================================
2024-02-04 19:10:07,549:INFO:Initializing create_model()
2024-02-04 19:10:07,549:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA7665970>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:10:07,549:INFO:Checking exceptions
2024-02-04 19:10:07,549:INFO:Importing libraries
2024-02-04 19:10:07,549:INFO:Copying training dataset
2024-02-04 19:10:07,549:INFO:Defining folds
2024-02-04 19:10:07,549:INFO:Declaring metric variables
2024-02-04 19:10:07,557:INFO:Importing untrained model
2024-02-04 19:10:07,557:INFO:Declaring custom model
2024-02-04 19:10:07,561:INFO:Bagging Regressor Imported successfully
2024-02-04 19:10:07,565:INFO:Starting cross validation
2024-02-04 19:10:07,565:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:10:46,935:INFO:Calculating mean and std
2024-02-04 19:10:46,936:INFO:Creating metrics dataframe
2024-02-04 19:10:46,940:INFO:Finalizing model
2024-02-04 19:11:01,603:INFO:Uploading results into container
2024-02-04 19:11:01,604:INFO:Uploading model into container now
2024-02-04 19:11:01,604:INFO:_master_model_container: 44
2024-02-04 19:11:01,605:INFO:_display_container: 8
2024-02-04 19:11:01,605:INFO:BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123)
2024-02-04 19:11:01,605:INFO:create_model() successfully completed......................................
2024-02-04 19:11:01,764:INFO:SubProcess create_model() end ==================================
2024-02-04 19:11:01,765:INFO:Creating Dashboard logs
2024-02-04 19:11:01,768:INFO:Model: Bagging Regressor
2024-02-04 19:11:01,790:INFO:Logged params: {'base_estimator': 'deprecated', 'bootstrap': True, 'bootstrap_features': False, 'estimator__alpha': 0.9, 'estimator__ccp_alpha': 0.0, 'estimator__criterion': 'friedman_mse', 'estimator__init': None, 'estimator__learning_rate': 0.1, 'estimator__loss': 'squared_error', 'estimator__max_depth': 3, 'estimator__max_features': None, 'estimator__max_leaf_nodes': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__n_estimators': 100, 'estimator__n_iter_no_change': None, 'estimator__random_state': 123, 'estimator__subsample': 1.0, 'estimator__tol': 0.0001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': GradientBoostingRegressor(random_state=123), 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-02-04 19:11:01,869:INFO:Initializing predict_model()
2024-02-04 19:11:01,869:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECD06CC1F0>)
2024-02-04 19:11:01,869:INFO:Checking exceptions
2024-02-04 19:11:01,869:INFO:Preloading libraries
2024-02-04 19:11:02,467:INFO:_master_model_container: 44
2024-02-04 19:11:02,467:INFO:_display_container: 8
2024-02-04 19:11:02,467:INFO:BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123)
2024-02-04 19:11:02,467:INFO:ensemble_model() successfully completed......................................
2024-02-04 19:11:02,627:INFO:Initializing ensemble_model()
2024-02-04 19:11:02,627:INFO:ensemble_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), method=Boosting, fold=None, n_estimators=10, round=4, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-04 19:11:02,627:INFO:Checking exceptions
2024-02-04 19:11:06,539:INFO:Importing libraries
2024-02-04 19:11:06,539:INFO:Copying training dataset
2024-02-04 19:11:06,539:INFO:Checking base model
2024-02-04 19:11:06,539:INFO:Base model : Gradient Boosting Regressor
2024-02-04 19:11:06,547:INFO:Importing untrained ensembler
2024-02-04 19:11:06,547:INFO:Ensemble method set to Boosting
2024-02-04 19:11:06,547:INFO:SubProcess create_model() called ==================================
2024-02-04 19:11:06,548:INFO:Initializing create_model()
2024-02-04 19:11:06,548:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=AdaBoostRegressor(estimator=GradientBoostingRegressor(random_state=123),
                  n_estimators=10, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECCEBFC9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:11:06,548:INFO:Checking exceptions
2024-02-04 19:11:06,548:INFO:Importing libraries
2024-02-04 19:11:06,548:INFO:Copying training dataset
2024-02-04 19:11:06,552:INFO:Defining folds
2024-02-04 19:11:06,552:INFO:Declaring metric variables
2024-02-04 19:11:06,555:INFO:Importing untrained model
2024-02-04 19:11:06,555:INFO:Declaring custom model
2024-02-04 19:11:06,559:INFO:AdaBoost Regressor Imported successfully
2024-02-04 19:11:06,567:INFO:Starting cross validation
2024-02-04 19:11:06,569:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:11:17,308:INFO:Calculating mean and std
2024-02-04 19:11:17,308:INFO:Creating metrics dataframe
2024-02-04 19:11:17,313:INFO:Finalizing model
2024-02-04 19:11:20,997:INFO:Uploading results into container
2024-02-04 19:11:20,997:INFO:Uploading model into container now
2024-02-04 19:11:20,997:INFO:_master_model_container: 45
2024-02-04 19:11:20,997:INFO:_display_container: 9
2024-02-04 19:11:20,998:INFO:AdaBoostRegressor(estimator=GradientBoostingRegressor(random_state=123),
                  n_estimators=10, random_state=123)
2024-02-04 19:11:20,998:INFO:create_model() successfully completed......................................
2024-02-04 19:11:21,091:INFO:SubProcess create_model() end ==================================
2024-02-04 19:11:21,091:INFO:Creating Dashboard logs
2024-02-04 19:11:21,091:INFO:Model: AdaBoost Regressor
2024-02-04 19:11:21,106:INFO:Logged params: {'base_estimator': 'deprecated', 'estimator__alpha': 0.9, 'estimator__ccp_alpha': 0.0, 'estimator__criterion': 'friedman_mse', 'estimator__init': None, 'estimator__learning_rate': 0.1, 'estimator__loss': 'squared_error', 'estimator__max_depth': 3, 'estimator__max_features': None, 'estimator__max_leaf_nodes': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__n_estimators': 100, 'estimator__n_iter_no_change': None, 'estimator__random_state': 123, 'estimator__subsample': 1.0, 'estimator__tol': 0.0001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': GradientBoostingRegressor(random_state=123), 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 10, 'random_state': 123}
2024-02-04 19:11:21,176:INFO:Initializing predict_model()
2024-02-04 19:11:21,176:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=AdaBoostRegressor(estimator=GradientBoostingRegressor(random_state=123),
                  n_estimators=10, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECD072A3A0>)
2024-02-04 19:11:21,176:INFO:Checking exceptions
2024-02-04 19:11:21,176:INFO:Preloading libraries
2024-02-04 19:11:21,578:INFO:_master_model_container: 45
2024-02-04 19:11:21,578:INFO:_display_container: 9
2024-02-04 19:11:21,578:INFO:AdaBoostRegressor(estimator=GradientBoostingRegressor(random_state=123),
                  n_estimators=10, random_state=123)
2024-02-04 19:11:21,578:INFO:ensemble_model() successfully completed......................................
2024-02-04 19:11:27,894:INFO:Initializing blend_models()
2024-02-04 19:11:27,894:INFO:blend_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator_list=[GradientBoostingRegressor(random_state=123), LGBMRegressor(n_jobs=-1, random_state=123), ExtraTreesRegressor(n_jobs=-1, random_state=123)], fold=None, round=4, choose_better=False, optimize=R2, method=auto, weights=None, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-04 19:11:27,894:INFO:Checking exceptions
2024-02-04 19:11:27,909:INFO:Importing libraries
2024-02-04 19:11:27,910:INFO:Copying training dataset
2024-02-04 19:11:27,910:INFO:Getting model names
2024-02-04 19:11:27,910:INFO:SubProcess create_model() called ==================================
2024-02-04 19:11:27,917:INFO:Initializing create_model()
2024-02-04 19:11:27,917:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=VotingRegressor(estimators=[('Gradient Boosting Regressor',
                             GradientBoostingRegressor(random_state=123)),
                            ('Light Gradient Boosting Machine',
                             LGBMRegressor(n_jobs=-1, random_state=123)),
                            ('Extra Trees Regressor',
                             ExtraTreesRegressor(n_jobs=-1, random_state=123))],
                n_jobs=-1), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECCF2D8880>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:11:27,917:INFO:Checking exceptions
2024-02-04 19:11:27,917:INFO:Importing libraries
2024-02-04 19:11:27,917:INFO:Copying training dataset
2024-02-04 19:11:27,917:INFO:Defining folds
2024-02-04 19:11:27,917:INFO:Declaring metric variables
2024-02-04 19:11:27,925:INFO:Importing untrained model
2024-02-04 19:11:27,925:INFO:Declaring custom model
2024-02-04 19:11:27,927:INFO:Voting Regressor Imported successfully
2024-02-04 19:11:27,936:INFO:Starting cross validation
2024-02-04 19:11:27,941:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:11:32,560:INFO:Calculating mean and std
2024-02-04 19:11:32,561:INFO:Creating metrics dataframe
2024-02-04 19:11:32,561:INFO:Finalizing model
2024-02-04 19:11:33,527:INFO:Uploading results into container
2024-02-04 19:11:33,527:INFO:Uploading model into container now
2024-02-04 19:11:33,527:INFO:_master_model_container: 46
2024-02-04 19:11:33,527:INFO:_display_container: 10
2024-02-04 19:11:33,530:INFO:VotingRegressor(estimators=[('Gradient Boosting Regressor',
                             GradientBoostingRegressor(random_state=123)),
                            ('Light Gradient Boosting Machine',
                             LGBMRegressor(n_jobs=-1, random_state=123)),
                            ('Extra Trees Regressor',
                             ExtraTreesRegressor(n_jobs=-1, random_state=123))],
                n_jobs=-1)
2024-02-04 19:11:33,530:INFO:create_model() successfully completed......................................
2024-02-04 19:11:33,627:INFO:SubProcess create_model() end ==================================
2024-02-04 19:11:33,627:INFO:Creating Dashboard logs
2024-02-04 19:11:33,627:INFO:Model: Voting Regressor
2024-02-04 19:11:33,656:INFO:Logged params: {'estimators': [('Gradient Boosting Regressor', GradientBoostingRegressor(random_state=123)), ('Light Gradient Boosting Machine', LGBMRegressor(n_jobs=-1, random_state=123)), ('Extra Trees Regressor', ExtraTreesRegressor(n_jobs=-1, random_state=123))], 'n_jobs': -1, 'verbose': False, 'weights': None, 'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=123), 'Light Gradient Boosting Machine': LGBMRegressor(n_jobs=-1, random_state=123), 'Extra Trees Regressor': ExtraTreesRegressor(n_jobs=-1, random_state=123), 'Gradient Boosting Regressor__alpha': 0.9, 'Gradient Boosting Regressor__ccp_alpha': 0.0, 'Gradient Boosting Regressor__criterion': 'friedman_mse', 'Gradient Boosting Regressor__init': None, 'Gradient Boosting Regressor__learning_rate': 0.1, 'Gradient Boosting Regressor__loss': 'squared_error', 'Gradient Boosting Regressor__max_depth': 3, 'Gradient Boosting Regressor__max_features': None, 'Gradient Boosting Regressor__max_leaf_nodes': None, 'Gradient Boosting Regressor__min_impurity_decrease': 0.0, 'Gradient Boosting Regressor__min_samples_leaf': 1, 'Gradient Boosting Regressor__min_samples_split': 2, 'Gradient Boosting Regressor__min_weight_fraction_leaf': 0.0, 'Gradient Boosting Regressor__n_estimators': 100, 'Gradient Boosting Regressor__n_iter_no_change': None, 'Gradient Boosting Regressor__random_state': 123, 'Gradient Boosting Regressor__subsample': 1.0, 'Gradient Boosting Regressor__tol': 0.0001, 'Gradient Boosting Regressor__validation_fraction': 0.1, 'Gradient Boosting Regressor__verbose': 0, 'Gradient Boosting Regressor__warm_start': False, 'Light Gradient Boosting Machine__boosting_type': 'gbdt', 'Light Gradient Boosting Machine__class_weight': None, 'Light Gradient Boosting Machine__colsample_bytree': 1.0, 'Light Gradient Boosting Machine__importance_type': 'split', 'Light Gradient Boosting Machine__learning_rate': 0.1, 'Light Gradient Boosting Machine__max_depth': -1, 'Light Gradient Boosting Machine__min_child_samples': 20, 'Light Gradient Boosting Machine__min_child_weight': 0.001, 'Light Gradient Boosting Machine__min_split_gain': 0.0, 'Light Gradient Boosting Machine__n_estimators': 100, 'Light Gradient Boosting Machine__n_jobs': -1, 'Light Gradient Boosting Machine__num_leaves': 31, 'Light Gradient Boosting Machine__objective': None, 'Light Gradient Boosting Machine__random_state': 123, 'Light Gradient Boosting Machine__reg_alpha': 0.0, 'Light Gradient Boosting Machine__reg_lambda': 0.0, 'Light Gradient Boosting Machine__subsample': 1.0, 'Light Gradient Boosting Machine__subsample_for_bin': 200000, 'Light Gradient Boosting Machine__subsample_freq': 0, 'Extra Trees Regressor__bootstrap': False, 'Extra Trees Regressor__ccp_alpha': 0.0, 'Extra Trees Regressor__criterion': 'squared_error', 'Extra Trees Regressor__max_depth': None, 'Extra Trees Regressor__max_features': 1.0, 'Extra Trees Regressor__max_leaf_nodes': None, 'Extra Trees Regressor__max_samples': None, 'Extra Trees Regressor__min_impurity_decrease': 0.0, 'Extra Trees Regressor__min_samples_leaf': 1, 'Extra Trees Regressor__min_samples_split': 2, 'Extra Trees Regressor__min_weight_fraction_leaf': 0.0, 'Extra Trees Regressor__n_estimators': 100, 'Extra Trees Regressor__n_jobs': -1, 'Extra Trees Regressor__oob_score': False, 'Extra Trees Regressor__random_state': 123, 'Extra Trees Regressor__verbose': 0, 'Extra Trees Regressor__warm_start': False}
2024-02-04 19:11:33,756:INFO:Initializing predict_model()
2024-02-04 19:11:33,756:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=VotingRegressor(estimators=[('Gradient Boosting Regressor',
                             GradientBoostingRegressor(random_state=123)),
                            ('Light Gradient Boosting Machine',
                             LGBMRegressor(n_jobs=-1, random_state=123)),
                            ('Extra Trees Regressor',
                             ExtraTreesRegressor(n_jobs=-1, random_state=123))],
                n_jobs=-1), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECD60269D0>)
2024-02-04 19:11:33,756:INFO:Checking exceptions
2024-02-04 19:11:33,756:INFO:Preloading libraries
2024-02-04 19:11:34,224:INFO:_master_model_container: 46
2024-02-04 19:11:34,224:INFO:_display_container: 10
2024-02-04 19:11:34,224:INFO:VotingRegressor(estimators=[('Gradient Boosting Regressor',
                             GradientBoostingRegressor(random_state=123)),
                            ('Light Gradient Boosting Machine',
                             LGBMRegressor(n_jobs=-1, random_state=123)),
                            ('Extra Trees Regressor',
                             ExtraTreesRegressor(n_jobs=-1, random_state=123))],
                n_jobs=-1)
2024-02-04 19:11:34,224:INFO:blend_models() successfully completed......................................
2024-02-04 19:11:41,710:INFO:Initializing stack_models()
2024-02-04 19:11:41,710:INFO:stack_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator_list=[GradientBoostingRegressor(random_state=123), LGBMRegressor(n_jobs=-1, random_state=123), ExtraTreesRegressor(n_jobs=-1, random_state=123)], meta_model=None, meta_model_fold=5, fold=None, round=4, method=auto, restack=True, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-04 19:11:41,710:INFO:Checking exceptions
2024-02-04 19:11:41,710:INFO:Defining meta model
2024-02-04 19:11:41,728:INFO:Getting model names
2024-02-04 19:11:41,728:INFO:[('Gradient Boosting Regressor', GradientBoostingRegressor(random_state=123)), ('Light Gradient Boosting Machine', LGBMRegressor(n_jobs=-1, random_state=123)), ('Extra Trees Regressor', ExtraTreesRegressor(n_jobs=-1, random_state=123))]
2024-02-04 19:11:41,728:INFO:SubProcess create_model() called ==================================
2024-02-04 19:11:41,737:INFO:Initializing create_model()
2024-02-04 19:11:41,737:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=StackingRegressor(cv=5,
                  estimators=[('Gradient Boosting Regressor',
                               GradientBoostingRegressor(random_state=123)),
                              ('Light Gradient Boosting Machine',
                               LGBMRegressor(n_jobs=-1, random_state=123)),
                              ('Extra Trees Regressor',
                               ExtraTreesRegressor(n_jobs=-1,
                                                   random_state=123))],
                  final_estimator=LinearRegression(n_jobs=-1), n_jobs=-1,
                  passthrough=True), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA7636FA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:11:41,737:INFO:Checking exceptions
2024-02-04 19:11:41,737:INFO:Importing libraries
2024-02-04 19:11:41,737:INFO:Copying training dataset
2024-02-04 19:11:41,737:INFO:Defining folds
2024-02-04 19:11:41,737:INFO:Declaring metric variables
2024-02-04 19:11:41,744:INFO:Importing untrained model
2024-02-04 19:11:41,744:INFO:Declaring custom model
2024-02-04 19:11:41,747:INFO:Stacking Regressor Imported successfully
2024-02-04 19:11:41,755:INFO:Starting cross validation
2024-02-04 19:11:41,755:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:12:03,650:INFO:Calculating mean and std
2024-02-04 19:12:03,651:INFO:Creating metrics dataframe
2024-02-04 19:12:03,657:INFO:Finalizing model
2024-02-04 19:12:06,207:INFO:Uploading results into container
2024-02-04 19:12:06,207:INFO:Uploading model into container now
2024-02-04 19:12:06,208:INFO:_master_model_container: 47
2024-02-04 19:12:06,208:INFO:_display_container: 11
2024-02-04 19:12:06,213:INFO:StackingRegressor(cv=5,
                  estimators=[('Gradient Boosting Regressor',
                               GradientBoostingRegressor(random_state=123)),
                              ('Light Gradient Boosting Machine',
                               LGBMRegressor(n_jobs=-1, random_state=123)),
                              ('Extra Trees Regressor',
                               ExtraTreesRegressor(n_jobs=-1,
                                                   random_state=123))],
                  final_estimator=LinearRegression(n_jobs=-1), n_jobs=-1,
                  passthrough=True)
2024-02-04 19:12:06,213:INFO:create_model() successfully completed......................................
2024-02-04 19:12:06,346:INFO:SubProcess create_model() end ==================================
2024-02-04 19:12:06,346:INFO:Creating Dashboard logs
2024-02-04 19:12:06,346:INFO:Model: Stacking Regressor
2024-02-04 19:12:06,383:INFO:Logged params: {'cv': 5, 'estimators': [('Gradient Boosting Regressor', GradientBoostingRegressor(random_state=123)), ('Light Gradient Boosting Machine', LGBMRegressor(n_jobs=-1, random_state=123)), ('Extra Trees Regressor', ExtraTreesRegressor(n_jobs=-1, random_state=123))], 'final_estimator__copy_X': True, 'final_estimator__fit_intercept': True, 'final_estimator__n_jobs': -1, 'final_estimator__positive': False, 'final_estimator': LinearRegression(n_jobs=-1), 'n_jobs': -1, 'passthrough': True, 'verbose': 0, 'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=123), 'Light Gradient Boosting Machine': LGBMRegressor(n_jobs=-1, random_state=123), 'Extra Trees Regressor': ExtraTreesRegressor(n_jobs=-1, random_state=123), 'Gradient Boosting Regressor__alpha': 0.9, 'Gradient Boosting Regressor__ccp_alpha': 0.0, 'Gradient Boosting Regressor__criterion': 'friedman_mse', 'Gradient Boosting Regressor__init': None, 'Gradient Boosting Regressor__learning_rate': 0.1, 'Gradient Boosting Regressor__loss': 'squared_error', 'Gradient Boosting Regressor__max_depth': 3, 'Gradient Boosting Regressor__max_features': None, 'Gradient Boosting Regressor__max_leaf_nodes': None, 'Gradient Boosting Regressor__min_impurity_decrease': 0.0, 'Gradient Boosting Regressor__min_samples_leaf': 1, 'Gradient Boosting Regressor__min_samples_split': 2, 'Gradient Boosting Regressor__min_weight_fraction_leaf': 0.0, 'Gradient Boosting Regressor__n_estimators': 100, 'Gradient Boosting Regressor__n_iter_no_change': None, 'Gradient Boosting Regressor__random_state': 123, 'Gradient Boosting Regressor__subsample': 1.0, 'Gradient Boosting Regressor__tol': 0.0001, 'Gradient Boosting Regressor__validation_fraction': 0.1, 'Gradient Boosting Regressor__verbose': 0, 'Gradient Boosting Regressor__warm_start': False, 'Light Gradient Boosting Machine__boosting_type': 'gbdt', 'Light Gradient Boosting Machine__class_weight': None, 'Light Gradient Boosting Machine__colsample_bytree': 1.0, 'Light Gradient Boosting Machine__importance_type': 'split', 'Light Gradient Boosting Machine__learning_rate': 0.1, 'Light Gradient Boosting Machine__max_depth': -1, 'Light Gradient Boosting Machine__min_child_samples': 20, 'Light Gradient Boosting Machine__min_child_weight': 0.001, 'Light Gradient Boosting Machine__min_split_gain': 0.0, 'Light Gradient Boosting Machine__n_estimators': 100, 'Light Gradient Boosting Machine__n_jobs': -1, 'Light Gradient Boosting Machine__num_leaves': 31, 'Light Gradient Boosting Machine__objective': None, 'Light Gradient Boosting Machine__random_state': 123, 'Light Gradient Boosting Machine__reg_alpha': 0.0, 'Light Gradient Boosting Machine__reg_lambda': 0.0, 'Light Gradient Boosting Machine__subsample': 1.0, 'Light Gradient Boosting Machine__subsample_for_bin': 200000, 'Light Gradient Boosting Machine__subsample_freq': 0, 'Extra Trees Regressor__bootstrap': False, 'Extra Trees Regressor__ccp_alpha': 0.0, 'Extra Trees Regressor__criterion': 'squared_error', 'Extra Trees Regressor__max_depth': None, 'Extra Trees Regressor__max_features': 1.0, 'Extra Trees Regressor__max_leaf_nodes': None, 'Extra Trees Regressor__max_samples': None, 'Extra Trees Regressor__min_impurity_decrease': 0.0, 'Extra Trees Regressor__min_samples_leaf': 1, 'Extra Trees Regressor__min_samples_split': 2, 'Extra Trees Regressor__min_weight_fraction_leaf': 0.0, 'Extra Trees Regressor__n_estimators': 100, 'Extra Trees Regressor__n_jobs': -1, 'Extra Trees Regressor__oob_score': False, 'Extra Trees Regressor__random_state': 123, 'Extra Trees Regressor__verbose': 0, 'Extra Trees Regressor__warm_start': False}
2024-02-04 19:12:06,490:INFO:Initializing predict_model()
2024-02-04 19:12:06,491:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=StackingRegressor(cv=5,
                  estimators=[('Gradient Boosting Regressor',
                               GradientBoostingRegressor(random_state=123)),
                              ('Light Gradient Boosting Machine',
                               LGBMRegressor(n_jobs=-1, random_state=123)),
                              ('Extra Trees Regressor',
                               ExtraTreesRegressor(n_jobs=-1,
                                                   random_state=123))],
                  final_estimator=LinearRegression(n_jobs=-1), n_jobs=-1,
                  passthrough=True), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECD6026160>)
2024-02-04 19:12:06,491:INFO:Checking exceptions
2024-02-04 19:12:06,491:INFO:Preloading libraries
2024-02-04 19:12:07,016:INFO:_master_model_container: 47
2024-02-04 19:12:07,016:INFO:_display_container: 11
2024-02-04 19:12:07,032:INFO:StackingRegressor(cv=5,
                  estimators=[('Gradient Boosting Regressor',
                               GradientBoostingRegressor(random_state=123)),
                              ('Light Gradient Boosting Machine',
                               LGBMRegressor(n_jobs=-1, random_state=123)),
                              ('Extra Trees Regressor',
                               ExtraTreesRegressor(n_jobs=-1,
                                                   random_state=123))],
                  final_estimator=LinearRegression(n_jobs=-1), n_jobs=-1,
                  passthrough=True)
2024-02-04 19:12:07,032:INFO:stack_models() successfully completed......................................
2024-02-04 19:15:25,084:INFO:Initializing interpret_model()
2024-02-04 19:15:25,084:INFO:interpret_model(estimator=LGBMRegressor(n_jobs=-1, random_state=123), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>)
2024-02-04 19:15:25,084:INFO:Checking exceptions
2024-02-04 19:15:25,084:INFO:Soft dependency imported: shap: 0.44.1
2024-02-04 19:15:25,599:INFO:plot type: summary
2024-02-04 19:15:25,599:INFO:Creating TreeExplainer
2024-02-04 19:15:25,690:INFO:Compiling shap values
2024-02-04 19:15:26,155:INFO:Visual Rendered Successfully
2024-02-04 19:15:26,155:INFO:interpret_model() successfully completed......................................
2024-02-04 19:15:33,684:INFO:Initializing interpret_model()
2024-02-04 19:15:33,684:INFO:interpret_model(estimator=LGBMRegressor(n_jobs=-1, random_state=123), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=correlation, save=False, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>)
2024-02-04 19:15:33,684:INFO:Checking exceptions
2024-02-04 19:15:33,685:INFO:Soft dependency imported: shap: 0.44.1
2024-02-04 19:15:33,775:INFO:plot type: correlation
2024-02-04 19:15:33,776:WARNING:No feature passed. Default value of feature used for correlation plot: MSSubClass_SPLIT FOYER
2024-02-04 19:15:33,776:INFO:Creating TreeExplainer
2024-02-04 19:15:33,847:INFO:Compiling shap values
2024-02-04 19:15:33,942:INFO:model type detected: type 2
2024-02-04 19:15:34,050:INFO:Visual Rendered Successfully
2024-02-04 19:15:34,050:INFO:interpret_model() successfully completed......................................
2024-02-04 19:15:50,452:INFO:Initializing interpret_model()
2024-02-04 19:15:50,452:INFO:interpret_model(estimator=LGBMRegressor(n_jobs=-1, random_state=123), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=12, plot=reason, save=False, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>)
2024-02-04 19:15:50,452:INFO:Checking exceptions
2024-02-04 19:15:50,452:INFO:Soft dependency imported: shap: 0.44.1
2024-02-04 19:15:50,547:INFO:plot type: reason
2024-02-04 19:15:50,547:INFO:model type detected: type 2
2024-02-04 19:15:50,547:INFO:Creating TreeExplainer
2024-02-04 19:15:50,620:INFO:Compiling shap values
2024-02-04 19:15:50,721:INFO:Visual Rendered Successfully
2024-02-04 19:15:50,721:INFO:interpret_model() successfully completed......................................
2024-02-04 19:17:38,721:INFO:Initializing automl()
2024-02-04 19:17:38,721:INFO:automl(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, optimize=R2, use_holdout=False, turbo=True, return_train_score=False)
2024-02-04 19:17:38,721:INFO:Model Selection Basis : CV Results on Training set
2024-02-04 19:17:38,721:INFO:Checking model 18
2024-02-04 19:17:38,721:INFO:Checking model 20
2024-02-04 19:17:38,721:INFO:Checking model 21
2024-02-04 19:17:38,721:INFO:Checking model 22
2024-02-04 19:17:38,721:INFO:Checking model 23
2024-02-04 19:17:38,721:INFO:Checking model 24
2024-02-04 19:17:38,721:INFO:Checking model 25
2024-02-04 19:17:38,721:INFO:Checking model 26
2024-02-04 19:17:38,721:INFO:Checking model 27
2024-02-04 19:17:38,721:INFO:Checking model 28
2024-02-04 19:17:38,721:INFO:Checking model 29
2024-02-04 19:17:38,721:INFO:Checking model 30
2024-02-04 19:17:38,721:INFO:Checking model 31
2024-02-04 19:17:38,721:INFO:Checking model 32
2024-02-04 19:17:38,721:INFO:Checking model 33
2024-02-04 19:17:38,721:INFO:Checking model 34
2024-02-04 19:17:38,721:INFO:Checking model 35
2024-02-04 19:17:38,721:INFO:Checking model 36
2024-02-04 19:17:38,721:INFO:Checking model 37
2024-02-04 19:17:38,721:INFO:Checking model 38
2024-02-04 19:17:38,721:INFO:Checking model 39
2024-02-04 19:17:38,721:INFO:Checking model 40
2024-02-04 19:17:38,721:INFO:Checking model 41
2024-02-04 19:17:38,721:INFO:Checking model 42
2024-02-04 19:17:38,721:INFO:Checking model 43
2024-02-04 19:17:38,721:INFO:Checking model 44
2024-02-04 19:17:38,721:INFO:Checking model 45
2024-02-04 19:17:38,728:INFO:Checking model 46
2024-02-04 19:17:38,728:INFO:Initializing create_model()
2024-02-04 19:17:38,728:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:17:38,728:INFO:Checking exceptions
2024-02-04 19:17:38,730:INFO:Importing libraries
2024-02-04 19:17:38,730:INFO:Copying training dataset
2024-02-04 19:17:38,734:INFO:Defining folds
2024-02-04 19:17:38,734:INFO:Declaring metric variables
2024-02-04 19:17:38,734:INFO:Importing untrained model
2024-02-04 19:17:38,734:INFO:Declaring custom model
2024-02-04 19:17:38,735:INFO:Bagging Regressor Imported successfully
2024-02-04 19:17:38,737:INFO:Cross validation set to False
2024-02-04 19:17:38,737:INFO:Fitting Model
2024-02-04 19:17:55,347:INFO:BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123)
2024-02-04 19:17:55,347:INFO:create_model() successfully completed......................................
2024-02-04 19:17:55,620:INFO:BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123)
2024-02-04 19:17:55,622:INFO:automl() successfully completed......................................
2024-02-04 19:18:09,500:INFO:Soft dependency imported: explainerdashboard: 0.4.5
2024-02-04 19:18:10,071:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\dash\dash.py:538: UserWarning:

JupyterDash is deprecated, use Dash instead.
See https://dash.plotly.com/dash-in-jupyter for more details.


2024-02-04 19:20:04,047:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,053:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,053:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,053:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,053:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,067:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,067:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,067:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,079:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,086:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,093:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,094:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,100:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,101:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,101:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,101:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,101:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,101:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,101:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,101:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,105:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,105:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,109:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,109:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,109:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,109:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,599:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,599:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,599:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,599:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,599:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,599:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,599:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,700:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,700:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,701:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,701:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,701:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,702:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,702:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,702:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,710:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,710:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,710:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,711:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:22:50,397:INFO:Initializing predict_model()
2024-02-04 19:22:50,397:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECDB347B80>)
2024-02-04 19:22:50,397:INFO:Checking exceptions
2024-02-04 19:22:50,397:INFO:Preloading libraries
2024-02-04 19:23:13,722:INFO:Soft dependency imported: gradio: 3.50.0
2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,739:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,740:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,741:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,741:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,742:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,742:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,742:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,743:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,743:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,744:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,744:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,744:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,744:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,744:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,747:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,759:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,759:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,760:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,760:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,760:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,762:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,762:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,763:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,763:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,764:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,764:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,764:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:28,127:INFO:Soft dependency imported: fastapi: 0.109.1
2024-02-04 19:23:28,127:INFO:Soft dependency imported: uvicorn: 0.27.0.post1
2024-02-04 19:23:28,132:INFO:Soft dependency imported: pydantic: 1.10.12
2024-02-04 19:23:28,150:INFO:Initializing save_model()
2024-02-04 19:23:28,150:INFO:save_model(model=GradientBoostingRegressor(random_state=123), model_name=housing_price_api, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2024-02-04 19:23:28,150:INFO:Adding model into prep_pipe
2024-02-04 19:23:28,165:INFO:housing_price_api.pkl saved in current working directory
2024-02-04 19:23:28,190:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', 'HeatingQ...
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('trained_model', GradientBoostingRegressor(random_state=123))])
2024-02-04 19:23:28,190:INFO:save_model() successfully completed......................................
2024-02-04 19:24:06,592:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:24:06,592:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:24:06,592:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:24:06,592:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:24:06,777:INFO:Initializing load_model()
2024-02-04 19:24:06,777:INFO:load_model(model_name=housing_price_api, platform=None, authentication=None, verbose=True)
2024-02-04 19:25:16,918:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:25:16,918:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:25:16,918:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:25:16,918:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:25:17,103:INFO:Initializing load_model()
2024-02-04 19:25:17,103:INFO:load_model(model_name=housing_price_api, platform=None, authentication=None, verbose=True)
2024-02-05 18:42:38,290:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-05 18:42:38,291:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-05 18:42:38,291:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-05 18:42:38,291:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-05 18:42:38,419:INFO:PyCaret RegressionExperiment
2024-02-05 18:42:38,420:INFO:Logging name: houseprice1
2024-02-05 18:42:38,420:INFO:ML Usecase: MLUsecase.REGRESSION
2024-02-05 18:42:38,420:INFO:version 3.2.0
2024-02-05 18:42:38,420:INFO:Initializing setup()
2024-02-05 18:42:38,420:INFO:self.USI: 170c
2024-02-05 18:42:38,420:INFO:self._variable_keys: {'USI', 'gpu_param', 'fold_shuffle_param', 'target_param', 'transform_target_param', '_ml_usecase', 'gpu_n_jobs_param', 'data', 'exp_id', 'y_train', 'X', 'exp_name_log', 'X_test', 'fold_groups_param', 'X_train', 'idx', 'y_test', 'n_jobs_param', '_available_plots', 'pipeline', 'logging_param', 'memory', 'html_param', 'fold_generator', 'seed', 'log_plots_param', 'y'}
2024-02-05 18:42:38,420:INFO:Checking environment
2024-02-05 18:42:38,420:INFO:python_version: 3.9.9
2024-02-05 18:42:38,420:INFO:python_build: ('tags/v3.9.9:ccb0e6a', 'Nov 15 2021 18:08:50')
2024-02-05 18:42:38,421:INFO:machine: AMD64
2024-02-05 18:42:38,421:INFO:platform: Windows-10-10.0.22621-SP0
2024-02-05 18:42:38,425:INFO:Memory: svmem(total=16856203264, available=8936173568, percent=47.0, used=7920029696, free=8936173568)
2024-02-05 18:42:38,425:INFO:Physical Core: 4
2024-02-05 18:42:38,425:INFO:Logical Core: 8
2024-02-05 18:42:38,425:INFO:Checking libraries
2024-02-05 18:42:38,425:INFO:System:
2024-02-05 18:42:38,425:INFO:    python: 3.9.9 (tags/v3.9.9:ccb0e6a, Nov 15 2021, 18:08:50) [MSC v.1929 64 bit (AMD64)]
2024-02-05 18:42:38,426:INFO:executable: c:\Users\joseg\AppData\Local\Programs\Python\Python39\python.exe
2024-02-05 18:42:38,426:INFO:   machine: Windows-10-10.0.22621-SP0
2024-02-05 18:42:38,426:INFO:PyCaret required dependencies:
2024-02-05 18:42:38,474:INFO:                 pip: 21.2.4
2024-02-05 18:42:38,475:INFO:          setuptools: 58.1.0
2024-02-05 18:42:38,475:INFO:             pycaret: 3.2.0
2024-02-05 18:42:38,475:INFO:             IPython: 8.18.1
2024-02-05 18:42:38,475:INFO:          ipywidgets: 8.1.1
2024-02-05 18:42:38,475:INFO:                tqdm: 4.66.1
2024-02-05 18:42:38,475:INFO:               numpy: 1.23.5
2024-02-05 18:42:38,475:INFO:              pandas: 1.5.3
2024-02-05 18:42:38,475:INFO:              jinja2: 3.1.3
2024-02-05 18:42:38,475:INFO:               scipy: 1.10.1
2024-02-05 18:42:38,475:INFO:              joblib: 1.3.2
2024-02-05 18:42:38,475:INFO:             sklearn: 1.2.2
2024-02-05 18:42:38,475:INFO:                pyod: 1.1.2
2024-02-05 18:42:38,475:INFO:            imblearn: 0.12.0
2024-02-05 18:42:38,475:INFO:   category_encoders: 2.6.3
2024-02-05 18:42:38,475:INFO:            lightgbm: 4.3.0
2024-02-05 18:42:38,475:INFO:               numba: 0.59.0
2024-02-05 18:42:38,475:INFO:            requests: 2.31.0
2024-02-05 18:42:38,475:INFO:          matplotlib: 3.6.0
2024-02-05 18:42:38,475:INFO:          scikitplot: 0.3.7
2024-02-05 18:42:38,475:INFO:         yellowbrick: 1.5
2024-02-05 18:42:38,475:INFO:              plotly: 5.18.0
2024-02-05 18:42:38,475:INFO:    plotly-resampler: Not installed
2024-02-05 18:42:38,475:INFO:             kaleido: 0.2.1
2024-02-05 18:42:38,475:INFO:           schemdraw: 0.15
2024-02-05 18:42:38,475:INFO:         statsmodels: 0.14.1
2024-02-05 18:42:38,475:INFO:              sktime: 0.21.1
2024-02-05 18:42:38,475:INFO:               tbats: 1.1.3
2024-02-05 18:42:38,475:INFO:            pmdarima: 2.0.4
2024-02-05 18:42:38,475:INFO:              psutil: 5.9.8
2024-02-05 18:42:38,475:INFO:          markupsafe: 2.1.5
2024-02-05 18:42:38,475:INFO:             pickle5: Not installed
2024-02-05 18:42:38,475:INFO:         cloudpickle: 3.0.0
2024-02-05 18:42:38,475:INFO:         deprecation: 2.1.0
2024-02-05 18:42:38,475:INFO:              xxhash: 3.4.1
2024-02-05 18:42:38,475:INFO:           wurlitzer: Not installed
2024-02-05 18:42:38,475:INFO:PyCaret optional dependencies:
2024-02-05 18:42:39,457:INFO:                shap: 0.44.1
2024-02-05 18:42:39,458:INFO:           interpret: Not installed
2024-02-05 18:42:39,458:INFO:                umap: Not installed
2024-02-05 18:42:39,458:INFO:     ydata_profiling: Not installed
2024-02-05 18:42:39,458:INFO:  explainerdashboard: 0.4.5
2024-02-05 18:42:39,458:INFO:             autoviz: Not installed
2024-02-05 18:42:39,458:INFO:           fairlearn: Not installed
2024-02-05 18:42:39,458:INFO:          deepchecks: Not installed
2024-02-05 18:42:39,458:INFO:             xgboost: Not installed
2024-02-05 18:42:39,458:INFO:            catboost: Not installed
2024-02-05 18:42:39,458:INFO:              kmodes: Not installed
2024-02-05 18:42:39,459:INFO:             mlxtend: Not installed
2024-02-05 18:42:39,459:INFO:       statsforecast: Not installed
2024-02-05 18:42:39,459:INFO:        tune_sklearn: Not installed
2024-02-05 18:42:39,459:INFO:                 ray: Not installed
2024-02-05 18:42:39,459:INFO:            hyperopt: Not installed
2024-02-05 18:42:39,459:INFO:              optuna: Not installed
2024-02-05 18:42:39,459:INFO:               skopt: Not installed
2024-02-05 18:42:39,459:INFO:              mlflow: 2.10.0
2024-02-05 18:42:39,459:INFO:              gradio: 3.50.0
2024-02-05 18:42:39,459:INFO:             fastapi: 0.109.1
2024-02-05 18:42:39,459:INFO:             uvicorn: 0.27.0.post1
2024-02-05 18:42:39,459:INFO:              m2cgen: Not installed
2024-02-05 18:42:39,459:INFO:           evidently: Not installed
2024-02-05 18:42:39,459:INFO:               fugue: Not installed
2024-02-05 18:42:39,459:INFO:           streamlit: Not installed
2024-02-05 18:42:39,459:INFO:             prophet: Not installed
2024-02-05 18:42:39,459:INFO:None
2024-02-05 18:42:39,459:INFO:Set up data.
2024-02-05 18:42:39,478:INFO:Set up folding strategy.
2024-02-05 18:42:39,478:INFO:Set up train/test split.
2024-02-05 18:42:39,493:INFO:Set up index.
2024-02-05 18:42:39,493:INFO:Assigning column types.
2024-02-05 18:42:39,499:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-02-05 18:42:39,500:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,503:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,508:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,561:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,596:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,596:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:39,596:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:39,596:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,603:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,607:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,658:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,694:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,694:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:39,695:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:39,695:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-02-05 18:42:39,698:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,704:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,747:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,778:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,778:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:39,778:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:39,782:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,785:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,829:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,869:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,869:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:39,869:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:39,869:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-02-05 18:42:39,880:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,934:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,977:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-05 18:42:39,977:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:39,977:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:39,986:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-05 18:42:40,036:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-05 18:42:40,073:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-05 18:42:40,075:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:40,075:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:40,075:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-02-05 18:42:40,131:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-05 18:42:40,172:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-05 18:42:40,173:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:40,173:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:40,230:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-05 18:42:40,272:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-05 18:42:40,272:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:40,272:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:40,273:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-02-05 18:42:40,335:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-05 18:42:40,378:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:40,378:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:40,433:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-05 18:42:40,467:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:40,467:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:40,467:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-02-05 18:42:40,550:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:40,551:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:40,630:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:40,630:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:40,630:INFO:Preparing preprocessing pipeline...
2024-02-05 18:42:40,630:INFO:Set up simple imputation.
2024-02-05 18:42:40,640:INFO:Set up encoding of ordinal features.
2024-02-05 18:42:40,641:INFO:Set up encoding of categorical features.
2024-02-05 18:42:40,812:INFO:Finished creating preprocessing pipeline.
2024-02-05 18:42:40,829:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))])
2024-02-05 18:42:40,829:INFO:Creating final display dataframe.
2024-02-05 18:42:41,086:INFO:Setup _display_container:                     Description         Value
0                    Session id           123
1                        Target     SalePrice
2                   Target type    Regression
3           Original data shape    (1456, 56)
4        Transformed data shape   (1456, 134)
5   Transformed train set shape   (1019, 134)
6    Transformed test set shape    (437, 134)
7              Ordinal features             1
8              Numeric features            40
9          Categorical features            15
10     Rows with missing values         99.7%
11                   Preprocess          True
12              Imputation type        simple
13           Numeric imputation          mean
14       Categorical imputation          mode
15     Maximum one-hot encoding            25
16              Encoding method          None
17               Fold Generator         KFold
18                  Fold Number            10
19                     CPU Jobs            -1
20                      Use GPU         False
21               Log Experiment  MlflowLogger
22              Experiment Name   houseprice1
23                          USI          170c
2024-02-05 18:42:41,176:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:41,176:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:41,260:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:41,260:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:42:41,261:INFO:Logging experiment in loggers
2024-02-05 18:42:41,362:INFO:SubProcess save_model() called ==================================
2024-02-05 18:42:41,389:INFO:Initializing save_model()
2024-02-05 18:42:41,389:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))]), model_name=C:\Users\joseg\AppData\Local\Temp\tmpmk2y4yoi\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2024-02-05 18:42:41,389:INFO:Adding model into prep_pipe
2024-02-05 18:42:41,389:WARNING:Only Model saved as it was a pipeline.
2024-02-05 18:42:41,398:INFO:C:\Users\joseg\AppData\Local\Temp\tmpmk2y4yoi\Transformation Pipeline.pkl saved in current working directory
2024-02-05 18:42:41,413:INFO:Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))])
2024-02-05 18:42:41,413:INFO:save_model() successfully completed......................................
2024-02-05 18:42:41,521:INFO:SubProcess save_model() end ==================================
2024-02-05 18:42:41,529:INFO:setup() successfully completed in 2.84s...............
2024-02-05 18:42:41,549:INFO:Initializing compare_models()
2024-02-05 18:42:41,550:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, include=None, fold=5, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, 'include': None, 'exclude': None, 'fold': 5, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2024-02-05 18:42:41,550:INFO:Checking exceptions
2024-02-05 18:42:41,554:INFO:Preparing display monitor
2024-02-05 18:42:41,579:INFO:Initializing Linear Regression
2024-02-05 18:42:41,579:INFO:Total runtime is 0.0 minutes
2024-02-05 18:42:41,583:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:41,583:INFO:Initializing create_model()
2024-02-05 18:42:41,583:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=lr, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:41,583:INFO:Checking exceptions
2024-02-05 18:42:41,583:INFO:Importing libraries
2024-02-05 18:42:41,583:INFO:Copying training dataset
2024-02-05 18:42:41,589:INFO:Defining folds
2024-02-05 18:42:41,589:INFO:Declaring metric variables
2024-02-05 18:42:41,589:INFO:Importing untrained model
2024-02-05 18:42:41,596:INFO:Linear Regression Imported successfully
2024-02-05 18:42:41,606:INFO:Starting cross validation
2024-02-05 18:42:41,613:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:44,422:INFO:Calculating mean and std
2024-02-05 18:42:44,423:INFO:Creating metrics dataframe
2024-02-05 18:42:44,425:INFO:Uploading results into container
2024-02-05 18:42:44,426:INFO:Uploading model into container now
2024-02-05 18:42:44,426:INFO:_master_model_container: 1
2024-02-05 18:42:44,426:INFO:_display_container: 2
2024-02-05 18:42:44,426:INFO:LinearRegression(n_jobs=-1)
2024-02-05 18:42:44,427:INFO:create_model() successfully completed......................................
2024-02-05 18:42:44,550:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:44,550:INFO:Creating metrics dataframe
2024-02-05 18:42:44,557:INFO:Initializing Lasso Regression
2024-02-05 18:42:44,557:INFO:Total runtime is 0.04963522752126058 minutes
2024-02-05 18:42:44,559:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:44,560:INFO:Initializing create_model()
2024-02-05 18:42:44,560:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=lasso, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:44,560:INFO:Checking exceptions
2024-02-05 18:42:44,560:INFO:Importing libraries
2024-02-05 18:42:44,561:INFO:Copying training dataset
2024-02-05 18:42:44,565:INFO:Defining folds
2024-02-05 18:42:44,566:INFO:Declaring metric variables
2024-02-05 18:42:44,569:INFO:Importing untrained model
2024-02-05 18:42:44,573:INFO:Lasso Regression Imported successfully
2024-02-05 18:42:44,580:INFO:Starting cross validation
2024-02-05 18:42:44,583:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:44,895:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.451e+11, tolerance: 5.149e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:42:44,902:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.972e+11, tolerance: 4.496e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:42:46,338:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+11, tolerance: 5.182e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:42:46,356:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.138e+11, tolerance: 4.925e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:42:46,365:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.206e+11, tolerance: 5.100e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:42:46,451:INFO:Calculating mean and std
2024-02-05 18:42:46,453:INFO:Creating metrics dataframe
2024-02-05 18:42:46,459:INFO:Uploading results into container
2024-02-05 18:42:46,461:INFO:Uploading model into container now
2024-02-05 18:42:46,461:INFO:_master_model_container: 2
2024-02-05 18:42:46,461:INFO:_display_container: 2
2024-02-05 18:42:46,461:INFO:Lasso(random_state=123)
2024-02-05 18:42:46,461:INFO:create_model() successfully completed......................................
2024-02-05 18:42:46,563:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:46,563:INFO:Creating metrics dataframe
2024-02-05 18:42:46,570:INFO:Initializing Ridge Regression
2024-02-05 18:42:46,570:INFO:Total runtime is 0.08317712545394898 minutes
2024-02-05 18:42:46,572:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:46,573:INFO:Initializing create_model()
2024-02-05 18:42:46,573:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=ridge, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:46,573:INFO:Checking exceptions
2024-02-05 18:42:46,573:INFO:Importing libraries
2024-02-05 18:42:46,573:INFO:Copying training dataset
2024-02-05 18:42:46,578:INFO:Defining folds
2024-02-05 18:42:46,579:INFO:Declaring metric variables
2024-02-05 18:42:46,584:INFO:Importing untrained model
2024-02-05 18:42:46,587:INFO:Ridge Regression Imported successfully
2024-02-05 18:42:46,594:INFO:Starting cross validation
2024-02-05 18:42:46,597:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:46,898:INFO:Calculating mean and std
2024-02-05 18:42:46,898:INFO:Creating metrics dataframe
2024-02-05 18:42:46,901:INFO:Uploading results into container
2024-02-05 18:42:46,901:INFO:Uploading model into container now
2024-02-05 18:42:46,902:INFO:_master_model_container: 3
2024-02-05 18:42:46,902:INFO:_display_container: 2
2024-02-05 18:42:46,902:INFO:Ridge(random_state=123)
2024-02-05 18:42:46,902:INFO:create_model() successfully completed......................................
2024-02-05 18:42:46,987:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:46,987:INFO:Creating metrics dataframe
2024-02-05 18:42:46,994:INFO:Initializing Elastic Net
2024-02-05 18:42:46,994:INFO:Total runtime is 0.09024411042531331 minutes
2024-02-05 18:42:46,996:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:46,996:INFO:Initializing create_model()
2024-02-05 18:42:46,996:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=en, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:46,997:INFO:Checking exceptions
2024-02-05 18:42:46,997:INFO:Importing libraries
2024-02-05 18:42:46,997:INFO:Copying training dataset
2024-02-05 18:42:47,001:INFO:Defining folds
2024-02-05 18:42:47,001:INFO:Declaring metric variables
2024-02-05 18:42:47,004:INFO:Importing untrained model
2024-02-05 18:42:47,008:INFO:Elastic Net Imported successfully
2024-02-05 18:42:47,013:INFO:Starting cross validation
2024-02-05 18:42:47,016:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:47,296:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.592e+11, tolerance: 5.182e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:42:47,305:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.038e+11, tolerance: 4.496e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:42:47,311:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.375e+11, tolerance: 4.925e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:42:47,311:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.659e+11, tolerance: 5.149e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:42:47,316:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.303e+11, tolerance: 5.100e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:42:47,383:INFO:Calculating mean and std
2024-02-05 18:42:47,384:INFO:Creating metrics dataframe
2024-02-05 18:42:47,388:INFO:Uploading results into container
2024-02-05 18:42:47,389:INFO:Uploading model into container now
2024-02-05 18:42:47,389:INFO:_master_model_container: 4
2024-02-05 18:42:47,389:INFO:_display_container: 2
2024-02-05 18:42:47,389:INFO:ElasticNet(random_state=123)
2024-02-05 18:42:47,389:INFO:create_model() successfully completed......................................
2024-02-05 18:42:47,532:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:47,533:INFO:Creating metrics dataframe
2024-02-05 18:42:47,544:INFO:Initializing Least Angle Regression
2024-02-05 18:42:47,544:INFO:Total runtime is 0.09941739638646443 minutes
2024-02-05 18:42:47,549:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:47,550:INFO:Initializing create_model()
2024-02-05 18:42:47,550:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=lar, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:47,550:INFO:Checking exceptions
2024-02-05 18:42:47,550:INFO:Importing libraries
2024-02-05 18:42:47,550:INFO:Copying training dataset
2024-02-05 18:42:47,563:INFO:Defining folds
2024-02-05 18:42:47,564:INFO:Declaring metric variables
2024-02-05 18:42:47,569:INFO:Importing untrained model
2024-02-05 18:42:47,575:INFO:Least Angle Regression Imported successfully
2024-02-05 18:42:47,587:INFO:Starting cross validation
2024-02-05 18:42:47,591:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:47,848:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.077e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,849:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=2.921e+02, with an active set of 49 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,853:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.866e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,853:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.379e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,855:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.845e+02, with an active set of 53 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,855:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.239e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,857:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=1.536e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(


2024-02-05 18:42:47,857:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=3.638e+06, with an active set of 130 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,859:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.070e+02, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,859:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=8.829e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,860:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=7.866e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,860:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=6.627e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,861:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=5.541e+01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,861:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=5.236e+01, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,861:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=5.124e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,861:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=2.647e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,862:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=4.716e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,862:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=4.493e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,862:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.188e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,863:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 167 iterations, i.e. alpha=2.516e+06, with an active set of 131 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,863:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=1.683e+02, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,865:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=2.478e+06, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,865:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=1.273e+02, with an active set of 75 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,865:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=9.821e+05, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,866:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=9.683e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,866:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=7.083e+05, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,867:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=6.203e+05, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,867:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=5.911e+05, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,867:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=2.670e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,868:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=5.158e+05, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,868:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=3.391e+05, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,868:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=3.311e+05, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,869:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=3.302e+05, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,869:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=6.480e+05, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,869:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=4.481e+05, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,873:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.120e+02, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,873:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 173 iterations, i.e. alpha=3.011e+08, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,875:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.519e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,876:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 170 iterations, i.e. alpha=3.301e+09, with an active set of 129 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,876:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 170 iterations, i.e. alpha=2.294e+09, with an active set of 129 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,877:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=7.747e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,886:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 177 iterations, i.e. alpha=1.130e+04, with an active set of 130 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,886:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 177 iterations, i.e. alpha=2.523e+03, with an active set of 130 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:47,956:INFO:Calculating mean and std
2024-02-05 18:42:47,957:INFO:Creating metrics dataframe
2024-02-05 18:42:47,959:INFO:Uploading results into container
2024-02-05 18:42:47,959:INFO:Uploading model into container now
2024-02-05 18:42:47,960:INFO:_master_model_container: 5
2024-02-05 18:42:47,960:INFO:_display_container: 2
2024-02-05 18:42:47,960:INFO:Lars(random_state=123)
2024-02-05 18:42:47,961:INFO:create_model() successfully completed......................................
2024-02-05 18:42:48,052:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:48,053:INFO:Creating metrics dataframe
2024-02-05 18:42:48,060:INFO:Initializing Lasso Least Angle Regression
2024-02-05 18:42:48,061:INFO:Total runtime is 0.10803244908650717 minutes
2024-02-05 18:42:48,063:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:48,063:INFO:Initializing create_model()
2024-02-05 18:42:48,063:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=llar, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:48,063:INFO:Checking exceptions
2024-02-05 18:42:48,064:INFO:Importing libraries
2024-02-05 18:42:48,064:INFO:Copying training dataset
2024-02-05 18:42:48,068:INFO:Defining folds
2024-02-05 18:42:48,068:INFO:Declaring metric variables
2024-02-05 18:42:48,072:INFO:Importing untrained model
2024-02-05 18:42:48,075:INFO:Lasso Least Angle Regression Imported successfully
2024-02-05 18:42:48,081:INFO:Starting cross validation
2024-02-05 18:42:48,084:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:48,309:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=4.349e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,312:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=2.372e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,318:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=1.063e+02, with an active set of 78 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,318:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=6.564e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,319:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=3.367e+02, with an active set of 47 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,320:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.697e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,323:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.695e+02, with an active set of 65 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,325:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=8.321e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,325:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 140 iterations, alpha=2.593e+01, previous alpha=2.593e+01, with an active set of 97 regressors.
  warnings.warn(

2024-02-05 18:42:48,327:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=5.136e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,327:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.971e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,328:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=4.937e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,329:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.452e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,330:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.873e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,331:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=2.455e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,332:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=2.465e+01, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,332:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 79 iterations, alpha=1.116e+03, previous alpha=1.723e+02, with an active set of 60 regressors.
  warnings.warn(

2024-02-05 18:42:48,333:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=1.866e+01, with an active set of 106 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,334:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 135 iterations, alpha=6.986e+01, previous alpha=2.488e+01, with an active set of 100 regressors.
  warnings.warn(

2024-02-05 18:42:48,334:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.392e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,334:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.392e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,334:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 138 iterations, alpha=1.717e+01, previous alpha=1.717e+01, with an active set of 107 regressors.
  warnings.warn(

2024-02-05 18:42:48,335:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=1.199e+02, with an active set of 71 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,336:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=9.636e+01, with an active set of 75 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:42:48,339:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 103 iterations, alpha=6.958e+01, previous alpha=6.958e+01, with an active set of 82 regressors.
  warnings.warn(

2024-02-05 18:42:48,407:INFO:Calculating mean and std
2024-02-05 18:42:48,408:INFO:Creating metrics dataframe
2024-02-05 18:42:48,411:INFO:Uploading results into container
2024-02-05 18:42:48,411:INFO:Uploading model into container now
2024-02-05 18:42:48,412:INFO:_master_model_container: 6
2024-02-05 18:42:48,412:INFO:_display_container: 2
2024-02-05 18:42:48,412:INFO:LassoLars(random_state=123)
2024-02-05 18:42:48,412:INFO:create_model() successfully completed......................................
2024-02-05 18:42:48,503:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:48,503:INFO:Creating metrics dataframe
2024-02-05 18:42:48,512:INFO:Initializing Orthogonal Matching Pursuit
2024-02-05 18:42:48,512:INFO:Total runtime is 0.1155505657196045 minutes
2024-02-05 18:42:48,514:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:48,515:INFO:Initializing create_model()
2024-02-05 18:42:48,515:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=omp, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:48,515:INFO:Checking exceptions
2024-02-05 18:42:48,515:INFO:Importing libraries
2024-02-05 18:42:48,515:INFO:Copying training dataset
2024-02-05 18:42:48,521:INFO:Defining folds
2024-02-05 18:42:48,521:INFO:Declaring metric variables
2024-02-05 18:42:48,523:INFO:Importing untrained model
2024-02-05 18:42:48,527:INFO:Orthogonal Matching Pursuit Imported successfully
2024-02-05 18:42:48,536:INFO:Starting cross validation
2024-02-05 18:42:48,538:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:48,830:INFO:Calculating mean and std
2024-02-05 18:42:48,831:INFO:Creating metrics dataframe
2024-02-05 18:42:48,834:INFO:Uploading results into container
2024-02-05 18:42:48,835:INFO:Uploading model into container now
2024-02-05 18:42:48,835:INFO:_master_model_container: 7
2024-02-05 18:42:48,835:INFO:_display_container: 2
2024-02-05 18:42:48,835:INFO:OrthogonalMatchingPursuit()
2024-02-05 18:42:48,835:INFO:create_model() successfully completed......................................
2024-02-05 18:42:48,930:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:48,930:INFO:Creating metrics dataframe
2024-02-05 18:42:48,940:INFO:Initializing Bayesian Ridge
2024-02-05 18:42:48,941:INFO:Total runtime is 0.12269491751988729 minutes
2024-02-05 18:42:48,945:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:48,945:INFO:Initializing create_model()
2024-02-05 18:42:48,945:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=br, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:48,945:INFO:Checking exceptions
2024-02-05 18:42:48,945:INFO:Importing libraries
2024-02-05 18:42:48,945:INFO:Copying training dataset
2024-02-05 18:42:48,952:INFO:Defining folds
2024-02-05 18:42:48,952:INFO:Declaring metric variables
2024-02-05 18:42:48,955:INFO:Importing untrained model
2024-02-05 18:42:48,959:INFO:Bayesian Ridge Imported successfully
2024-02-05 18:42:48,965:INFO:Starting cross validation
2024-02-05 18:42:48,967:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:49,304:INFO:Calculating mean and std
2024-02-05 18:42:49,306:INFO:Creating metrics dataframe
2024-02-05 18:42:49,308:INFO:Uploading results into container
2024-02-05 18:42:49,308:INFO:Uploading model into container now
2024-02-05 18:42:49,310:INFO:_master_model_container: 8
2024-02-05 18:42:49,311:INFO:_display_container: 2
2024-02-05 18:42:49,311:INFO:BayesianRidge()
2024-02-05 18:42:49,311:INFO:create_model() successfully completed......................................
2024-02-05 18:42:49,410:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:49,411:INFO:Creating metrics dataframe
2024-02-05 18:42:49,418:INFO:Initializing Passive Aggressive Regressor
2024-02-05 18:42:49,418:INFO:Total runtime is 0.1306518276532491 minutes
2024-02-05 18:42:49,421:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:49,422:INFO:Initializing create_model()
2024-02-05 18:42:49,422:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=par, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:49,422:INFO:Checking exceptions
2024-02-05 18:42:49,422:INFO:Importing libraries
2024-02-05 18:42:49,422:INFO:Copying training dataset
2024-02-05 18:42:49,426:INFO:Defining folds
2024-02-05 18:42:49,426:INFO:Declaring metric variables
2024-02-05 18:42:49,429:INFO:Importing untrained model
2024-02-05 18:42:49,433:INFO:Passive Aggressive Regressor Imported successfully
2024-02-05 18:42:49,441:INFO:Starting cross validation
2024-02-05 18:42:49,443:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:49,746:INFO:Calculating mean and std
2024-02-05 18:42:49,747:INFO:Creating metrics dataframe
2024-02-05 18:42:49,751:INFO:Uploading results into container
2024-02-05 18:42:49,751:INFO:Uploading model into container now
2024-02-05 18:42:49,751:INFO:_master_model_container: 9
2024-02-05 18:42:49,751:INFO:_display_container: 2
2024-02-05 18:42:49,752:INFO:PassiveAggressiveRegressor(random_state=123)
2024-02-05 18:42:49,752:INFO:create_model() successfully completed......................................
2024-02-05 18:42:49,843:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:49,843:INFO:Creating metrics dataframe
2024-02-05 18:42:49,852:INFO:Initializing Huber Regressor
2024-02-05 18:42:49,853:INFO:Total runtime is 0.13790121873219807 minutes
2024-02-05 18:42:49,857:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:49,857:INFO:Initializing create_model()
2024-02-05 18:42:49,857:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=huber, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:49,857:INFO:Checking exceptions
2024-02-05 18:42:49,857:INFO:Importing libraries
2024-02-05 18:42:49,857:INFO:Copying training dataset
2024-02-05 18:42:49,862:INFO:Defining folds
2024-02-05 18:42:49,862:INFO:Declaring metric variables
2024-02-05 18:42:49,867:INFO:Importing untrained model
2024-02-05 18:42:49,870:INFO:Huber Regressor Imported successfully
2024-02-05 18:42:49,877:INFO:Starting cross validation
2024-02-05 18:42:49,880:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:50,134:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 18:42:50,165:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 18:42:50,168:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 18:42:50,175:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 18:42:50,176:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 18:42:50,255:INFO:Calculating mean and std
2024-02-05 18:42:50,257:INFO:Creating metrics dataframe
2024-02-05 18:42:50,260:INFO:Uploading results into container
2024-02-05 18:42:50,260:INFO:Uploading model into container now
2024-02-05 18:42:50,260:INFO:_master_model_container: 10
2024-02-05 18:42:50,260:INFO:_display_container: 2
2024-02-05 18:42:50,260:INFO:HuberRegressor()
2024-02-05 18:42:50,261:INFO:create_model() successfully completed......................................
2024-02-05 18:42:50,354:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:50,354:INFO:Creating metrics dataframe
2024-02-05 18:42:50,364:INFO:Initializing K Neighbors Regressor
2024-02-05 18:42:50,364:INFO:Total runtime is 0.14641989469528197 minutes
2024-02-05 18:42:50,367:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:50,367:INFO:Initializing create_model()
2024-02-05 18:42:50,367:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=knn, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:50,367:INFO:Checking exceptions
2024-02-05 18:42:50,367:INFO:Importing libraries
2024-02-05 18:42:50,367:INFO:Copying training dataset
2024-02-05 18:42:50,372:INFO:Defining folds
2024-02-05 18:42:50,373:INFO:Declaring metric variables
2024-02-05 18:42:50,376:INFO:Importing untrained model
2024-02-05 18:42:50,378:INFO:K Neighbors Regressor Imported successfully
2024-02-05 18:42:50,385:INFO:Starting cross validation
2024-02-05 18:42:50,388:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:50,757:INFO:Calculating mean and std
2024-02-05 18:42:50,758:INFO:Creating metrics dataframe
2024-02-05 18:42:50,761:INFO:Uploading results into container
2024-02-05 18:42:50,762:INFO:Uploading model into container now
2024-02-05 18:42:50,762:INFO:_master_model_container: 11
2024-02-05 18:42:50,762:INFO:_display_container: 2
2024-02-05 18:42:50,762:INFO:KNeighborsRegressor(n_jobs=-1)
2024-02-05 18:42:50,763:INFO:create_model() successfully completed......................................
2024-02-05 18:42:50,857:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:50,857:INFO:Creating metrics dataframe
2024-02-05 18:42:50,866:INFO:Initializing Decision Tree Regressor
2024-02-05 18:42:50,866:INFO:Total runtime is 0.15478684107462565 minutes
2024-02-05 18:42:50,870:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:50,870:INFO:Initializing create_model()
2024-02-05 18:42:50,870:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=dt, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:50,870:INFO:Checking exceptions
2024-02-05 18:42:50,870:INFO:Importing libraries
2024-02-05 18:42:50,870:INFO:Copying training dataset
2024-02-05 18:42:50,876:INFO:Defining folds
2024-02-05 18:42:50,876:INFO:Declaring metric variables
2024-02-05 18:42:50,878:INFO:Importing untrained model
2024-02-05 18:42:50,884:INFO:Decision Tree Regressor Imported successfully
2024-02-05 18:42:50,890:INFO:Starting cross validation
2024-02-05 18:42:50,892:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:51,220:INFO:Calculating mean and std
2024-02-05 18:42:51,220:INFO:Creating metrics dataframe
2024-02-05 18:42:51,223:INFO:Uploading results into container
2024-02-05 18:42:51,223:INFO:Uploading model into container now
2024-02-05 18:42:51,223:INFO:_master_model_container: 12
2024-02-05 18:42:51,223:INFO:_display_container: 2
2024-02-05 18:42:51,224:INFO:DecisionTreeRegressor(random_state=123)
2024-02-05 18:42:51,224:INFO:create_model() successfully completed......................................
2024-02-05 18:42:51,318:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:51,319:INFO:Creating metrics dataframe
2024-02-05 18:42:51,327:INFO:Initializing Random Forest Regressor
2024-02-05 18:42:51,327:INFO:Total runtime is 0.16246784925460814 minutes
2024-02-05 18:42:51,332:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:51,333:INFO:Initializing create_model()
2024-02-05 18:42:51,333:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=rf, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:51,333:INFO:Checking exceptions
2024-02-05 18:42:51,333:INFO:Importing libraries
2024-02-05 18:42:51,333:INFO:Copying training dataset
2024-02-05 18:42:51,339:INFO:Defining folds
2024-02-05 18:42:51,339:INFO:Declaring metric variables
2024-02-05 18:42:51,341:INFO:Importing untrained model
2024-02-05 18:42:51,345:INFO:Random Forest Regressor Imported successfully
2024-02-05 18:42:51,351:INFO:Starting cross validation
2024-02-05 18:42:51,355:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:52,900:INFO:Calculating mean and std
2024-02-05 18:42:52,902:INFO:Creating metrics dataframe
2024-02-05 18:42:52,905:INFO:Uploading results into container
2024-02-05 18:42:52,905:INFO:Uploading model into container now
2024-02-05 18:42:52,906:INFO:_master_model_container: 13
2024-02-05 18:42:52,906:INFO:_display_container: 2
2024-02-05 18:42:52,907:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2024-02-05 18:42:52,907:INFO:create_model() successfully completed......................................
2024-02-05 18:42:53,001:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:53,001:INFO:Creating metrics dataframe
2024-02-05 18:42:53,009:INFO:Initializing Extra Trees Regressor
2024-02-05 18:42:53,009:INFO:Total runtime is 0.19050227403640746 minutes
2024-02-05 18:42:53,013:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:53,013:INFO:Initializing create_model()
2024-02-05 18:42:53,013:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=et, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:53,013:INFO:Checking exceptions
2024-02-05 18:42:53,013:INFO:Importing libraries
2024-02-05 18:42:53,013:INFO:Copying training dataset
2024-02-05 18:42:53,019:INFO:Defining folds
2024-02-05 18:42:53,019:INFO:Declaring metric variables
2024-02-05 18:42:53,023:INFO:Importing untrained model
2024-02-05 18:42:53,027:INFO:Extra Trees Regressor Imported successfully
2024-02-05 18:42:53,033:INFO:Starting cross validation
2024-02-05 18:42:53,035:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:54,098:INFO:Calculating mean and std
2024-02-05 18:42:54,099:INFO:Creating metrics dataframe
2024-02-05 18:42:54,102:INFO:Uploading results into container
2024-02-05 18:42:54,103:INFO:Uploading model into container now
2024-02-05 18:42:54,103:INFO:_master_model_container: 14
2024-02-05 18:42:54,103:INFO:_display_container: 2
2024-02-05 18:42:54,103:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2024-02-05 18:42:54,103:INFO:create_model() successfully completed......................................
2024-02-05 18:42:54,195:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:54,195:INFO:Creating metrics dataframe
2024-02-05 18:42:54,207:INFO:Initializing AdaBoost Regressor
2024-02-05 18:42:54,207:INFO:Total runtime is 0.21047387123107908 minutes
2024-02-05 18:42:54,210:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:54,211:INFO:Initializing create_model()
2024-02-05 18:42:54,211:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=ada, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:54,211:INFO:Checking exceptions
2024-02-05 18:42:54,211:INFO:Importing libraries
2024-02-05 18:42:54,211:INFO:Copying training dataset
2024-02-05 18:42:54,216:INFO:Defining folds
2024-02-05 18:42:54,216:INFO:Declaring metric variables
2024-02-05 18:42:54,219:INFO:Importing untrained model
2024-02-05 18:42:54,225:INFO:AdaBoost Regressor Imported successfully
2024-02-05 18:42:54,230:INFO:Starting cross validation
2024-02-05 18:42:54,232:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:54,789:INFO:Calculating mean and std
2024-02-05 18:42:54,790:INFO:Creating metrics dataframe
2024-02-05 18:42:54,793:INFO:Uploading results into container
2024-02-05 18:42:54,793:INFO:Uploading model into container now
2024-02-05 18:42:54,793:INFO:_master_model_container: 15
2024-02-05 18:42:54,793:INFO:_display_container: 2
2024-02-05 18:42:54,793:INFO:AdaBoostRegressor(random_state=123)
2024-02-05 18:42:54,794:INFO:create_model() successfully completed......................................
2024-02-05 18:42:54,889:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:54,889:INFO:Creating metrics dataframe
2024-02-05 18:42:54,898:INFO:Initializing Gradient Boosting Regressor
2024-02-05 18:42:54,898:INFO:Total runtime is 0.2219903230667114 minutes
2024-02-05 18:42:54,902:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:54,902:INFO:Initializing create_model()
2024-02-05 18:42:54,903:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=gbr, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:54,903:INFO:Checking exceptions
2024-02-05 18:42:54,903:INFO:Importing libraries
2024-02-05 18:42:54,904:INFO:Copying training dataset
2024-02-05 18:42:54,909:INFO:Defining folds
2024-02-05 18:42:54,909:INFO:Declaring metric variables
2024-02-05 18:42:54,912:INFO:Importing untrained model
2024-02-05 18:42:54,915:INFO:Gradient Boosting Regressor Imported successfully
2024-02-05 18:42:54,920:INFO:Starting cross validation
2024-02-05 18:42:54,922:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:55,685:INFO:Calculating mean and std
2024-02-05 18:42:55,687:INFO:Creating metrics dataframe
2024-02-05 18:42:55,689:INFO:Uploading results into container
2024-02-05 18:42:55,689:INFO:Uploading model into container now
2024-02-05 18:42:55,690:INFO:_master_model_container: 16
2024-02-05 18:42:55,690:INFO:_display_container: 2
2024-02-05 18:42:55,690:INFO:GradientBoostingRegressor(random_state=123)
2024-02-05 18:42:55,690:INFO:create_model() successfully completed......................................
2024-02-05 18:42:55,782:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:55,782:INFO:Creating metrics dataframe
2024-02-05 18:42:55,791:INFO:Initializing Light Gradient Boosting Machine
2024-02-05 18:42:55,792:INFO:Total runtime is 0.23688533306121823 minutes
2024-02-05 18:42:55,794:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:55,794:INFO:Initializing create_model()
2024-02-05 18:42:55,794:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=lightgbm, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:55,794:INFO:Checking exceptions
2024-02-05 18:42:55,795:INFO:Importing libraries
2024-02-05 18:42:55,795:INFO:Copying training dataset
2024-02-05 18:42:55,801:INFO:Defining folds
2024-02-05 18:42:55,801:INFO:Declaring metric variables
2024-02-05 18:42:55,804:INFO:Importing untrained model
2024-02-05 18:42:55,808:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-05 18:42:55,814:INFO:Starting cross validation
2024-02-05 18:42:55,815:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:56,532:INFO:Calculating mean and std
2024-02-05 18:42:56,534:INFO:Creating metrics dataframe
2024-02-05 18:42:56,536:INFO:Uploading results into container
2024-02-05 18:42:56,537:INFO:Uploading model into container now
2024-02-05 18:42:56,537:INFO:_master_model_container: 17
2024-02-05 18:42:56,537:INFO:_display_container: 2
2024-02-05 18:42:56,538:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-05 18:42:56,538:INFO:create_model() successfully completed......................................
2024-02-05 18:42:56,646:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:56,647:INFO:Creating metrics dataframe
2024-02-05 18:42:56,655:INFO:Initializing Dummy Regressor
2024-02-05 18:42:56,655:INFO:Total runtime is 0.25127256711324053 minutes
2024-02-05 18:42:56,658:INFO:SubProcess create_model() called ==================================
2024-02-05 18:42:56,658:INFO:Initializing create_model()
2024-02-05 18:42:56,658:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=dummy, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B322A2B0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:56,658:INFO:Checking exceptions
2024-02-05 18:42:56,658:INFO:Importing libraries
2024-02-05 18:42:56,659:INFO:Copying training dataset
2024-02-05 18:42:56,663:INFO:Defining folds
2024-02-05 18:42:56,663:INFO:Declaring metric variables
2024-02-05 18:42:56,667:INFO:Importing untrained model
2024-02-05 18:42:56,669:INFO:Dummy Regressor Imported successfully
2024-02-05 18:42:56,673:INFO:Starting cross validation
2024-02-05 18:42:56,674:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:42:56,940:INFO:Calculating mean and std
2024-02-05 18:42:56,940:INFO:Creating metrics dataframe
2024-02-05 18:42:56,943:INFO:Uploading results into container
2024-02-05 18:42:56,944:INFO:Uploading model into container now
2024-02-05 18:42:56,944:INFO:_master_model_container: 18
2024-02-05 18:42:56,944:INFO:_display_container: 2
2024-02-05 18:42:56,944:INFO:DummyRegressor()
2024-02-05 18:42:56,944:INFO:create_model() successfully completed......................................
2024-02-05 18:42:57,029:INFO:SubProcess create_model() end ==================================
2024-02-05 18:42:57,030:INFO:Creating metrics dataframe
2024-02-05 18:42:57,043:INFO:Initializing create_model()
2024-02-05 18:42:57,044:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:42:57,044:INFO:Checking exceptions
2024-02-05 18:42:57,045:INFO:Importing libraries
2024-02-05 18:42:57,045:INFO:Copying training dataset
2024-02-05 18:42:57,049:INFO:Defining folds
2024-02-05 18:42:57,049:INFO:Declaring metric variables
2024-02-05 18:42:57,049:INFO:Importing untrained model
2024-02-05 18:42:57,049:INFO:Declaring custom model
2024-02-05 18:42:57,049:INFO:Gradient Boosting Regressor Imported successfully
2024-02-05 18:42:57,051:INFO:Cross validation set to False
2024-02-05 18:42:57,051:INFO:Fitting Model
2024-02-05 18:42:57,614:INFO:GradientBoostingRegressor(random_state=123)
2024-02-05 18:42:57,614:INFO:create_model() successfully completed......................................
2024-02-05 18:42:57,704:INFO:Creating Dashboard logs
2024-02-05 18:42:57,708:INFO:Model: Gradient Boosting Regressor
2024-02-05 18:42:57,734:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-05 18:42:57,800:INFO:Initializing predict_model()
2024-02-05 18:42:57,801:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D7B295F430>)
2024-02-05 18:42:57,801:INFO:Checking exceptions
2024-02-05 18:42:57,801:INFO:Preloading libraries
2024-02-05 18:42:58,680:INFO:Creating Dashboard logs
2024-02-05 18:42:58,683:INFO:Model: Light Gradient Boosting Machine
2024-02-05 18:42:58,708:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2024-02-05 18:42:58,884:INFO:Creating Dashboard logs
2024-02-05 18:42:58,888:INFO:Model: Extra Trees Regressor
2024-02-05 18:42:58,912:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-02-05 18:42:59,086:INFO:Creating Dashboard logs
2024-02-05 18:42:59,090:INFO:Model: Random Forest Regressor
2024-02-05 18:42:59,113:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-02-05 18:42:59,285:INFO:Creating Dashboard logs
2024-02-05 18:42:59,288:INFO:Model: Ridge Regression
2024-02-05 18:42:59,312:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}
2024-02-05 18:42:59,481:INFO:Creating Dashboard logs
2024-02-05 18:42:59,485:INFO:Model: Lasso Least Angle Regression
2024-02-05 18:42:59,506:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'max_iter': 500, 'normalize': 'deprecated', 'positive': False, 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2024-02-05 18:42:59,674:INFO:Creating Dashboard logs
2024-02-05 18:42:59,677:INFO:Model: Lasso Regression
2024-02-05 18:42:59,701:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2024-02-05 18:42:59,868:INFO:Creating Dashboard logs
2024-02-05 18:42:59,871:INFO:Model: Elastic Net
2024-02-05 18:42:59,894:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2024-02-05 18:43:00,068:INFO:Creating Dashboard logs
2024-02-05 18:43:00,071:INFO:Model: Bayesian Ridge
2024-02-05 18:43:00,092:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'n_iter': 300, 'tol': 0.001, 'verbose': False}
2024-02-05 18:43:00,258:INFO:Creating Dashboard logs
2024-02-05 18:43:00,260:INFO:Model: AdaBoost Regressor
2024-02-05 18:43:00,283:INFO:Logged params: {'base_estimator': 'deprecated', 'estimator': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50, 'random_state': 123}
2024-02-05 18:43:00,444:INFO:Creating Dashboard logs
2024-02-05 18:43:00,447:INFO:Model: Orthogonal Matching Pursuit
2024-02-05 18:43:00,471:INFO:Logged params: {'fit_intercept': True, 'n_nonzero_coefs': None, 'normalize': 'deprecated', 'precompute': 'auto', 'tol': None}
2024-02-05 18:43:00,635:INFO:Creating Dashboard logs
2024-02-05 18:43:00,638:INFO:Model: Decision Tree Regressor
2024-02-05 18:43:00,663:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 123, 'splitter': 'best'}
2024-02-05 18:43:00,830:INFO:Creating Dashboard logs
2024-02-05 18:43:00,833:INFO:Model: Huber Regressor
2024-02-05 18:43:00,857:INFO:Logged params: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': True, 'max_iter': 100, 'tol': 1e-05, 'warm_start': False}
2024-02-05 18:43:01,022:INFO:Creating Dashboard logs
2024-02-05 18:43:01,024:INFO:Model: K Neighbors Regressor
2024-02-05 18:43:01,046:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2024-02-05 18:43:01,240:INFO:Creating Dashboard logs
2024-02-05 18:43:01,244:INFO:Model: Passive Aggressive Regressor
2024-02-05 18:43:01,266:INFO:Logged params: {'C': 1.0, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'fit_intercept': True, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'n_iter_no_change': 5, 'random_state': 123, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-05 18:43:01,439:INFO:Creating Dashboard logs
2024-02-05 18:43:01,442:INFO:Model: Dummy Regressor
2024-02-05 18:43:01,465:INFO:Logged params: {'constant': None, 'quantile': None, 'strategy': 'mean'}
2024-02-05 18:43:01,631:INFO:Creating Dashboard logs
2024-02-05 18:43:01,634:INFO:Model: Linear Regression
2024-02-05 18:43:01,658:INFO:Logged params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'positive': False}
2024-02-05 18:43:01,818:INFO:Creating Dashboard logs
2024-02-05 18:43:01,821:INFO:Model: Least Angle Regression
2024-02-05 18:43:01,845:INFO:Logged params: {'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'n_nonzero_coefs': 500, 'normalize': 'deprecated', 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2024-02-05 18:43:02,023:INFO:_master_model_container: 18
2024-02-05 18:43:02,024:INFO:_display_container: 2
2024-02-05 18:43:02,024:INFO:GradientBoostingRegressor(random_state=123)
2024-02-05 18:43:02,024:INFO:compare_models() successfully completed......................................
2024-02-05 18:43:02,223:INFO:Initializing plot_model()
2024-02-05 18:43:02,223:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, system=True)
2024-02-05 18:43:02,223:INFO:Checking exceptions
2024-02-05 18:43:02,229:INFO:Preloading libraries
2024-02-05 18:43:02,236:INFO:Copying training dataset
2024-02-05 18:43:02,237:INFO:Plot type: residuals
2024-02-05 18:43:02,534:INFO:Fitting Model
2024-02-05 18:43:02,535:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names
  warnings.warn(

2024-02-05 18:43:02,562:INFO:Scoring test/hold-out set
2024-02-05 18:43:02,875:INFO:Visual Rendered Successfully
2024-02-05 18:43:02,971:INFO:plot_model() successfully completed......................................
2024-02-05 18:43:03,011:INFO:Initializing plot_model()
2024-02-05 18:43:03,011:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, system=True)
2024-02-05 18:43:03,011:INFO:Checking exceptions
2024-02-05 18:43:03,021:INFO:Preloading libraries
2024-02-05 18:43:03,027:INFO:Copying training dataset
2024-02-05 18:43:03,027:INFO:Plot type: error
2024-02-05 18:43:03,268:INFO:Fitting Model
2024-02-05 18:43:03,268:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names
  warnings.warn(

2024-02-05 18:43:03,268:INFO:Scoring test/hold-out set
2024-02-05 18:43:03,412:INFO:Visual Rendered Successfully
2024-02-05 18:43:03,506:INFO:plot_model() successfully completed......................................
2024-02-05 18:43:03,545:INFO:Initializing plot_model()
2024-02-05 18:43:03,545:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, system=True)
2024-02-05 18:43:03,545:INFO:Checking exceptions
2024-02-05 18:43:03,545:INFO:Preloading libraries
2024-02-05 18:43:03,557:INFO:Copying training dataset
2024-02-05 18:43:03,557:INFO:Plot type: feature
2024-02-05 18:43:03,557:WARNING:No coef_ found. Trying feature_importances_
2024-02-05 18:43:03,692:INFO:Visual Rendered Successfully
2024-02-05 18:43:03,786:INFO:plot_model() successfully completed......................................
2024-02-05 18:43:03,822:INFO:Initializing create_model()
2024-02-05 18:43:03,822:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:03,822:INFO:Checking exceptions
2024-02-05 18:43:03,836:INFO:Importing libraries
2024-02-05 18:43:03,836:INFO:Copying training dataset
2024-02-05 18:43:03,842:INFO:Defining folds
2024-02-05 18:43:03,842:INFO:Declaring metric variables
2024-02-05 18:43:03,844:INFO:Importing untrained model
2024-02-05 18:43:03,844:INFO:Declaring custom model
2024-02-05 18:43:03,848:INFO:Gradient Boosting Regressor Imported successfully
2024-02-05 18:43:03,851:INFO:Starting cross validation
2024-02-05 18:43:03,851:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:05,664:INFO:Calculating mean and std
2024-02-05 18:43:05,665:INFO:Creating metrics dataframe
2024-02-05 18:43:05,669:INFO:Finalizing model
2024-02-05 18:43:06,236:INFO:Creating Dashboard logs
2024-02-05 18:43:06,239:INFO:Model: Gradient Boosting Regressor
2024-02-05 18:43:06,265:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-05 18:43:06,323:INFO:Initializing predict_model()
2024-02-05 18:43:06,323:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D7B33AB3A0>)
2024-02-05 18:43:06,323:INFO:Checking exceptions
2024-02-05 18:43:06,323:INFO:Preloading libraries
2024-02-05 18:43:06,785:INFO:Uploading results into container
2024-02-05 18:43:06,785:INFO:Uploading model into container now
2024-02-05 18:43:06,796:INFO:_master_model_container: 19
2024-02-05 18:43:06,796:INFO:_display_container: 3
2024-02-05 18:43:06,796:INFO:GradientBoostingRegressor(random_state=123)
2024-02-05 18:43:06,796:INFO:create_model() successfully completed......................................
2024-02-05 18:43:06,924:INFO:Initializing create_model()
2024-02-05 18:43:06,925:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=lightgbm, fold=3, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:06,925:INFO:Checking exceptions
2024-02-05 18:43:06,938:INFO:Importing libraries
2024-02-05 18:43:06,938:INFO:Copying training dataset
2024-02-05 18:43:06,943:INFO:Defining folds
2024-02-05 18:43:06,943:INFO:Declaring metric variables
2024-02-05 18:43:06,946:INFO:Importing untrained model
2024-02-05 18:43:06,951:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-05 18:43:06,957:INFO:Starting cross validation
2024-02-05 18:43:06,959:INFO:Cross validating with KFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:07,448:INFO:Calculating mean and std
2024-02-05 18:43:07,449:INFO:Creating metrics dataframe
2024-02-05 18:43:07,454:INFO:Finalizing model
2024-02-05 18:43:07,613:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-02-05 18:43:07,615:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000465 seconds.
2024-02-05 18:43:07,615:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-05 18:43:07,615:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-05 18:43:07,616:INFO:[LightGBM] [Info] Total Bins 2881
2024-02-05 18:43:07,616:INFO:[LightGBM] [Info] Number of data points in the train set: 1019, number of used features: 93
2024-02-05 18:43:07,616:INFO:[LightGBM] [Info] Start training from score 180679.625123
2024-02-05 18:43:07,694:INFO:Creating Dashboard logs
2024-02-05 18:43:07,697:INFO:Model: Light Gradient Boosting Machine
2024-02-05 18:43:07,727:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2024-02-05 18:43:07,799:INFO:Initializing predict_model()
2024-02-05 18:43:07,799:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D7B2AD8430>)
2024-02-05 18:43:07,799:INFO:Checking exceptions
2024-02-05 18:43:07,799:INFO:Preloading libraries
2024-02-05 18:43:08,262:INFO:Uploading results into container
2024-02-05 18:43:08,262:INFO:Uploading model into container now
2024-02-05 18:43:08,271:INFO:_master_model_container: 20
2024-02-05 18:43:08,271:INFO:_display_container: 4
2024-02-05 18:43:08,272:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-05 18:43:08,272:INFO:create_model() successfully completed......................................
2024-02-05 18:43:08,428:INFO:Initializing compare_models()
2024-02-05 18:43:08,428:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, include=['lightgbm', 'gbr', 'br'], fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, 'include': ['lightgbm', 'gbr', 'br'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2024-02-05 18:43:08,428:INFO:Checking exceptions
2024-02-05 18:43:08,428:INFO:Preparing display monitor
2024-02-05 18:43:08,455:INFO:Initializing Light Gradient Boosting Machine
2024-02-05 18:43:08,455:INFO:Total runtime is 0.0 minutes
2024-02-05 18:43:08,489:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:08,489:INFO:Initializing create_model()
2024-02-05 18:43:08,489:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2A85070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:08,489:INFO:Checking exceptions
2024-02-05 18:43:08,489:INFO:Importing libraries
2024-02-05 18:43:08,489:INFO:Copying training dataset
2024-02-05 18:43:08,495:INFO:Defining folds
2024-02-05 18:43:08,495:INFO:Declaring metric variables
2024-02-05 18:43:08,501:INFO:Importing untrained model
2024-02-05 18:43:08,504:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-05 18:43:08,514:INFO:Starting cross validation
2024-02-05 18:43:08,517:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:10,200:INFO:Calculating mean and std
2024-02-05 18:43:10,201:INFO:Creating metrics dataframe
2024-02-05 18:43:10,204:INFO:Uploading results into container
2024-02-05 18:43:10,205:INFO:Uploading model into container now
2024-02-05 18:43:10,206:INFO:_master_model_container: 21
2024-02-05 18:43:10,207:INFO:_display_container: 5
2024-02-05 18:43:10,207:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-05 18:43:10,207:INFO:create_model() successfully completed......................................
2024-02-05 18:43:10,323:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:10,323:INFO:Creating metrics dataframe
2024-02-05 18:43:10,329:INFO:Initializing Gradient Boosting Regressor
2024-02-05 18:43:10,329:INFO:Total runtime is 0.03123361269632975 minutes
2024-02-05 18:43:10,333:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:10,333:INFO:Initializing create_model()
2024-02-05 18:43:10,333:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2A85070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:10,333:INFO:Checking exceptions
2024-02-05 18:43:10,333:INFO:Importing libraries
2024-02-05 18:43:10,333:INFO:Copying training dataset
2024-02-05 18:43:10,337:INFO:Defining folds
2024-02-05 18:43:10,337:INFO:Declaring metric variables
2024-02-05 18:43:10,353:INFO:Importing untrained model
2024-02-05 18:43:10,369:INFO:Gradient Boosting Regressor Imported successfully
2024-02-05 18:43:10,369:INFO:Starting cross validation
2024-02-05 18:43:10,369:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:12,086:INFO:Calculating mean and std
2024-02-05 18:43:12,087:INFO:Creating metrics dataframe
2024-02-05 18:43:12,089:INFO:Uploading results into container
2024-02-05 18:43:12,090:INFO:Uploading model into container now
2024-02-05 18:43:12,090:INFO:_master_model_container: 22
2024-02-05 18:43:12,090:INFO:_display_container: 5
2024-02-05 18:43:12,090:INFO:GradientBoostingRegressor(random_state=123)
2024-02-05 18:43:12,091:INFO:create_model() successfully completed......................................
2024-02-05 18:43:12,186:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:12,186:INFO:Creating metrics dataframe
2024-02-05 18:43:12,192:INFO:Initializing Bayesian Ridge
2024-02-05 18:43:12,193:INFO:Total runtime is 0.06229335069656372 minutes
2024-02-05 18:43:12,197:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:12,197:INFO:Initializing create_model()
2024-02-05 18:43:12,197:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2A85070>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:12,197:INFO:Checking exceptions
2024-02-05 18:43:12,197:INFO:Importing libraries
2024-02-05 18:43:12,197:INFO:Copying training dataset
2024-02-05 18:43:12,202:INFO:Defining folds
2024-02-05 18:43:12,203:INFO:Declaring metric variables
2024-02-05 18:43:12,237:INFO:Importing untrained model
2024-02-05 18:43:12,244:INFO:Bayesian Ridge Imported successfully
2024-02-05 18:43:12,252:INFO:Starting cross validation
2024-02-05 18:43:12,258:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:12,904:INFO:Calculating mean and std
2024-02-05 18:43:12,905:INFO:Creating metrics dataframe
2024-02-05 18:43:12,908:INFO:Uploading results into container
2024-02-05 18:43:12,909:INFO:Uploading model into container now
2024-02-05 18:43:12,909:INFO:_master_model_container: 23
2024-02-05 18:43:12,909:INFO:_display_container: 5
2024-02-05 18:43:12,910:INFO:BayesianRidge()
2024-02-05 18:43:12,910:INFO:create_model() successfully completed......................................
2024-02-05 18:43:13,003:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:13,003:INFO:Creating metrics dataframe
2024-02-05 18:43:13,023:INFO:Initializing create_model()
2024-02-05 18:43:13,023:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:13,024:INFO:Checking exceptions
2024-02-05 18:43:13,025:INFO:Importing libraries
2024-02-05 18:43:13,025:INFO:Copying training dataset
2024-02-05 18:43:13,030:INFO:Defining folds
2024-02-05 18:43:13,030:INFO:Declaring metric variables
2024-02-05 18:43:13,030:INFO:Importing untrained model
2024-02-05 18:43:13,030:INFO:Declaring custom model
2024-02-05 18:43:13,030:INFO:Gradient Boosting Regressor Imported successfully
2024-02-05 18:43:13,032:INFO:Cross validation set to False
2024-02-05 18:43:13,032:INFO:Fitting Model
2024-02-05 18:43:13,595:INFO:GradientBoostingRegressor(random_state=123)
2024-02-05 18:43:13,596:INFO:create_model() successfully completed......................................
2024-02-05 18:43:13,695:INFO:Creating Dashboard logs
2024-02-05 18:43:13,702:INFO:Model: Gradient Boosting Regressor
2024-02-05 18:43:13,735:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-05 18:43:13,796:INFO:Initializing predict_model()
2024-02-05 18:43:13,796:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D7B30C18B0>)
2024-02-05 18:43:13,796:INFO:Checking exceptions
2024-02-05 18:43:13,796:INFO:Preloading libraries
2024-02-05 18:43:14,213:INFO:Creating Dashboard logs
2024-02-05 18:43:14,216:INFO:Model: Light Gradient Boosting Machine
2024-02-05 18:43:14,241:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2024-02-05 18:43:14,422:INFO:Creating Dashboard logs
2024-02-05 18:43:14,424:INFO:Model: Bayesian Ridge
2024-02-05 18:43:14,447:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'n_iter': 300, 'tol': 0.001, 'verbose': False}
2024-02-05 18:43:14,633:INFO:_master_model_container: 23
2024-02-05 18:43:14,633:INFO:_display_container: 5
2024-02-05 18:43:14,634:INFO:GradientBoostingRegressor(random_state=123)
2024-02-05 18:43:14,634:INFO:compare_models() successfully completed......................................
2024-02-05 18:43:14,751:INFO:Initializing compare_models()
2024-02-05 18:43:14,751:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, include=None, fold=None, round=4, cross_validation=True, sort=MAE, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MAE', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2024-02-05 18:43:14,751:INFO:Checking exceptions
2024-02-05 18:43:14,754:INFO:Preparing display monitor
2024-02-05 18:43:14,774:INFO:Initializing Linear Regression
2024-02-05 18:43:14,774:INFO:Total runtime is 0.0 minutes
2024-02-05 18:43:14,777:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:14,777:INFO:Initializing create_model()
2024-02-05 18:43:14,777:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:14,777:INFO:Checking exceptions
2024-02-05 18:43:14,777:INFO:Importing libraries
2024-02-05 18:43:14,777:INFO:Copying training dataset
2024-02-05 18:43:14,781:INFO:Defining folds
2024-02-05 18:43:14,781:INFO:Declaring metric variables
2024-02-05 18:43:14,784:INFO:Importing untrained model
2024-02-05 18:43:14,786:INFO:Linear Regression Imported successfully
2024-02-05 18:43:14,793:INFO:Starting cross validation
2024-02-05 18:43:14,794:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:15,449:INFO:Calculating mean and std
2024-02-05 18:43:15,450:INFO:Creating metrics dataframe
2024-02-05 18:43:15,452:INFO:Uploading results into container
2024-02-05 18:43:15,452:INFO:Uploading model into container now
2024-02-05 18:43:15,453:INFO:_master_model_container: 24
2024-02-05 18:43:15,453:INFO:_display_container: 6
2024-02-05 18:43:15,453:INFO:LinearRegression(n_jobs=-1)
2024-02-05 18:43:15,453:INFO:create_model() successfully completed......................................
2024-02-05 18:43:15,544:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:15,544:INFO:Creating metrics dataframe
2024-02-05 18:43:15,550:INFO:Initializing Lasso Regression
2024-02-05 18:43:15,550:INFO:Total runtime is 0.012944793701171875 minutes
2024-02-05 18:43:15,553:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:15,553:INFO:Initializing create_model()
2024-02-05 18:43:15,553:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:15,553:INFO:Checking exceptions
2024-02-05 18:43:15,553:INFO:Importing libraries
2024-02-05 18:43:15,553:INFO:Copying training dataset
2024-02-05 18:43:15,558:INFO:Defining folds
2024-02-05 18:43:15,558:INFO:Declaring metric variables
2024-02-05 18:43:15,562:INFO:Importing untrained model
2024-02-05 18:43:15,565:INFO:Lasso Regression Imported successfully
2024-02-05 18:43:15,570:INFO:Starting cross validation
2024-02-05 18:43:15,572:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:15,937:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.241e+11, tolerance: 5.445e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:15,947:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.631e+11, tolerance: 5.696e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:15,950:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.799e+11, tolerance: 5.738e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:15,950:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.657e+11, tolerance: 5.662e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:15,963:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.569e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:15,965:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.692e+11, tolerance: 5.599e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:15,971:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.375e+11, tolerance: 5.336e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:15,977:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.563e+11, tolerance: 5.378e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:16,281:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.674e+11, tolerance: 5.647e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:16,288:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.671e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:16,338:INFO:Calculating mean and std
2024-02-05 18:43:16,338:INFO:Creating metrics dataframe
2024-02-05 18:43:16,338:INFO:Uploading results into container
2024-02-05 18:43:16,338:INFO:Uploading model into container now
2024-02-05 18:43:16,338:INFO:_master_model_container: 25
2024-02-05 18:43:16,338:INFO:_display_container: 6
2024-02-05 18:43:16,338:INFO:Lasso(random_state=123)
2024-02-05 18:43:16,338:INFO:create_model() successfully completed......................................
2024-02-05 18:43:16,441:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:16,442:INFO:Creating metrics dataframe
2024-02-05 18:43:16,449:INFO:Initializing Ridge Regression
2024-02-05 18:43:16,449:INFO:Total runtime is 0.027923301855723063 minutes
2024-02-05 18:43:16,452:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:16,453:INFO:Initializing create_model()
2024-02-05 18:43:16,453:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:16,453:INFO:Checking exceptions
2024-02-05 18:43:16,453:INFO:Importing libraries
2024-02-05 18:43:16,453:INFO:Copying training dataset
2024-02-05 18:43:16,457:INFO:Defining folds
2024-02-05 18:43:16,458:INFO:Declaring metric variables
2024-02-05 18:43:16,462:INFO:Importing untrained model
2024-02-05 18:43:16,467:INFO:Ridge Regression Imported successfully
2024-02-05 18:43:16,474:INFO:Starting cross validation
2024-02-05 18:43:16,477:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:17,089:INFO:Calculating mean and std
2024-02-05 18:43:17,090:INFO:Creating metrics dataframe
2024-02-05 18:43:17,092:INFO:Uploading results into container
2024-02-05 18:43:17,093:INFO:Uploading model into container now
2024-02-05 18:43:17,093:INFO:_master_model_container: 26
2024-02-05 18:43:17,093:INFO:_display_container: 6
2024-02-05 18:43:17,094:INFO:Ridge(random_state=123)
2024-02-05 18:43:17,094:INFO:create_model() successfully completed......................................
2024-02-05 18:43:17,192:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:17,193:INFO:Creating metrics dataframe
2024-02-05 18:43:17,200:INFO:Initializing Elastic Net
2024-02-05 18:43:17,200:INFO:Total runtime is 0.04043509165445963 minutes
2024-02-05 18:43:17,204:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:17,204:INFO:Initializing create_model()
2024-02-05 18:43:17,204:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:17,204:INFO:Checking exceptions
2024-02-05 18:43:17,204:INFO:Importing libraries
2024-02-05 18:43:17,204:INFO:Copying training dataset
2024-02-05 18:43:17,209:INFO:Defining folds
2024-02-05 18:43:17,209:INFO:Declaring metric variables
2024-02-05 18:43:17,211:INFO:Importing untrained model
2024-02-05 18:43:17,216:INFO:Elastic Net Imported successfully
2024-02-05 18:43:17,222:INFO:Starting cross validation
2024-02-05 18:43:17,231:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:17,611:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.889e+11, tolerance: 5.696e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:17,635:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.858e+11, tolerance: 5.662e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:17,636:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.721e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:17,661:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+11, tolerance: 5.378e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:17,661:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.761e+11, tolerance: 5.445e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:17,666:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.602e+11, tolerance: 5.336e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:17,667:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.863e+11, tolerance: 5.599e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:17,669:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.013e+11, tolerance: 5.738e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:17,919:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.964e+11, tolerance: 5.647e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:17,937:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.975e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 18:43:18,007:INFO:Calculating mean and std
2024-02-05 18:43:18,008:INFO:Creating metrics dataframe
2024-02-05 18:43:18,010:INFO:Uploading results into container
2024-02-05 18:43:18,011:INFO:Uploading model into container now
2024-02-05 18:43:18,011:INFO:_master_model_container: 27
2024-02-05 18:43:18,011:INFO:_display_container: 6
2024-02-05 18:43:18,012:INFO:ElasticNet(random_state=123)
2024-02-05 18:43:18,012:INFO:create_model() successfully completed......................................
2024-02-05 18:43:18,111:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:18,112:INFO:Creating metrics dataframe
2024-02-05 18:43:18,122:INFO:Initializing Least Angle Regression
2024-02-05 18:43:18,122:INFO:Total runtime is 0.05580114126205444 minutes
2024-02-05 18:43:18,126:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:18,126:INFO:Initializing create_model()
2024-02-05 18:43:18,126:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:18,126:INFO:Checking exceptions
2024-02-05 18:43:18,126:INFO:Importing libraries
2024-02-05 18:43:18,126:INFO:Copying training dataset
2024-02-05 18:43:18,131:INFO:Defining folds
2024-02-05 18:43:18,131:INFO:Declaring metric variables
2024-02-05 18:43:18,134:INFO:Importing untrained model
2024-02-05 18:43:18,138:INFO:Least Angle Regression Imported successfully
2024-02-05 18:43:18,144:INFO:Starting cross validation
2024-02-05 18:43:18,146:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:18,432:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.718e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,437:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.851e+02, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,447:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.911e+02, with an active set of 99 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,454:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=1.924e+05, with an active set of 109 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,454:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 160 iterations, i.e. alpha=1.780e+05, with an active set of 112 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,457:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.157e+03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,459:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.555e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,459:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=3.936e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,459:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.731e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,461:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.412e+02, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,463:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.519e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,466:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=1.562e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,467:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.496e+09, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,468:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=5.167e+07, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,468:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=3.422e+07, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,469:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=2.687e+07, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,470:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=2.606e+07, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,470:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=2.275e+07, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,471:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.908e+07, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,471:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=1.562e+07, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,471:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.264e+07, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,471:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.442e+07, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,472:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.173e+07, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,472:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.137e+07, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,472:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.059e+07, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,473:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=1.029e+07, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,473:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=9.833e+06, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,474:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=9.103e+06, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,474:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=8.950e+06, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,474:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=7.986e+06, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,475:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=7.476e+06, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,475:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=7.475e+06, with an active set of 91 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,475:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.243e+06, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,475:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.242e+06, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,477:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=6.703e+06, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,478:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=5.425e+06, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,478:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=5.332e+06, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,478:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=4.831e+06, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,479:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=4.529e+06, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,480:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=3.694e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,480:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=3.221e+06, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,480:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=2.801e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,481:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 115 iterations, i.e. alpha=2.370e+06, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,482:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.244e+12, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,482:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=1.750e+11, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,482:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.543e+11, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,482:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=3.621e+02, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,483:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.062e+11, with an active set of 107 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,484:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.027e+11, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,484:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=7.904e+10, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,485:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=7.308e+10, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,485:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=6.501e+10, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,485:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=5.857e+10, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,485:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=5.713e+10, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,486:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.149e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,486:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=5.810e+10, with an active set of 113 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,486:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.051e+02, with an active set of 72 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,486:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=5.809e+10, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=3.100e+10, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,494:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=9.162e+13, with an active set of 131 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,494:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=7.581e+13, with an active set of 131 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=2.116e+02, with an active set of 116 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,500:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=1.924e+02, with an active set of 116 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,501:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 146 iterations, i.e. alpha=1.674e+02, with an active set of 121 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,501:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=1.569e+02, with an active set of 124 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,503:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=1.388e+03, with an active set of 128 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,503:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=1.232e+03, with an active set of 128 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,504:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=1.900e+02, with an active set of 128 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,504:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=7.168e+00, with an active set of 128 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,504:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=3.774e+05, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,505:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=3.759e+05, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,505:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=8.153e+04, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,727:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=3.745e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,731:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.029e+02, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,738:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=5.669e+03, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,739:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=2.536e+03, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,739:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=1.472e+02, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,740:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.739e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,743:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=2.546e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,745:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.190e+02, with an active set of 73 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,745:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=6.923e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,746:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=5.772e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,747:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.716e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=3.202e+01, with an active set of 98 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,749:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.832e+01, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,749:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.695e+01, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,751:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=1.943e+01, with an active set of 107 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,751:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=1.431e+01, with an active set of 108 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,752:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=1.773e+01, with an active set of 111 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,752:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=1.219e+01, with an active set of 116 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,752:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=1.102e+01, with an active set of 116 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,754:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 144 iterations, i.e. alpha=4.280e+01, with an active set of 125 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,755:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=3.768e+02, with an active set of 128 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,756:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=1.298e+04, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,756:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=7.540e+01, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:18,811:INFO:Calculating mean and std
2024-02-05 18:43:18,812:INFO:Creating metrics dataframe
2024-02-05 18:43:18,814:INFO:Uploading results into container
2024-02-05 18:43:18,814:INFO:Uploading model into container now
2024-02-05 18:43:18,814:INFO:_master_model_container: 28
2024-02-05 18:43:18,814:INFO:_display_container: 6
2024-02-05 18:43:18,814:INFO:Lars(random_state=123)
2024-02-05 18:43:18,814:INFO:create_model() successfully completed......................................
2024-02-05 18:43:18,914:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:18,914:INFO:Creating metrics dataframe
2024-02-05 18:43:18,923:INFO:Initializing Lasso Least Angle Regression
2024-02-05 18:43:18,924:INFO:Total runtime is 0.06916477680206298 minutes
2024-02-05 18:43:18,927:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:18,927:INFO:Initializing create_model()
2024-02-05 18:43:18,928:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:18,928:INFO:Checking exceptions
2024-02-05 18:43:18,928:INFO:Importing libraries
2024-02-05 18:43:18,928:INFO:Copying training dataset
2024-02-05 18:43:18,933:INFO:Defining folds
2024-02-05 18:43:18,933:INFO:Declaring metric variables
2024-02-05 18:43:18,937:INFO:Importing untrained model
2024-02-05 18:43:18,941:INFO:Lasso Least Angle Regression Imported successfully
2024-02-05 18:43:18,947:INFO:Starting cross validation
2024-02-05 18:43:18,951:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:19,308:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.955e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,308:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 70 iterations, alpha=2.179e+02, previous alpha=2.179e+02, with an active set of 57 regressors.
  warnings.warn(

2024-02-05 18:43:19,317:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=7.153e+03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,317:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.274e+03, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,327:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.353e+03, previous alpha=1.302e+03, with an active set of 29 regressors.
  warnings.warn(

2024-02-05 18:43:19,327:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=4.005e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,327:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=3.697e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,331:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.234e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,332:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.087e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,339:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=3.340e+02, with an active set of 46 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,341:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=8.289e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,342:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=8.289e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,342:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 101 iterations, alpha=8.102e+01, previous alpha=8.102e+01, with an active set of 80 regressors.
  warnings.warn(

2024-02-05 18:43:19,343:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.603e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,344:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 86 iterations, alpha=1.479e+02, previous alpha=1.479e+02, with an active set of 71 regressors.
  warnings.warn(

2024-02-05 18:43:19,344:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 140 iterations, alpha=2.414e+01, previous alpha=2.414e+01, with an active set of 95 regressors.
  warnings.warn(

2024-02-05 18:43:19,349:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.426e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,352:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.522e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,355:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.795e+02, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,356:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=4.102e+02, with an active set of 45 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,360:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=4.274e+00, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,361:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 146 iterations, i.e. alpha=1.586e+00, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,361:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=6.685e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,362:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 147 iterations, i.e. alpha=1.586e+00, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,362:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=6.615e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,363:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 151 iterations, alpha=1.419e+00, previous alpha=1.419e+00, with an active set of 118 regressors.
  warnings.warn(

2024-02-05 18:43:19,363:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 103 iterations, alpha=5.927e+01, previous alpha=5.849e+01, with an active set of 82 regressors.
  warnings.warn(

2024-02-05 18:43:19,369:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 111 iterations, alpha=7.889e+01, previous alpha=5.351e+01, with an active set of 88 regressors.
  warnings.warn(

2024-02-05 18:43:19,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.325e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,585:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.167e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,589:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.380e+02, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,590:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=9.583e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,590:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=9.092e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,590:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 89 iterations, alpha=8.936e+01, previous alpha=8.854e+01, with an active set of 78 regressors.
  warnings.warn(

2024-02-05 18:43:19,596:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.168e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,597:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=4.459e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,599:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.956e+02, with an active set of 57 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 18:43:19,606:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 129 iterations, alpha=4.342e+01, previous alpha=3.885e+01, with an active set of 96 regressors.
  warnings.warn(

2024-02-05 18:43:19,658:INFO:Calculating mean and std
2024-02-05 18:43:19,658:INFO:Creating metrics dataframe
2024-02-05 18:43:19,661:INFO:Uploading results into container
2024-02-05 18:43:19,661:INFO:Uploading model into container now
2024-02-05 18:43:19,661:INFO:_master_model_container: 29
2024-02-05 18:43:19,661:INFO:_display_container: 6
2024-02-05 18:43:19,661:INFO:LassoLars(random_state=123)
2024-02-05 18:43:19,662:INFO:create_model() successfully completed......................................
2024-02-05 18:43:19,761:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:19,761:INFO:Creating metrics dataframe
2024-02-05 18:43:19,768:INFO:Initializing Orthogonal Matching Pursuit
2024-02-05 18:43:19,769:INFO:Total runtime is 0.08325748046239216 minutes
2024-02-05 18:43:19,772:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:19,772:INFO:Initializing create_model()
2024-02-05 18:43:19,773:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:19,773:INFO:Checking exceptions
2024-02-05 18:43:19,773:INFO:Importing libraries
2024-02-05 18:43:19,773:INFO:Copying training dataset
2024-02-05 18:43:19,778:INFO:Defining folds
2024-02-05 18:43:19,778:INFO:Declaring metric variables
2024-02-05 18:43:19,781:INFO:Importing untrained model
2024-02-05 18:43:19,786:INFO:Orthogonal Matching Pursuit Imported successfully
2024-02-05 18:43:19,792:INFO:Starting cross validation
2024-02-05 18:43:19,794:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:20,387:INFO:Calculating mean and std
2024-02-05 18:43:20,388:INFO:Creating metrics dataframe
2024-02-05 18:43:20,390:INFO:Uploading results into container
2024-02-05 18:43:20,390:INFO:Uploading model into container now
2024-02-05 18:43:20,390:INFO:_master_model_container: 30
2024-02-05 18:43:20,390:INFO:_display_container: 6
2024-02-05 18:43:20,391:INFO:OrthogonalMatchingPursuit()
2024-02-05 18:43:20,391:INFO:create_model() successfully completed......................................
2024-02-05 18:43:20,491:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:20,491:INFO:Creating metrics dataframe
2024-02-05 18:43:20,499:INFO:Initializing Bayesian Ridge
2024-02-05 18:43:20,499:INFO:Total runtime is 0.09542484680811564 minutes
2024-02-05 18:43:20,502:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:20,502:INFO:Initializing create_model()
2024-02-05 18:43:20,502:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:20,502:INFO:Checking exceptions
2024-02-05 18:43:20,503:INFO:Importing libraries
2024-02-05 18:43:20,503:INFO:Copying training dataset
2024-02-05 18:43:20,508:INFO:Defining folds
2024-02-05 18:43:20,508:INFO:Declaring metric variables
2024-02-05 18:43:20,511:INFO:Importing untrained model
2024-02-05 18:43:20,516:INFO:Bayesian Ridge Imported successfully
2024-02-05 18:43:20,522:INFO:Starting cross validation
2024-02-05 18:43:20,524:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:21,158:INFO:Calculating mean and std
2024-02-05 18:43:21,159:INFO:Creating metrics dataframe
2024-02-05 18:43:21,161:INFO:Uploading results into container
2024-02-05 18:43:21,161:INFO:Uploading model into container now
2024-02-05 18:43:21,161:INFO:_master_model_container: 31
2024-02-05 18:43:21,161:INFO:_display_container: 6
2024-02-05 18:43:21,162:INFO:BayesianRidge()
2024-02-05 18:43:21,162:INFO:create_model() successfully completed......................................
2024-02-05 18:43:21,264:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:21,264:INFO:Creating metrics dataframe
2024-02-05 18:43:21,273:INFO:Initializing Passive Aggressive Regressor
2024-02-05 18:43:21,273:INFO:Total runtime is 0.10832378069559732 minutes
2024-02-05 18:43:21,276:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:21,277:INFO:Initializing create_model()
2024-02-05 18:43:21,277:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:21,277:INFO:Checking exceptions
2024-02-05 18:43:21,277:INFO:Importing libraries
2024-02-05 18:43:21,277:INFO:Copying training dataset
2024-02-05 18:43:21,282:INFO:Defining folds
2024-02-05 18:43:21,283:INFO:Declaring metric variables
2024-02-05 18:43:21,286:INFO:Importing untrained model
2024-02-05 18:43:21,290:INFO:Passive Aggressive Regressor Imported successfully
2024-02-05 18:43:21,296:INFO:Starting cross validation
2024-02-05 18:43:21,299:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:21,928:INFO:Calculating mean and std
2024-02-05 18:43:21,930:INFO:Creating metrics dataframe
2024-02-05 18:43:21,931:INFO:Uploading results into container
2024-02-05 18:43:21,932:INFO:Uploading model into container now
2024-02-05 18:43:21,932:INFO:_master_model_container: 32
2024-02-05 18:43:21,932:INFO:_display_container: 6
2024-02-05 18:43:21,932:INFO:PassiveAggressiveRegressor(random_state=123)
2024-02-05 18:43:21,932:INFO:create_model() successfully completed......................................
2024-02-05 18:43:22,028:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:22,028:INFO:Creating metrics dataframe
2024-02-05 18:43:22,036:INFO:Initializing Huber Regressor
2024-02-05 18:43:22,037:INFO:Total runtime is 0.1210539698600769 minutes
2024-02-05 18:43:22,041:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:22,042:INFO:Initializing create_model()
2024-02-05 18:43:22,042:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:22,042:INFO:Checking exceptions
2024-02-05 18:43:22,042:INFO:Importing libraries
2024-02-05 18:43:22,042:INFO:Copying training dataset
2024-02-05 18:43:22,047:INFO:Defining folds
2024-02-05 18:43:22,047:INFO:Declaring metric variables
2024-02-05 18:43:22,052:INFO:Importing untrained model
2024-02-05 18:43:22,054:INFO:Huber Regressor Imported successfully
2024-02-05 18:43:22,062:INFO:Starting cross validation
2024-02-05 18:43:22,064:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:22,453:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 18:43:22,518:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 18:43:22,524:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 18:43:22,529:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 18:43:22,538:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 18:43:22,544:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 18:43:22,556:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 18:43:22,567:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 18:43:22,796:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 18:43:22,803:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 18:43:22,858:INFO:Calculating mean and std
2024-02-05 18:43:22,859:INFO:Creating metrics dataframe
2024-02-05 18:43:22,861:INFO:Uploading results into container
2024-02-05 18:43:22,862:INFO:Uploading model into container now
2024-02-05 18:43:22,862:INFO:_master_model_container: 33
2024-02-05 18:43:22,862:INFO:_display_container: 6
2024-02-05 18:43:22,862:INFO:HuberRegressor()
2024-02-05 18:43:22,862:INFO:create_model() successfully completed......................................
2024-02-05 18:43:22,961:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:22,961:INFO:Creating metrics dataframe
2024-02-05 18:43:22,971:INFO:Initializing K Neighbors Regressor
2024-02-05 18:43:22,972:INFO:Total runtime is 0.1366454482078552 minutes
2024-02-05 18:43:22,975:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:22,976:INFO:Initializing create_model()
2024-02-05 18:43:22,976:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:22,976:INFO:Checking exceptions
2024-02-05 18:43:22,976:INFO:Importing libraries
2024-02-05 18:43:22,976:INFO:Copying training dataset
2024-02-05 18:43:22,982:INFO:Defining folds
2024-02-05 18:43:22,983:INFO:Declaring metric variables
2024-02-05 18:43:22,987:INFO:Importing untrained model
2024-02-05 18:43:22,989:INFO:K Neighbors Regressor Imported successfully
2024-02-05 18:43:22,998:INFO:Starting cross validation
2024-02-05 18:43:23,000:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:23,602:INFO:Calculating mean and std
2024-02-05 18:43:23,603:INFO:Creating metrics dataframe
2024-02-05 18:43:23,606:INFO:Uploading results into container
2024-02-05 18:43:23,606:INFO:Uploading model into container now
2024-02-05 18:43:23,607:INFO:_master_model_container: 34
2024-02-05 18:43:23,607:INFO:_display_container: 6
2024-02-05 18:43:23,607:INFO:KNeighborsRegressor(n_jobs=-1)
2024-02-05 18:43:23,607:INFO:create_model() successfully completed......................................
2024-02-05 18:43:23,703:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:23,703:INFO:Creating metrics dataframe
2024-02-05 18:43:23,713:INFO:Initializing Decision Tree Regressor
2024-02-05 18:43:23,714:INFO:Total runtime is 0.14901176293690999 minutes
2024-02-05 18:43:23,719:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:23,720:INFO:Initializing create_model()
2024-02-05 18:43:23,720:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:23,720:INFO:Checking exceptions
2024-02-05 18:43:23,720:INFO:Importing libraries
2024-02-05 18:43:23,720:INFO:Copying training dataset
2024-02-05 18:43:23,725:INFO:Defining folds
2024-02-05 18:43:23,725:INFO:Declaring metric variables
2024-02-05 18:43:23,728:INFO:Importing untrained model
2024-02-05 18:43:23,732:INFO:Decision Tree Regressor Imported successfully
2024-02-05 18:43:23,737:INFO:Starting cross validation
2024-02-05 18:43:23,741:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:24,387:INFO:Calculating mean and std
2024-02-05 18:43:24,387:INFO:Creating metrics dataframe
2024-02-05 18:43:24,391:INFO:Uploading results into container
2024-02-05 18:43:24,392:INFO:Uploading model into container now
2024-02-05 18:43:24,392:INFO:_master_model_container: 35
2024-02-05 18:43:24,392:INFO:_display_container: 6
2024-02-05 18:43:24,392:INFO:DecisionTreeRegressor(random_state=123)
2024-02-05 18:43:24,392:INFO:create_model() successfully completed......................................
2024-02-05 18:43:24,489:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:24,489:INFO:Creating metrics dataframe
2024-02-05 18:43:24,497:INFO:Initializing Random Forest Regressor
2024-02-05 18:43:24,497:INFO:Total runtime is 0.16205861965815224 minutes
2024-02-05 18:43:24,502:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:24,502:INFO:Initializing create_model()
2024-02-05 18:43:24,502:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:24,502:INFO:Checking exceptions
2024-02-05 18:43:24,502:INFO:Importing libraries
2024-02-05 18:43:24,503:INFO:Copying training dataset
2024-02-05 18:43:24,508:INFO:Defining folds
2024-02-05 18:43:24,508:INFO:Declaring metric variables
2024-02-05 18:43:24,511:INFO:Importing untrained model
2024-02-05 18:43:24,515:INFO:Random Forest Regressor Imported successfully
2024-02-05 18:43:24,528:INFO:Starting cross validation
2024-02-05 18:43:24,530:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:27,684:INFO:Calculating mean and std
2024-02-05 18:43:27,686:INFO:Creating metrics dataframe
2024-02-05 18:43:27,689:INFO:Uploading results into container
2024-02-05 18:43:27,690:INFO:Uploading model into container now
2024-02-05 18:43:27,690:INFO:_master_model_container: 36
2024-02-05 18:43:27,690:INFO:_display_container: 6
2024-02-05 18:43:27,690:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2024-02-05 18:43:27,691:INFO:create_model() successfully completed......................................
2024-02-05 18:43:27,787:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:27,787:INFO:Creating metrics dataframe
2024-02-05 18:43:27,796:INFO:Initializing Extra Trees Regressor
2024-02-05 18:43:27,796:INFO:Total runtime is 0.21703266700108845 minutes
2024-02-05 18:43:27,800:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:27,800:INFO:Initializing create_model()
2024-02-05 18:43:27,800:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:27,800:INFO:Checking exceptions
2024-02-05 18:43:27,800:INFO:Importing libraries
2024-02-05 18:43:27,801:INFO:Copying training dataset
2024-02-05 18:43:27,805:INFO:Defining folds
2024-02-05 18:43:27,806:INFO:Declaring metric variables
2024-02-05 18:43:27,809:INFO:Importing untrained model
2024-02-05 18:43:27,813:INFO:Extra Trees Regressor Imported successfully
2024-02-05 18:43:27,820:INFO:Starting cross validation
2024-02-05 18:43:27,823:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:30,077:INFO:Calculating mean and std
2024-02-05 18:43:30,078:INFO:Creating metrics dataframe
2024-02-05 18:43:30,081:INFO:Uploading results into container
2024-02-05 18:43:30,081:INFO:Uploading model into container now
2024-02-05 18:43:30,082:INFO:_master_model_container: 37
2024-02-05 18:43:30,082:INFO:_display_container: 6
2024-02-05 18:43:30,082:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2024-02-05 18:43:30,082:INFO:create_model() successfully completed......................................
2024-02-05 18:43:30,177:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:30,177:INFO:Creating metrics dataframe
2024-02-05 18:43:30,187:INFO:Initializing AdaBoost Regressor
2024-02-05 18:43:30,187:INFO:Total runtime is 0.2568886121114095 minutes
2024-02-05 18:43:30,190:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:30,190:INFO:Initializing create_model()
2024-02-05 18:43:30,190:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:30,190:INFO:Checking exceptions
2024-02-05 18:43:30,190:INFO:Importing libraries
2024-02-05 18:43:30,191:INFO:Copying training dataset
2024-02-05 18:43:30,196:INFO:Defining folds
2024-02-05 18:43:30,196:INFO:Declaring metric variables
2024-02-05 18:43:30,199:INFO:Importing untrained model
2024-02-05 18:43:30,204:INFO:AdaBoost Regressor Imported successfully
2024-02-05 18:43:30,209:INFO:Starting cross validation
2024-02-05 18:43:30,211:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:31,358:INFO:Calculating mean and std
2024-02-05 18:43:31,360:INFO:Creating metrics dataframe
2024-02-05 18:43:31,362:INFO:Uploading results into container
2024-02-05 18:43:31,362:INFO:Uploading model into container now
2024-02-05 18:43:31,362:INFO:_master_model_container: 38
2024-02-05 18:43:31,362:INFO:_display_container: 6
2024-02-05 18:43:31,363:INFO:AdaBoostRegressor(random_state=123)
2024-02-05 18:43:31,363:INFO:create_model() successfully completed......................................
2024-02-05 18:43:31,458:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:31,458:INFO:Creating metrics dataframe
2024-02-05 18:43:31,467:INFO:Initializing Gradient Boosting Regressor
2024-02-05 18:43:31,467:INFO:Total runtime is 0.27821412483851116 minutes
2024-02-05 18:43:31,470:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:31,470:INFO:Initializing create_model()
2024-02-05 18:43:31,470:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:31,471:INFO:Checking exceptions
2024-02-05 18:43:31,471:INFO:Importing libraries
2024-02-05 18:43:31,471:INFO:Copying training dataset
2024-02-05 18:43:31,477:INFO:Defining folds
2024-02-05 18:43:31,478:INFO:Declaring metric variables
2024-02-05 18:43:31,481:INFO:Importing untrained model
2024-02-05 18:43:31,484:INFO:Gradient Boosting Regressor Imported successfully
2024-02-05 18:43:31,490:INFO:Starting cross validation
2024-02-05 18:43:31,492:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:33,152:INFO:Calculating mean and std
2024-02-05 18:43:33,153:INFO:Creating metrics dataframe
2024-02-05 18:43:33,155:INFO:Uploading results into container
2024-02-05 18:43:33,155:INFO:Uploading model into container now
2024-02-05 18:43:33,156:INFO:_master_model_container: 39
2024-02-05 18:43:33,156:INFO:_display_container: 6
2024-02-05 18:43:33,156:INFO:GradientBoostingRegressor(random_state=123)
2024-02-05 18:43:33,156:INFO:create_model() successfully completed......................................
2024-02-05 18:43:33,252:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:33,252:INFO:Creating metrics dataframe
2024-02-05 18:43:33,262:INFO:Initializing Light Gradient Boosting Machine
2024-02-05 18:43:33,262:INFO:Total runtime is 0.3081456462542216 minutes
2024-02-05 18:43:33,266:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:33,266:INFO:Initializing create_model()
2024-02-05 18:43:33,266:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:33,266:INFO:Checking exceptions
2024-02-05 18:43:33,266:INFO:Importing libraries
2024-02-05 18:43:33,266:INFO:Copying training dataset
2024-02-05 18:43:33,270:INFO:Defining folds
2024-02-05 18:43:33,271:INFO:Declaring metric variables
2024-02-05 18:43:33,275:INFO:Importing untrained model
2024-02-05 18:43:33,280:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-05 18:43:33,286:INFO:Starting cross validation
2024-02-05 18:43:33,291:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:34,816:INFO:Calculating mean and std
2024-02-05 18:43:34,818:INFO:Creating metrics dataframe
2024-02-05 18:43:34,821:INFO:Uploading results into container
2024-02-05 18:43:34,822:INFO:Uploading model into container now
2024-02-05 18:43:34,822:INFO:_master_model_container: 40
2024-02-05 18:43:34,822:INFO:_display_container: 6
2024-02-05 18:43:34,823:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-05 18:43:34,823:INFO:create_model() successfully completed......................................
2024-02-05 18:43:34,937:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:34,937:INFO:Creating metrics dataframe
2024-02-05 18:43:34,945:INFO:Initializing Dummy Regressor
2024-02-05 18:43:34,945:INFO:Total runtime is 0.33619327545166017 minutes
2024-02-05 18:43:34,948:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:34,948:INFO:Initializing create_model()
2024-02-05 18:43:34,948:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B2AC1760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:34,948:INFO:Checking exceptions
2024-02-05 18:43:34,948:INFO:Importing libraries
2024-02-05 18:43:34,949:INFO:Copying training dataset
2024-02-05 18:43:34,954:INFO:Defining folds
2024-02-05 18:43:34,954:INFO:Declaring metric variables
2024-02-05 18:43:34,957:INFO:Importing untrained model
2024-02-05 18:43:34,961:INFO:Dummy Regressor Imported successfully
2024-02-05 18:43:34,967:INFO:Starting cross validation
2024-02-05 18:43:34,970:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:35,554:INFO:Calculating mean and std
2024-02-05 18:43:35,554:INFO:Creating metrics dataframe
2024-02-05 18:43:35,557:INFO:Uploading results into container
2024-02-05 18:43:35,557:INFO:Uploading model into container now
2024-02-05 18:43:35,557:INFO:_master_model_container: 41
2024-02-05 18:43:35,557:INFO:_display_container: 6
2024-02-05 18:43:35,557:INFO:DummyRegressor()
2024-02-05 18:43:35,557:INFO:create_model() successfully completed......................................
2024-02-05 18:43:35,654:INFO:SubProcess create_model() end ==================================
2024-02-05 18:43:35,654:INFO:Creating metrics dataframe
2024-02-05 18:43:35,672:INFO:Initializing create_model()
2024-02-05 18:43:35,673:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:35,673:INFO:Checking exceptions
2024-02-05 18:43:35,674:INFO:Importing libraries
2024-02-05 18:43:35,674:INFO:Copying training dataset
2024-02-05 18:43:35,678:INFO:Defining folds
2024-02-05 18:43:35,678:INFO:Declaring metric variables
2024-02-05 18:43:35,678:INFO:Importing untrained model
2024-02-05 18:43:35,678:INFO:Declaring custom model
2024-02-05 18:43:35,680:INFO:Gradient Boosting Regressor Imported successfully
2024-02-05 18:43:35,682:INFO:Cross validation set to False
2024-02-05 18:43:35,682:INFO:Fitting Model
2024-02-05 18:43:36,255:INFO:GradientBoostingRegressor(random_state=123)
2024-02-05 18:43:36,255:INFO:create_model() successfully completed......................................
2024-02-05 18:43:36,353:INFO:Creating Dashboard logs
2024-02-05 18:43:36,356:INFO:Model: Gradient Boosting Regressor
2024-02-05 18:43:36,382:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-05 18:43:36,444:INFO:Initializing predict_model()
2024-02-05 18:43:36,445:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D7B2DE3790>)
2024-02-05 18:43:36,445:INFO:Checking exceptions
2024-02-05 18:43:36,445:INFO:Preloading libraries
2024-02-05 18:43:36,862:INFO:Initializing create_model()
2024-02-05 18:43:36,863:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:36,863:INFO:Checking exceptions
2024-02-05 18:43:36,864:INFO:Importing libraries
2024-02-05 18:43:36,864:INFO:Copying training dataset
2024-02-05 18:43:36,869:INFO:Defining folds
2024-02-05 18:43:36,869:INFO:Declaring metric variables
2024-02-05 18:43:36,869:INFO:Importing untrained model
2024-02-05 18:43:36,869:INFO:Declaring custom model
2024-02-05 18:43:36,869:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-05 18:43:36,871:INFO:Cross validation set to False
2024-02-05 18:43:36,871:INFO:Fitting Model
2024-02-05 18:43:36,999:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-02-05 18:43:37,001:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000706 seconds.
2024-02-05 18:43:37,001:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-05 18:43:37,001:INFO:[LightGBM] [Info] Total Bins 2881
2024-02-05 18:43:37,001:INFO:[LightGBM] [Info] Number of data points in the train set: 1019, number of used features: 93
2024-02-05 18:43:37,002:INFO:[LightGBM] [Info] Start training from score 180679.625123
2024-02-05 18:43:37,073:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-05 18:43:37,074:INFO:create_model() successfully completed......................................
2024-02-05 18:43:37,184:INFO:Creating Dashboard logs
2024-02-05 18:43:37,187:INFO:Model: Light Gradient Boosting Machine
2024-02-05 18:43:37,210:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2024-02-05 18:43:37,271:INFO:Initializing predict_model()
2024-02-05 18:43:37,271:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D7B2DE3A60>)
2024-02-05 18:43:37,271:INFO:Checking exceptions
2024-02-05 18:43:37,271:INFO:Preloading libraries
2024-02-05 18:43:37,728:INFO:Initializing create_model()
2024-02-05 18:43:37,728:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:37,728:INFO:Checking exceptions
2024-02-05 18:43:37,730:INFO:Importing libraries
2024-02-05 18:43:37,731:INFO:Copying training dataset
2024-02-05 18:43:37,734:INFO:Defining folds
2024-02-05 18:43:37,734:INFO:Declaring metric variables
2024-02-05 18:43:37,734:INFO:Importing untrained model
2024-02-05 18:43:37,734:INFO:Declaring custom model
2024-02-05 18:43:37,735:INFO:Extra Trees Regressor Imported successfully
2024-02-05 18:43:37,736:INFO:Cross validation set to False
2024-02-05 18:43:37,736:INFO:Fitting Model
2024-02-05 18:43:38,081:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2024-02-05 18:43:38,081:INFO:create_model() successfully completed......................................
2024-02-05 18:43:38,180:INFO:Creating Dashboard logs
2024-02-05 18:43:38,184:INFO:Model: Extra Trees Regressor
2024-02-05 18:43:38,217:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-02-05 18:43:38,277:INFO:Initializing predict_model()
2024-02-05 18:43:38,277:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D7B2EFB280>)
2024-02-05 18:43:38,277:INFO:Checking exceptions
2024-02-05 18:43:38,277:INFO:Preloading libraries
2024-02-05 18:43:38,751:INFO:Creating Dashboard logs
2024-02-05 18:43:38,754:INFO:Model: Random Forest Regressor
2024-02-05 18:43:38,778:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-02-05 18:43:38,956:INFO:Creating Dashboard logs
2024-02-05 18:43:38,959:INFO:Model: Bayesian Ridge
2024-02-05 18:43:38,983:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'n_iter': 300, 'tol': 0.001, 'verbose': False}
2024-02-05 18:43:39,159:INFO:Creating Dashboard logs
2024-02-05 18:43:39,162:INFO:Model: Lasso Least Angle Regression
2024-02-05 18:43:39,187:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'max_iter': 500, 'normalize': 'deprecated', 'positive': False, 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2024-02-05 18:43:39,391:INFO:Creating Dashboard logs
2024-02-05 18:43:39,395:INFO:Model: Ridge Regression
2024-02-05 18:43:39,418:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}
2024-02-05 18:43:39,590:INFO:Creating Dashboard logs
2024-02-05 18:43:39,592:INFO:Model: Elastic Net
2024-02-05 18:43:39,617:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2024-02-05 18:43:39,792:INFO:Creating Dashboard logs
2024-02-05 18:43:39,795:INFO:Model: Lasso Regression
2024-02-05 18:43:39,822:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2024-02-05 18:43:39,999:INFO:Creating Dashboard logs
2024-02-05 18:43:40,002:INFO:Model: AdaBoost Regressor
2024-02-05 18:43:40,028:INFO:Logged params: {'base_estimator': 'deprecated', 'estimator': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50, 'random_state': 123}
2024-02-05 18:43:40,208:INFO:Creating Dashboard logs
2024-02-05 18:43:40,210:INFO:Model: Orthogonal Matching Pursuit
2024-02-05 18:43:40,234:INFO:Logged params: {'fit_intercept': True, 'n_nonzero_coefs': None, 'normalize': 'deprecated', 'precompute': 'auto', 'tol': None}
2024-02-05 18:43:40,405:INFO:Creating Dashboard logs
2024-02-05 18:43:40,409:INFO:Model: Decision Tree Regressor
2024-02-05 18:43:40,433:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 123, 'splitter': 'best'}
2024-02-05 18:43:40,615:INFO:Creating Dashboard logs
2024-02-05 18:43:40,618:INFO:Model: Huber Regressor
2024-02-05 18:43:40,645:INFO:Logged params: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': True, 'max_iter': 100, 'tol': 1e-05, 'warm_start': False}
2024-02-05 18:43:40,814:INFO:Creating Dashboard logs
2024-02-05 18:43:40,818:INFO:Model: K Neighbors Regressor
2024-02-05 18:43:40,842:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2024-02-05 18:43:41,013:INFO:Creating Dashboard logs
2024-02-05 18:43:41,016:INFO:Model: Passive Aggressive Regressor
2024-02-05 18:43:41,041:INFO:Logged params: {'C': 1.0, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'fit_intercept': True, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'n_iter_no_change': 5, 'random_state': 123, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-05 18:43:41,220:INFO:Creating Dashboard logs
2024-02-05 18:43:41,223:INFO:Model: Dummy Regressor
2024-02-05 18:43:41,249:INFO:Logged params: {'constant': None, 'quantile': None, 'strategy': 'mean'}
2024-02-05 18:43:41,420:INFO:Creating Dashboard logs
2024-02-05 18:43:41,424:INFO:Model: Linear Regression
2024-02-05 18:43:41,450:INFO:Logged params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'positive': False}
2024-02-05 18:43:41,618:INFO:Creating Dashboard logs
2024-02-05 18:43:41,621:INFO:Model: Least Angle Regression
2024-02-05 18:43:41,645:INFO:Logged params: {'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'n_nonzero_coefs': 500, 'normalize': 'deprecated', 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2024-02-05 18:43:41,834:INFO:_master_model_container: 41
2024-02-05 18:43:41,834:INFO:_display_container: 6
2024-02-05 18:43:41,834:INFO:[GradientBoostingRegressor(random_state=123), LGBMRegressor(n_jobs=-1, random_state=123), ExtraTreesRegressor(n_jobs=-1, random_state=123)]
2024-02-05 18:43:41,834:INFO:compare_models() successfully completed......................................
2024-02-05 18:43:41,978:INFO:Initializing create_model()
2024-02-05 18:43:41,979:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:43:41,979:INFO:Checking exceptions
2024-02-05 18:43:41,993:INFO:Importing libraries
2024-02-05 18:43:41,993:INFO:Copying training dataset
2024-02-05 18:43:41,999:INFO:Defining folds
2024-02-05 18:43:41,999:INFO:Declaring metric variables
2024-02-05 18:43:42,002:INFO:Importing untrained model
2024-02-05 18:43:42,002:INFO:Declaring custom model
2024-02-05 18:43:42,006:INFO:Gradient Boosting Regressor Imported successfully
2024-02-05 18:43:42,012:INFO:Starting cross validation
2024-02-05 18:43:42,016:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:43:43,741:INFO:Calculating mean and std
2024-02-05 18:43:43,742:INFO:Creating metrics dataframe
2024-02-05 18:43:43,746:INFO:Finalizing model
2024-02-05 18:43:44,305:INFO:Creating Dashboard logs
2024-02-05 18:43:44,307:INFO:Model: Gradient Boosting Regressor
2024-02-05 18:43:44,332:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-05 18:43:44,394:INFO:Initializing predict_model()
2024-02-05 18:43:44,394:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D7B2AD88B0>)
2024-02-05 18:43:44,394:INFO:Checking exceptions
2024-02-05 18:43:44,394:INFO:Preloading libraries
2024-02-05 18:43:44,811:INFO:Uploading results into container
2024-02-05 18:43:44,812:INFO:Uploading model into container now
2024-02-05 18:43:44,820:INFO:_master_model_container: 42
2024-02-05 18:43:44,820:INFO:_display_container: 7
2024-02-05 18:43:44,821:INFO:GradientBoostingRegressor(random_state=123)
2024-02-05 18:43:44,821:INFO:create_model() successfully completed......................................
2024-02-05 18:43:44,960:INFO:Initializing tune_model()
2024-02-05 18:43:44,960:INFO:tune_model(estimator=GradientBoostingRegressor(random_state=123), fold=None, round=4, n_iter=10, custom_grid=None, optimize=R2, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>)
2024-02-05 18:43:44,960:INFO:Checking exceptions
2024-02-05 18:43:44,974:INFO:Copying training dataset
2024-02-05 18:43:44,974:INFO:Checking base model
2024-02-05 18:43:44,974:INFO:Base model : Gradient Boosting Regressor
2024-02-05 18:43:44,986:INFO:Declaring metric variables
2024-02-05 18:43:44,988:INFO:Defining Hyperparameters
2024-02-05 18:43:45,097:INFO:Tuning with n_jobs=-1
2024-02-05 18:43:45,097:INFO:Initializing RandomizedSearchCV
2024-02-05 18:43:57,964:INFO:best_params: {'actual_estimator__subsample': 0.85, 'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 5, 'actual_estimator__min_samples_leaf': 5, 'actual_estimator__min_impurity_decrease': 0.02, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 7, 'actual_estimator__learning_rate': 0.15}
2024-02-05 18:43:57,965:INFO:Hyperparameter search completed
2024-02-05 18:43:57,965:INFO:SubProcess create_model() called ==================================
2024-02-05 18:43:57,965:INFO:Initializing create_model()
2024-02-05 18:43:57,965:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D78A1ECB80>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.85, 'n_estimators': 230, 'min_samples_split': 5, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.02, 'max_features': 1.0, 'max_depth': 7, 'learning_rate': 0.15})
2024-02-05 18:43:57,965:INFO:Checking exceptions
2024-02-05 18:43:57,965:INFO:Importing libraries
2024-02-05 18:43:57,965:INFO:Copying training dataset
2024-02-05 18:43:57,970:INFO:Defining folds
2024-02-05 18:43:57,970:INFO:Declaring metric variables
2024-02-05 18:43:57,974:INFO:Importing untrained model
2024-02-05 18:43:57,974:INFO:Declaring custom model
2024-02-05 18:43:57,977:INFO:Gradient Boosting Regressor Imported successfully
2024-02-05 18:43:57,984:INFO:Starting cross validation
2024-02-05 18:43:57,986:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:44:03,185:INFO:Calculating mean and std
2024-02-05 18:44:03,186:INFO:Creating metrics dataframe
2024-02-05 18:44:03,190:INFO:Finalizing model
2024-02-05 18:44:05,185:INFO:Uploading results into container
2024-02-05 18:44:05,187:INFO:Uploading model into container now
2024-02-05 18:44:05,187:INFO:_master_model_container: 43
2024-02-05 18:44:05,187:INFO:_display_container: 8
2024-02-05 18:44:05,188:INFO:GradientBoostingRegressor(learning_rate=0.15, max_depth=7, max_features=1.0,
                          min_impurity_decrease=0.02, min_samples_leaf=5,
                          min_samples_split=5, n_estimators=230,
                          random_state=123, subsample=0.85)
2024-02-05 18:44:05,189:INFO:create_model() successfully completed......................................
2024-02-05 18:44:05,298:INFO:SubProcess create_model() end ==================================
2024-02-05 18:44:05,298:INFO:choose_better activated
2024-02-05 18:44:05,301:INFO:SubProcess create_model() called ==================================
2024-02-05 18:44:05,301:INFO:Initializing create_model()
2024-02-05 18:44:05,301:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:44:05,301:INFO:Checking exceptions
2024-02-05 18:44:05,302:INFO:Importing libraries
2024-02-05 18:44:05,302:INFO:Copying training dataset
2024-02-05 18:44:05,307:INFO:Defining folds
2024-02-05 18:44:05,307:INFO:Declaring metric variables
2024-02-05 18:44:05,307:INFO:Importing untrained model
2024-02-05 18:44:05,307:INFO:Declaring custom model
2024-02-05 18:44:05,308:INFO:Gradient Boosting Regressor Imported successfully
2024-02-05 18:44:05,308:INFO:Starting cross validation
2024-02-05 18:44:05,309:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:44:06,977:INFO:Calculating mean and std
2024-02-05 18:44:06,977:INFO:Creating metrics dataframe
2024-02-05 18:44:06,979:INFO:Finalizing model
2024-02-05 18:44:07,549:INFO:Uploading results into container
2024-02-05 18:44:07,549:INFO:Uploading model into container now
2024-02-05 18:44:07,550:INFO:_master_model_container: 44
2024-02-05 18:44:07,550:INFO:_display_container: 9
2024-02-05 18:44:07,550:INFO:GradientBoostingRegressor(random_state=123)
2024-02-05 18:44:07,550:INFO:create_model() successfully completed......................................
2024-02-05 18:44:07,664:INFO:SubProcess create_model() end ==================================
2024-02-05 18:44:07,665:INFO:GradientBoostingRegressor(random_state=123) result for R2 is 0.8969
2024-02-05 18:44:07,666:INFO:GradientBoostingRegressor(learning_rate=0.15, max_depth=7, max_features=1.0,
                          min_impurity_decrease=0.02, min_samples_leaf=5,
                          min_samples_split=5, n_estimators=230,
                          random_state=123, subsample=0.85) result for R2 is 0.8931
2024-02-05 18:44:07,666:INFO:GradientBoostingRegressor(random_state=123) is best model
2024-02-05 18:44:07,666:INFO:choose_better completed
2024-02-05 18:44:07,666:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-02-05 18:44:07,666:INFO:Creating Dashboard logs
2024-02-05 18:44:07,669:INFO:Model: Gradient Boosting Regressor
2024-02-05 18:44:07,701:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-05 18:44:07,765:INFO:Initializing predict_model()
2024-02-05 18:44:07,765:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D7B30C1040>)
2024-02-05 18:44:07,765:INFO:Checking exceptions
2024-02-05 18:44:07,765:INFO:Preloading libraries
2024-02-05 18:44:08,206:INFO:_master_model_container: 44
2024-02-05 18:44:08,206:INFO:_display_container: 8
2024-02-05 18:44:08,206:INFO:GradientBoostingRegressor(random_state=123)
2024-02-05 18:44:08,206:INFO:tune_model() successfully completed......................................
2024-02-05 18:44:08,504:INFO:Initializing ensemble_model()
2024-02-05 18:44:08,504:INFO:ensemble_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), method=Bagging, fold=None, n_estimators=50, round=4, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-05 18:44:08,504:INFO:Checking exceptions
2024-02-05 18:44:08,522:INFO:Importing libraries
2024-02-05 18:44:08,522:INFO:Copying training dataset
2024-02-05 18:44:08,522:INFO:Checking base model
2024-02-05 18:44:08,523:INFO:Base model : Gradient Boosting Regressor
2024-02-05 18:44:08,528:INFO:Importing untrained ensembler
2024-02-05 18:44:08,528:INFO:Ensemble method set to Bagging
2024-02-05 18:44:08,528:INFO:SubProcess create_model() called ==================================
2024-02-05 18:44:08,529:INFO:Initializing create_model()
2024-02-05 18:44:08,529:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B332D550>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:44:08,529:INFO:Checking exceptions
2024-02-05 18:44:08,529:INFO:Importing libraries
2024-02-05 18:44:08,529:INFO:Copying training dataset
2024-02-05 18:44:08,534:INFO:Defining folds
2024-02-05 18:44:08,534:INFO:Declaring metric variables
2024-02-05 18:44:08,537:INFO:Importing untrained model
2024-02-05 18:44:08,537:INFO:Declaring custom model
2024-02-05 18:44:08,541:INFO:Bagging Regressor Imported successfully
2024-02-05 18:44:08,549:INFO:Starting cross validation
2024-02-05 18:44:08,553:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:44:45,922:INFO:Calculating mean and std
2024-02-05 18:44:45,924:INFO:Creating metrics dataframe
2024-02-05 18:44:45,927:INFO:Finalizing model
2024-02-05 18:45:00,535:INFO:Uploading results into container
2024-02-05 18:45:00,536:INFO:Uploading model into container now
2024-02-05 18:45:00,537:INFO:_master_model_container: 45
2024-02-05 18:45:00,537:INFO:_display_container: 9
2024-02-05 18:45:00,538:INFO:BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123)
2024-02-05 18:45:00,538:INFO:create_model() successfully completed......................................
2024-02-05 18:45:00,641:INFO:SubProcess create_model() end ==================================
2024-02-05 18:45:00,642:INFO:Creating Dashboard logs
2024-02-05 18:45:00,645:INFO:Model: Bagging Regressor
2024-02-05 18:45:00,673:INFO:Logged params: {'base_estimator': 'deprecated', 'bootstrap': True, 'bootstrap_features': False, 'estimator__alpha': 0.9, 'estimator__ccp_alpha': 0.0, 'estimator__criterion': 'friedman_mse', 'estimator__init': None, 'estimator__learning_rate': 0.1, 'estimator__loss': 'squared_error', 'estimator__max_depth': 3, 'estimator__max_features': None, 'estimator__max_leaf_nodes': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__n_estimators': 100, 'estimator__n_iter_no_change': None, 'estimator__random_state': 123, 'estimator__subsample': 1.0, 'estimator__tol': 0.0001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': GradientBoostingRegressor(random_state=123), 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-02-05 18:45:00,744:INFO:Initializing predict_model()
2024-02-05 18:45:00,744:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D7B30C1790>)
2024-02-05 18:45:00,744:INFO:Checking exceptions
2024-02-05 18:45:00,744:INFO:Preloading libraries
2024-02-05 18:45:01,323:INFO:_master_model_container: 45
2024-02-05 18:45:01,323:INFO:_display_container: 9
2024-02-05 18:45:01,323:INFO:BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123)
2024-02-05 18:45:01,323:INFO:ensemble_model() successfully completed......................................
2024-02-05 18:45:01,490:INFO:Initializing ensemble_model()
2024-02-05 18:45:01,490:INFO:ensemble_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), method=Boosting, fold=None, n_estimators=10, round=4, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-05 18:45:01,490:INFO:Checking exceptions
2024-02-05 18:45:05,398:INFO:Importing libraries
2024-02-05 18:45:05,398:INFO:Copying training dataset
2024-02-05 18:45:05,399:INFO:Checking base model
2024-02-05 18:45:05,399:INFO:Base model : Gradient Boosting Regressor
2024-02-05 18:45:05,405:INFO:Importing untrained ensembler
2024-02-05 18:45:05,405:INFO:Ensemble method set to Boosting
2024-02-05 18:45:05,405:INFO:SubProcess create_model() called ==================================
2024-02-05 18:45:05,405:INFO:Initializing create_model()
2024-02-05 18:45:05,405:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=AdaBoostRegressor(estimator=GradientBoostingRegressor(random_state=123),
                  n_estimators=10, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B25CA700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:45:05,405:INFO:Checking exceptions
2024-02-05 18:45:05,405:INFO:Importing libraries
2024-02-05 18:45:05,405:INFO:Copying training dataset
2024-02-05 18:45:05,411:INFO:Defining folds
2024-02-05 18:45:05,411:INFO:Declaring metric variables
2024-02-05 18:45:05,414:INFO:Importing untrained model
2024-02-05 18:45:05,414:INFO:Declaring custom model
2024-02-05 18:45:05,418:INFO:AdaBoost Regressor Imported successfully
2024-02-05 18:45:05,429:INFO:Starting cross validation
2024-02-05 18:45:05,431:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:45:15,266:INFO:Calculating mean and std
2024-02-05 18:45:15,267:INFO:Creating metrics dataframe
2024-02-05 18:45:15,272:INFO:Finalizing model
2024-02-05 18:45:19,026:INFO:Uploading results into container
2024-02-05 18:45:19,027:INFO:Uploading model into container now
2024-02-05 18:45:19,027:INFO:_master_model_container: 46
2024-02-05 18:45:19,027:INFO:_display_container: 10
2024-02-05 18:45:19,029:INFO:AdaBoostRegressor(estimator=GradientBoostingRegressor(random_state=123),
                  n_estimators=10, random_state=123)
2024-02-05 18:45:19,029:INFO:create_model() successfully completed......................................
2024-02-05 18:45:19,139:INFO:SubProcess create_model() end ==================================
2024-02-05 18:45:19,139:INFO:Creating Dashboard logs
2024-02-05 18:45:19,142:INFO:Model: AdaBoost Regressor
2024-02-05 18:45:19,170:INFO:Logged params: {'base_estimator': 'deprecated', 'estimator__alpha': 0.9, 'estimator__ccp_alpha': 0.0, 'estimator__criterion': 'friedman_mse', 'estimator__init': None, 'estimator__learning_rate': 0.1, 'estimator__loss': 'squared_error', 'estimator__max_depth': 3, 'estimator__max_features': None, 'estimator__max_leaf_nodes': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__n_estimators': 100, 'estimator__n_iter_no_change': None, 'estimator__random_state': 123, 'estimator__subsample': 1.0, 'estimator__tol': 0.0001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': GradientBoostingRegressor(random_state=123), 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 10, 'random_state': 123}
2024-02-05 18:45:19,242:INFO:Initializing predict_model()
2024-02-05 18:45:19,242:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=AdaBoostRegressor(estimator=GradientBoostingRegressor(random_state=123),
                  n_estimators=10, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D7B2EFB4C0>)
2024-02-05 18:45:19,242:INFO:Checking exceptions
2024-02-05 18:45:19,242:INFO:Preloading libraries
2024-02-05 18:45:19,716:INFO:_master_model_container: 46
2024-02-05 18:45:19,716:INFO:_display_container: 10
2024-02-05 18:45:19,717:INFO:AdaBoostRegressor(estimator=GradientBoostingRegressor(random_state=123),
                  n_estimators=10, random_state=123)
2024-02-05 18:45:19,717:INFO:ensemble_model() successfully completed......................................
2024-02-05 18:45:20,004:INFO:Initializing blend_models()
2024-02-05 18:45:20,004:INFO:blend_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator_list=[GradientBoostingRegressor(random_state=123), LGBMRegressor(n_jobs=-1, random_state=123), ExtraTreesRegressor(n_jobs=-1, random_state=123)], fold=None, round=4, choose_better=False, optimize=R2, method=auto, weights=None, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-05 18:45:20,004:INFO:Checking exceptions
2024-02-05 18:45:20,021:INFO:Importing libraries
2024-02-05 18:45:20,021:INFO:Copying training dataset
2024-02-05 18:45:20,025:INFO:Getting model names
2024-02-05 18:45:20,029:INFO:SubProcess create_model() called ==================================
2024-02-05 18:45:20,034:INFO:Initializing create_model()
2024-02-05 18:45:20,036:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=VotingRegressor(estimators=[('Gradient Boosting Regressor',
                             GradientBoostingRegressor(random_state=123)),
                            ('Light Gradient Boosting Machine',
                             LGBMRegressor(n_jobs=-1, random_state=123)),
                            ('Extra Trees Regressor',
                             ExtraTreesRegressor(n_jobs=-1, random_state=123))],
                n_jobs=-1), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B31583A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:45:20,037:INFO:Checking exceptions
2024-02-05 18:45:20,037:INFO:Importing libraries
2024-02-05 18:45:20,037:INFO:Copying training dataset
2024-02-05 18:45:20,045:INFO:Defining folds
2024-02-05 18:45:20,045:INFO:Declaring metric variables
2024-02-05 18:45:20,048:INFO:Importing untrained model
2024-02-05 18:45:20,048:INFO:Declaring custom model
2024-02-05 18:45:20,052:INFO:Voting Regressor Imported successfully
2024-02-05 18:45:20,057:INFO:Starting cross validation
2024-02-05 18:45:20,059:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:45:24,775:INFO:Calculating mean and std
2024-02-05 18:45:24,777:INFO:Creating metrics dataframe
2024-02-05 18:45:24,788:INFO:Finalizing model
2024-02-05 18:45:25,664:INFO:Uploading results into container
2024-02-05 18:45:25,665:INFO:Uploading model into container now
2024-02-05 18:45:25,666:INFO:_master_model_container: 47
2024-02-05 18:45:25,666:INFO:_display_container: 11
2024-02-05 18:45:25,669:INFO:VotingRegressor(estimators=[('Gradient Boosting Regressor',
                             GradientBoostingRegressor(random_state=123)),
                            ('Light Gradient Boosting Machine',
                             LGBMRegressor(n_jobs=-1, random_state=123)),
                            ('Extra Trees Regressor',
                             ExtraTreesRegressor(n_jobs=-1, random_state=123))],
                n_jobs=-1)
2024-02-05 18:45:25,669:INFO:create_model() successfully completed......................................
2024-02-05 18:45:25,775:INFO:SubProcess create_model() end ==================================
2024-02-05 18:45:25,776:INFO:Creating Dashboard logs
2024-02-05 18:45:25,779:INFO:Model: Voting Regressor
2024-02-05 18:45:25,806:INFO:Logged params: {'estimators': [('Gradient Boosting Regressor', GradientBoostingRegressor(random_state=123)), ('Light Gradient Boosting Machine', LGBMRegressor(n_jobs=-1, random_state=123)), ('Extra Trees Regressor', ExtraTreesRegressor(n_jobs=-1, random_state=123))], 'n_jobs': -1, 'verbose': False, 'weights': None, 'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=123), 'Light Gradient Boosting Machine': LGBMRegressor(n_jobs=-1, random_state=123), 'Extra Trees Regressor': ExtraTreesRegressor(n_jobs=-1, random_state=123), 'Gradient Boosting Regressor__alpha': 0.9, 'Gradient Boosting Regressor__ccp_alpha': 0.0, 'Gradient Boosting Regressor__criterion': 'friedman_mse', 'Gradient Boosting Regressor__init': None, 'Gradient Boosting Regressor__learning_rate': 0.1, 'Gradient Boosting Regressor__loss': 'squared_error', 'Gradient Boosting Regressor__max_depth': 3, 'Gradient Boosting Regressor__max_features': None, 'Gradient Boosting Regressor__max_leaf_nodes': None, 'Gradient Boosting Regressor__min_impurity_decrease': 0.0, 'Gradient Boosting Regressor__min_samples_leaf': 1, 'Gradient Boosting Regressor__min_samples_split': 2, 'Gradient Boosting Regressor__min_weight_fraction_leaf': 0.0, 'Gradient Boosting Regressor__n_estimators': 100, 'Gradient Boosting Regressor__n_iter_no_change': None, 'Gradient Boosting Regressor__random_state': 123, 'Gradient Boosting Regressor__subsample': 1.0, 'Gradient Boosting Regressor__tol': 0.0001, 'Gradient Boosting Regressor__validation_fraction': 0.1, 'Gradient Boosting Regressor__verbose': 0, 'Gradient Boosting Regressor__warm_start': False, 'Light Gradient Boosting Machine__boosting_type': 'gbdt', 'Light Gradient Boosting Machine__class_weight': None, 'Light Gradient Boosting Machine__colsample_bytree': 1.0, 'Light Gradient Boosting Machine__importance_type': 'split', 'Light Gradient Boosting Machine__learning_rate': 0.1, 'Light Gradient Boosting Machine__max_depth': -1, 'Light Gradient Boosting Machine__min_child_samples': 20, 'Light Gradient Boosting Machine__min_child_weight': 0.001, 'Light Gradient Boosting Machine__min_split_gain': 0.0, 'Light Gradient Boosting Machine__n_estimators': 100, 'Light Gradient Boosting Machine__n_jobs': -1, 'Light Gradient Boosting Machine__num_leaves': 31, 'Light Gradient Boosting Machine__objective': None, 'Light Gradient Boosting Machine__random_state': 123, 'Light Gradient Boosting Machine__reg_alpha': 0.0, 'Light Gradient Boosting Machine__reg_lambda': 0.0, 'Light Gradient Boosting Machine__subsample': 1.0, 'Light Gradient Boosting Machine__subsample_for_bin': 200000, 'Light Gradient Boosting Machine__subsample_freq': 0, 'Extra Trees Regressor__bootstrap': False, 'Extra Trees Regressor__ccp_alpha': 0.0, 'Extra Trees Regressor__criterion': 'squared_error', 'Extra Trees Regressor__max_depth': None, 'Extra Trees Regressor__max_features': 1.0, 'Extra Trees Regressor__max_leaf_nodes': None, 'Extra Trees Regressor__max_samples': None, 'Extra Trees Regressor__min_impurity_decrease': 0.0, 'Extra Trees Regressor__min_samples_leaf': 1, 'Extra Trees Regressor__min_samples_split': 2, 'Extra Trees Regressor__min_weight_fraction_leaf': 0.0, 'Extra Trees Regressor__n_estimators': 100, 'Extra Trees Regressor__n_jobs': -1, 'Extra Trees Regressor__oob_score': False, 'Extra Trees Regressor__random_state': 123, 'Extra Trees Regressor__verbose': 0, 'Extra Trees Regressor__warm_start': False}
2024-02-05 18:45:25,907:INFO:Initializing predict_model()
2024-02-05 18:45:25,908:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=VotingRegressor(estimators=[('Gradient Boosting Regressor',
                             GradientBoostingRegressor(random_state=123)),
                            ('Light Gradient Boosting Machine',
                             LGBMRegressor(n_jobs=-1, random_state=123)),
                            ('Extra Trees Regressor',
                             ExtraTreesRegressor(n_jobs=-1, random_state=123))],
                n_jobs=-1), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D7B4428160>)
2024-02-05 18:45:25,908:INFO:Checking exceptions
2024-02-05 18:45:25,908:INFO:Preloading libraries
2024-02-05 18:45:26,415:INFO:_master_model_container: 47
2024-02-05 18:45:26,415:INFO:_display_container: 11
2024-02-05 18:45:26,418:INFO:VotingRegressor(estimators=[('Gradient Boosting Regressor',
                             GradientBoostingRegressor(random_state=123)),
                            ('Light Gradient Boosting Machine',
                             LGBMRegressor(n_jobs=-1, random_state=123)),
                            ('Extra Trees Regressor',
                             ExtraTreesRegressor(n_jobs=-1, random_state=123))],
                n_jobs=-1)
2024-02-05 18:45:26,418:INFO:blend_models() successfully completed......................................
2024-02-05 18:45:26,686:INFO:Initializing stack_models()
2024-02-05 18:45:26,686:INFO:stack_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator_list=[GradientBoostingRegressor(random_state=123), LGBMRegressor(n_jobs=-1, random_state=123), ExtraTreesRegressor(n_jobs=-1, random_state=123)], meta_model=None, meta_model_fold=5, fold=None, round=4, method=auto, restack=True, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-05 18:45:26,686:INFO:Checking exceptions
2024-02-05 18:45:26,693:INFO:Defining meta model
2024-02-05 18:45:26,709:INFO:Getting model names
2024-02-05 18:45:26,709:INFO:[('Gradient Boosting Regressor', GradientBoostingRegressor(random_state=123)), ('Light Gradient Boosting Machine', LGBMRegressor(n_jobs=-1, random_state=123)), ('Extra Trees Regressor', ExtraTreesRegressor(n_jobs=-1, random_state=123))]
2024-02-05 18:45:26,713:INFO:SubProcess create_model() called ==================================
2024-02-05 18:45:26,716:INFO:Initializing create_model()
2024-02-05 18:45:26,716:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=StackingRegressor(cv=5,
                  estimators=[('Gradient Boosting Regressor',
                               GradientBoostingRegressor(random_state=123)),
                              ('Light Gradient Boosting Machine',
                               LGBMRegressor(n_jobs=-1, random_state=123)),
                              ('Extra Trees Regressor',
                               ExtraTreesRegressor(n_jobs=-1,
                                                   random_state=123))],
                  final_estimator=LinearRegression(n_jobs=-1), n_jobs=-1,
                  passthrough=True), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7B25C0700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:45:26,716:INFO:Checking exceptions
2024-02-05 18:45:26,716:INFO:Importing libraries
2024-02-05 18:45:26,716:INFO:Copying training dataset
2024-02-05 18:45:26,720:INFO:Defining folds
2024-02-05 18:45:26,721:INFO:Declaring metric variables
2024-02-05 18:45:26,723:INFO:Importing untrained model
2024-02-05 18:45:26,723:INFO:Declaring custom model
2024-02-05 18:45:26,727:INFO:Stacking Regressor Imported successfully
2024-02-05 18:45:26,733:INFO:Starting cross validation
2024-02-05 18:45:26,736:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 18:45:46,686:INFO:Calculating mean and std
2024-02-05 18:45:46,688:INFO:Creating metrics dataframe
2024-02-05 18:45:46,696:INFO:Finalizing model
2024-02-05 18:45:49,231:INFO:Uploading results into container
2024-02-05 18:45:49,232:INFO:Uploading model into container now
2024-02-05 18:45:49,232:INFO:_master_model_container: 48
2024-02-05 18:45:49,232:INFO:_display_container: 12
2024-02-05 18:45:49,236:INFO:StackingRegressor(cv=5,
                  estimators=[('Gradient Boosting Regressor',
                               GradientBoostingRegressor(random_state=123)),
                              ('Light Gradient Boosting Machine',
                               LGBMRegressor(n_jobs=-1, random_state=123)),
                              ('Extra Trees Regressor',
                               ExtraTreesRegressor(n_jobs=-1,
                                                   random_state=123))],
                  final_estimator=LinearRegression(n_jobs=-1), n_jobs=-1,
                  passthrough=True)
2024-02-05 18:45:49,236:INFO:create_model() successfully completed......................................
2024-02-05 18:45:49,337:INFO:SubProcess create_model() end ==================================
2024-02-05 18:45:49,338:INFO:Creating Dashboard logs
2024-02-05 18:45:49,342:INFO:Model: Stacking Regressor
2024-02-05 18:45:49,370:INFO:Logged params: {'cv': 5, 'estimators': [('Gradient Boosting Regressor', GradientBoostingRegressor(random_state=123)), ('Light Gradient Boosting Machine', LGBMRegressor(n_jobs=-1, random_state=123)), ('Extra Trees Regressor', ExtraTreesRegressor(n_jobs=-1, random_state=123))], 'final_estimator__copy_X': True, 'final_estimator__fit_intercept': True, 'final_estimator__n_jobs': -1, 'final_estimator__positive': False, 'final_estimator': LinearRegression(n_jobs=-1), 'n_jobs': -1, 'passthrough': True, 'verbose': 0, 'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=123), 'Light Gradient Boosting Machine': LGBMRegressor(n_jobs=-1, random_state=123), 'Extra Trees Regressor': ExtraTreesRegressor(n_jobs=-1, random_state=123), 'Gradient Boosting Regressor__alpha': 0.9, 'Gradient Boosting Regressor__ccp_alpha': 0.0, 'Gradient Boosting Regressor__criterion': 'friedman_mse', 'Gradient Boosting Regressor__init': None, 'Gradient Boosting Regressor__learning_rate': 0.1, 'Gradient Boosting Regressor__loss': 'squared_error', 'Gradient Boosting Regressor__max_depth': 3, 'Gradient Boosting Regressor__max_features': None, 'Gradient Boosting Regressor__max_leaf_nodes': None, 'Gradient Boosting Regressor__min_impurity_decrease': 0.0, 'Gradient Boosting Regressor__min_samples_leaf': 1, 'Gradient Boosting Regressor__min_samples_split': 2, 'Gradient Boosting Regressor__min_weight_fraction_leaf': 0.0, 'Gradient Boosting Regressor__n_estimators': 100, 'Gradient Boosting Regressor__n_iter_no_change': None, 'Gradient Boosting Regressor__random_state': 123, 'Gradient Boosting Regressor__subsample': 1.0, 'Gradient Boosting Regressor__tol': 0.0001, 'Gradient Boosting Regressor__validation_fraction': 0.1, 'Gradient Boosting Regressor__verbose': 0, 'Gradient Boosting Regressor__warm_start': False, 'Light Gradient Boosting Machine__boosting_type': 'gbdt', 'Light Gradient Boosting Machine__class_weight': None, 'Light Gradient Boosting Machine__colsample_bytree': 1.0, 'Light Gradient Boosting Machine__importance_type': 'split', 'Light Gradient Boosting Machine__learning_rate': 0.1, 'Light Gradient Boosting Machine__max_depth': -1, 'Light Gradient Boosting Machine__min_child_samples': 20, 'Light Gradient Boosting Machine__min_child_weight': 0.001, 'Light Gradient Boosting Machine__min_split_gain': 0.0, 'Light Gradient Boosting Machine__n_estimators': 100, 'Light Gradient Boosting Machine__n_jobs': -1, 'Light Gradient Boosting Machine__num_leaves': 31, 'Light Gradient Boosting Machine__objective': None, 'Light Gradient Boosting Machine__random_state': 123, 'Light Gradient Boosting Machine__reg_alpha': 0.0, 'Light Gradient Boosting Machine__reg_lambda': 0.0, 'Light Gradient Boosting Machine__subsample': 1.0, 'Light Gradient Boosting Machine__subsample_for_bin': 200000, 'Light Gradient Boosting Machine__subsample_freq': 0, 'Extra Trees Regressor__bootstrap': False, 'Extra Trees Regressor__ccp_alpha': 0.0, 'Extra Trees Regressor__criterion': 'squared_error', 'Extra Trees Regressor__max_depth': None, 'Extra Trees Regressor__max_features': 1.0, 'Extra Trees Regressor__max_leaf_nodes': None, 'Extra Trees Regressor__max_samples': None, 'Extra Trees Regressor__min_impurity_decrease': 0.0, 'Extra Trees Regressor__min_samples_leaf': 1, 'Extra Trees Regressor__min_samples_split': 2, 'Extra Trees Regressor__min_weight_fraction_leaf': 0.0, 'Extra Trees Regressor__n_estimators': 100, 'Extra Trees Regressor__n_jobs': -1, 'Extra Trees Regressor__oob_score': False, 'Extra Trees Regressor__random_state': 123, 'Extra Trees Regressor__verbose': 0, 'Extra Trees Regressor__warm_start': False}
2024-02-05 18:45:49,478:INFO:Initializing predict_model()
2024-02-05 18:45:49,478:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=StackingRegressor(cv=5,
                  estimators=[('Gradient Boosting Regressor',
                               GradientBoostingRegressor(random_state=123)),
                              ('Light Gradient Boosting Machine',
                               LGBMRegressor(n_jobs=-1, random_state=123)),
                              ('Extra Trees Regressor',
                               ExtraTreesRegressor(n_jobs=-1,
                                                   random_state=123))],
                  final_estimator=LinearRegression(n_jobs=-1), n_jobs=-1,
                  passthrough=True), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D7B4426430>)
2024-02-05 18:45:49,478:INFO:Checking exceptions
2024-02-05 18:45:49,478:INFO:Preloading libraries
2024-02-05 18:45:49,982:INFO:_master_model_container: 48
2024-02-05 18:45:49,982:INFO:_display_container: 12
2024-02-05 18:45:49,985:INFO:StackingRegressor(cv=5,
                  estimators=[('Gradient Boosting Regressor',
                               GradientBoostingRegressor(random_state=123)),
                              ('Light Gradient Boosting Machine',
                               LGBMRegressor(n_jobs=-1, random_state=123)),
                              ('Extra Trees Regressor',
                               ExtraTreesRegressor(n_jobs=-1,
                                                   random_state=123))],
                  final_estimator=LinearRegression(n_jobs=-1), n_jobs=-1,
                  passthrough=True)
2024-02-05 18:45:49,986:INFO:stack_models() successfully completed......................................
2024-02-05 18:45:50,352:INFO:Initializing interpret_model()
2024-02-05 18:45:50,353:INFO:interpret_model(estimator=LGBMRegressor(n_jobs=-1, random_state=123), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>)
2024-02-05 18:45:50,353:INFO:Checking exceptions
2024-02-05 18:45:50,353:INFO:Soft dependency imported: shap: 0.44.1
2024-02-05 18:45:50,770:INFO:plot type: summary
2024-02-05 18:45:50,770:INFO:Creating TreeExplainer
2024-02-05 18:45:50,855:INFO:Compiling shap values
2024-02-05 18:45:51,387:INFO:Visual Rendered Successfully
2024-02-05 18:45:51,388:INFO:interpret_model() successfully completed......................................
2024-02-05 18:45:51,537:INFO:Initializing interpret_model()
2024-02-05 18:45:51,538:INFO:interpret_model(estimator=LGBMRegressor(n_jobs=-1, random_state=123), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=correlation, save=False, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>)
2024-02-05 18:45:51,538:INFO:Checking exceptions
2024-02-05 18:45:51,538:INFO:Soft dependency imported: shap: 0.44.1
2024-02-05 18:45:51,648:INFO:plot type: correlation
2024-02-05 18:45:51,648:WARNING:No feature passed. Default value of feature used for correlation plot: MSSubClass_SPLIT FOYER
2024-02-05 18:45:51,648:INFO:Creating TreeExplainer
2024-02-05 18:45:51,742:INFO:Compiling shap values
2024-02-05 18:45:51,835:INFO:model type detected: type 2
2024-02-05 18:45:51,961:INFO:Visual Rendered Successfully
2024-02-05 18:45:51,961:INFO:interpret_model() successfully completed......................................
2024-02-05 18:45:52,172:INFO:Initializing interpret_model()
2024-02-05 18:45:52,172:INFO:interpret_model(estimator=LGBMRegressor(n_jobs=-1, random_state=123), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=12, plot=reason, save=False, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>)
2024-02-05 18:45:52,172:INFO:Checking exceptions
2024-02-05 18:45:52,172:INFO:Soft dependency imported: shap: 0.44.1
2024-02-05 18:45:52,294:INFO:plot type: reason
2024-02-05 18:45:52,294:INFO:model type detected: type 2
2024-02-05 18:45:52,294:INFO:Creating TreeExplainer
2024-02-05 18:45:52,373:INFO:Compiling shap values
2024-02-05 18:45:52,476:INFO:Visual Rendered Successfully
2024-02-05 18:45:52,476:INFO:interpret_model() successfully completed......................................
2024-02-05 18:45:52,758:INFO:Initializing automl()
2024-02-05 18:45:52,758:INFO:automl(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, optimize=R2, use_holdout=False, turbo=True, return_train_score=False)
2024-02-05 18:45:52,758:INFO:Model Selection Basis : CV Results on Training set
2024-02-05 18:45:52,758:INFO:Checking model 18
2024-02-05 18:45:52,758:INFO:Checking model 20
2024-02-05 18:45:52,759:INFO:Checking model 21
2024-02-05 18:45:52,759:INFO:Checking model 22
2024-02-05 18:45:52,759:INFO:Checking model 23
2024-02-05 18:45:52,759:INFO:Checking model 24
2024-02-05 18:45:52,759:INFO:Checking model 25
2024-02-05 18:45:52,759:INFO:Checking model 26
2024-02-05 18:45:52,759:INFO:Checking model 27
2024-02-05 18:45:52,759:INFO:Checking model 28
2024-02-05 18:45:52,760:INFO:Checking model 29
2024-02-05 18:45:52,760:INFO:Checking model 30
2024-02-05 18:45:52,760:INFO:Checking model 31
2024-02-05 18:45:52,760:INFO:Checking model 32
2024-02-05 18:45:52,761:INFO:Checking model 33
2024-02-05 18:45:52,761:INFO:Checking model 34
2024-02-05 18:45:52,761:INFO:Checking model 35
2024-02-05 18:45:52,761:INFO:Checking model 36
2024-02-05 18:45:52,761:INFO:Checking model 37
2024-02-05 18:45:52,761:INFO:Checking model 38
2024-02-05 18:45:52,761:INFO:Checking model 39
2024-02-05 18:45:52,761:INFO:Checking model 40
2024-02-05 18:45:52,761:INFO:Checking model 41
2024-02-05 18:45:52,762:INFO:Checking model 42
2024-02-05 18:45:52,762:INFO:Checking model 43
2024-02-05 18:45:52,762:INFO:Checking model 44
2024-02-05 18:45:52,762:INFO:Checking model 45
2024-02-05 18:45:52,762:INFO:Checking model 46
2024-02-05 18:45:52,762:INFO:Checking model 47
2024-02-05 18:45:52,762:INFO:Initializing create_model()
2024-02-05 18:45:52,762:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:45:52,762:INFO:Checking exceptions
2024-02-05 18:45:52,762:INFO:Importing libraries
2024-02-05 18:45:52,762:INFO:Copying training dataset
2024-02-05 18:45:52,769:INFO:Defining folds
2024-02-05 18:45:52,769:INFO:Declaring metric variables
2024-02-05 18:45:52,770:INFO:Importing untrained model
2024-02-05 18:45:52,770:INFO:Declaring custom model
2024-02-05 18:45:52,771:INFO:Bagging Regressor Imported successfully
2024-02-05 18:45:52,773:INFO:Cross validation set to False
2024-02-05 18:45:52,773:INFO:Fitting Model
2024-02-05 18:46:07,769:INFO:BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123)
2024-02-05 18:46:07,769:INFO:create_model() successfully completed......................................
2024-02-05 18:46:07,961:INFO:BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123)
2024-02-05 18:46:07,961:INFO:automl() successfully completed......................................
2024-02-05 18:46:08,035:INFO:Soft dependency imported: explainerdashboard: 0.4.5
2024-02-05 18:46:08,481:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\dash\dash.py:538: UserWarning:

JupyterDash is deprecated, use Dash instead.
See https://dash.plotly.com/dash-in-jupyter for more details.


2024-02-05 18:46:21,586:INFO:Initializing predict_model()
2024-02-05 18:46:21,586:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001D7B295F3A0>)
2024-02-05 18:46:21,586:INFO:Checking exceptions
2024-02-05 18:46:21,586:INFO:Preloading libraries
2024-02-05 18:46:22,073:INFO:Soft dependency imported: gradio: 3.50.0
2024-02-05 18:46:22,076:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,076:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,078:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,078:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,078:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,078:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,079:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,080:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,082:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,082:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,083:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,083:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,083:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,083:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,084:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,085:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,085:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,086:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,086:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,088:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,088:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,090:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,090:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,091:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,092:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,092:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,092:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,092:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,093:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,093:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,093:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,093:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,094:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,096:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,096:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,097:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,097:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,097:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,098:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,098:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,100:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,100:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,102:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,102:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,102:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-05 18:46:22,102:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,102:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-05 18:46:22,282:INFO:Soft dependency imported: fastapi: 0.109.1
2024-02-05 18:46:22,283:INFO:Soft dependency imported: uvicorn: 0.27.0.post1
2024-02-05 18:46:22,283:INFO:Soft dependency imported: pydantic: 1.10.12
2024-02-05 18:46:22,300:INFO:Initializing save_model()
2024-02-05 18:46:22,300:INFO:save_model(model=GradientBoostingRegressor(random_state=123), model_name=housing_price_api, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2024-02-05 18:46:22,300:INFO:Adding model into prep_pipe
2024-02-05 18:46:22,334:INFO:housing_price_api.pkl saved in current working directory
2024-02-05 18:46:22,365:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', 'HeatingQ...
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('trained_model', GradientBoostingRegressor(random_state=123))])
2024-02-05 18:46:22,365:INFO:save_model() successfully completed......................................
2024-02-05 18:46:25,092:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-05 18:46:25,092:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-05 18:46:25,092:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-05 18:46:25,092:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-05 18:46:25,348:INFO:Initializing load_model()
2024-02-05 18:46:25,348:INFO:load_model(model_name=housing_price_api, platform=None, authentication=None, verbose=True)
2024-02-05 18:46:25,831:INFO:Initializing finalize_model()
2024-02-05 18:46:25,831:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2024-02-05 18:46:25,831:INFO:Finalizing GradientBoostingRegressor(random_state=123)
2024-02-05 18:46:25,831:INFO:Initializing create_model()
2024-02-05 18:46:25,831:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D79C124430>, estimator=GradientBoostingRegressor(random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 18:46:25,831:INFO:Checking exceptions
2024-02-05 18:46:25,839:INFO:Importing libraries
2024-02-05 18:46:25,840:INFO:Copying training dataset
2024-02-05 18:46:25,840:INFO:Defining folds
2024-02-05 18:46:25,840:INFO:Declaring metric variables
2024-02-05 18:46:25,840:INFO:Importing untrained model
2024-02-05 18:46:25,840:INFO:Declaring custom model
2024-02-05 18:46:25,841:INFO:Gradient Boosting Regressor Imported successfully
2024-02-05 18:46:25,843:INFO:Cross validation set to False
2024-02-05 18:46:25,843:INFO:Fitting Model
2024-02-05 18:46:26,702:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', 'HeatingQ...
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=123))])
2024-02-05 18:46:26,702:INFO:create_model() successfully completed......................................
2024-02-05 18:46:26,857:INFO:Creating Dashboard logs
2024-02-05 18:46:26,858:INFO:Model: Gradient Boosting Regressor
2024-02-05 18:46:26,894:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-05 18:46:27,112:INFO:_master_model_container: 48
2024-02-05 18:46:27,112:INFO:_display_container: 13
2024-02-05 18:46:27,128:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', 'HeatingQ...
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('actual_estimator',
                 GradientBoostingRegressor(random_state=123))])
2024-02-05 18:46:27,128:INFO:finalize_model() successfully completed......................................
2024-02-05 18:50:59,845:INFO:Initializing save_model()
2024-02-05 18:50:59,845:INFO:save_model(model=GradientBoostingRegressor(random_state=123), model_name=houseprice_best_model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2024-02-05 18:50:59,845:INFO:Adding model into prep_pipe
2024-02-05 18:50:59,864:INFO:houseprice_best_model.pkl saved in current working directory
2024-02-05 18:50:59,880:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', 'HeatingQ...
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('trained_model', GradientBoostingRegressor(random_state=123))])
2024-02-05 18:50:59,880:INFO:save_model() successfully completed......................................
2024-02-05 18:52:08,296:INFO:Initializing load_model()
2024-02-05 18:52:08,296:INFO:load_model(model_name=houseprice_best_model, platform=None, authentication=None, verbose=True)
2024-02-05 18:52:22,961:INFO:Initializing load_model()
2024-02-05 18:52:22,962:INFO:load_model(model_name=houseprice_best_model, platform=None, authentication=None, verbose=True)
2024-02-05 18:55:50,910:INFO:PyCaret RegressionExperiment
2024-02-05 18:55:50,910:INFO:Logging name: houseprice1
2024-02-05 18:55:50,910:INFO:ML Usecase: MLUsecase.REGRESSION
2024-02-05 18:55:50,911:INFO:version 3.2.0
2024-02-05 18:55:50,911:INFO:Initializing setup()
2024-02-05 18:55:50,911:INFO:self.USI: 9a36
2024-02-05 18:55:50,911:INFO:self._variable_keys: {'USI', 'fold_shuffle_param', 'target_param', 'transform_target_param', 'data', 'exp_id', 'y_train', 'X', 'X_test', 'fold_groups_param', 'X_train', 'memory', 'html_param', 'seed', 'gpu_param', '_ml_usecase', 'gpu_n_jobs_param', 'exp_name_log', 'idx', 'y_test', 'n_jobs_param', '_available_plots', 'pipeline', 'logging_param', 'fold_generator', 'log_plots_param', 'y'}
2024-02-05 18:55:50,911:INFO:Checking environment
2024-02-05 18:55:50,911:INFO:python_version: 3.9.9
2024-02-05 18:55:50,911:INFO:python_build: ('tags/v3.9.9:ccb0e6a', 'Nov 15 2021 18:08:50')
2024-02-05 18:55:50,911:INFO:machine: AMD64
2024-02-05 18:55:50,911:INFO:platform: Windows-10-10.0.22621-SP0
2024-02-05 18:55:50,914:INFO:Memory: svmem(total=16856203264, available=8610398208, percent=48.9, used=8245805056, free=8610398208)
2024-02-05 18:55:50,914:INFO:Physical Core: 4
2024-02-05 18:55:50,914:INFO:Logical Core: 8
2024-02-05 18:55:50,914:INFO:Checking libraries
2024-02-05 18:55:50,914:INFO:System:
2024-02-05 18:55:50,914:INFO:    python: 3.9.9 (tags/v3.9.9:ccb0e6a, Nov 15 2021, 18:08:50) [MSC v.1929 64 bit (AMD64)]
2024-02-05 18:55:50,914:INFO:executable: c:\Users\joseg\AppData\Local\Programs\Python\Python39\python.exe
2024-02-05 18:55:50,914:INFO:   machine: Windows-10-10.0.22621-SP0
2024-02-05 18:55:50,914:INFO:PyCaret required dependencies:
2024-02-05 18:55:50,914:INFO:                 pip: 21.2.4
2024-02-05 18:55:50,915:INFO:          setuptools: 58.1.0
2024-02-05 18:55:50,915:INFO:             pycaret: 3.2.0
2024-02-05 18:55:50,915:INFO:             IPython: 8.18.1
2024-02-05 18:55:50,915:INFO:          ipywidgets: 8.1.1
2024-02-05 18:55:50,915:INFO:                tqdm: 4.66.1
2024-02-05 18:55:50,915:INFO:               numpy: 1.23.5
2024-02-05 18:55:50,915:INFO:              pandas: 1.5.3
2024-02-05 18:55:50,915:INFO:              jinja2: 3.1.3
2024-02-05 18:55:50,915:INFO:               scipy: 1.10.1
2024-02-05 18:55:50,915:INFO:              joblib: 1.3.2
2024-02-05 18:55:50,915:INFO:             sklearn: 1.2.2
2024-02-05 18:55:50,915:INFO:                pyod: 1.1.2
2024-02-05 18:55:50,915:INFO:            imblearn: 0.12.0
2024-02-05 18:55:50,915:INFO:   category_encoders: 2.6.3
2024-02-05 18:55:50,915:INFO:            lightgbm: 4.3.0
2024-02-05 18:55:50,915:INFO:               numba: 0.59.0
2024-02-05 18:55:50,915:INFO:            requests: 2.31.0
2024-02-05 18:55:50,915:INFO:          matplotlib: 3.6.0
2024-02-05 18:55:50,915:INFO:          scikitplot: 0.3.7
2024-02-05 18:55:50,915:INFO:         yellowbrick: 1.5
2024-02-05 18:55:50,915:INFO:              plotly: 5.18.0
2024-02-05 18:55:50,915:INFO:    plotly-resampler: Not installed
2024-02-05 18:55:50,915:INFO:             kaleido: 0.2.1
2024-02-05 18:55:50,915:INFO:           schemdraw: 0.15
2024-02-05 18:55:50,915:INFO:         statsmodels: 0.14.1
2024-02-05 18:55:50,915:INFO:              sktime: 0.21.1
2024-02-05 18:55:50,915:INFO:               tbats: 1.1.3
2024-02-05 18:55:50,915:INFO:            pmdarima: 2.0.4
2024-02-05 18:55:50,915:INFO:              psutil: 5.9.8
2024-02-05 18:55:50,915:INFO:          markupsafe: 2.1.5
2024-02-05 18:55:50,915:INFO:             pickle5: Not installed
2024-02-05 18:55:50,916:INFO:         cloudpickle: 3.0.0
2024-02-05 18:55:50,916:INFO:         deprecation: 2.1.0
2024-02-05 18:55:50,916:INFO:              xxhash: 3.4.1
2024-02-05 18:55:50,916:INFO:           wurlitzer: Not installed
2024-02-05 18:55:50,916:INFO:PyCaret optional dependencies:
2024-02-05 18:55:50,916:INFO:                shap: 0.44.1
2024-02-05 18:55:50,916:INFO:           interpret: Not installed
2024-02-05 18:55:50,916:INFO:                umap: Not installed
2024-02-05 18:55:50,916:INFO:     ydata_profiling: Not installed
2024-02-05 18:55:50,916:INFO:  explainerdashboard: 0.4.5
2024-02-05 18:55:50,916:INFO:             autoviz: Not installed
2024-02-05 18:55:50,916:INFO:           fairlearn: Not installed
2024-02-05 18:55:50,916:INFO:          deepchecks: Not installed
2024-02-05 18:55:50,916:INFO:             xgboost: Not installed
2024-02-05 18:55:50,916:INFO:            catboost: Not installed
2024-02-05 18:55:50,916:INFO:              kmodes: Not installed
2024-02-05 18:55:50,916:INFO:             mlxtend: Not installed
2024-02-05 18:55:50,916:INFO:       statsforecast: Not installed
2024-02-05 18:55:50,916:INFO:        tune_sklearn: Not installed
2024-02-05 18:55:50,916:INFO:                 ray: Not installed
2024-02-05 18:55:50,916:INFO:            hyperopt: Not installed
2024-02-05 18:55:50,916:INFO:              optuna: Not installed
2024-02-05 18:55:50,916:INFO:               skopt: Not installed
2024-02-05 18:55:50,916:INFO:              mlflow: 2.10.0
2024-02-05 18:55:50,916:INFO:              gradio: 3.50.0
2024-02-05 18:55:50,916:INFO:             fastapi: 0.109.1
2024-02-05 18:55:50,916:INFO:             uvicorn: 0.27.0.post1
2024-02-05 18:55:50,916:INFO:              m2cgen: Not installed
2024-02-05 18:55:50,916:INFO:           evidently: Not installed
2024-02-05 18:55:50,916:INFO:               fugue: Not installed
2024-02-05 18:55:50,916:INFO:           streamlit: Not installed
2024-02-05 18:55:50,916:INFO:             prophet: Not installed
2024-02-05 18:55:50,916:INFO:None
2024-02-05 18:55:50,917:INFO:Set up data.
2024-02-05 18:55:50,936:INFO:Set up folding strategy.
2024-02-05 18:55:50,937:INFO:Set up train/test split.
2024-02-05 18:55:50,945:INFO:Set up index.
2024-02-05 18:55:50,946:INFO:Assigning column types.
2024-02-05 18:55:50,950:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-02-05 18:55:51,041:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,041:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,123:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,124:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,124:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-02-05 18:55:51,208:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,208:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,289:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,290:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,291:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-02-05 18:55:51,374:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,374:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,455:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,456:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,457:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-02-05 18:55:51,541:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,541:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,621:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,621:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,621:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-02-05 18:55:51,707:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,707:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,787:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,787:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,787:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-02-05 18:55:51,873:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,873:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,959:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,959:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:51,960:INFO:Preparing preprocessing pipeline...
2024-02-05 18:55:51,960:INFO:Set up simple imputation.
2024-02-05 18:55:51,965:INFO:Set up encoding of ordinal features.
2024-02-05 18:55:51,966:INFO:Set up encoding of categorical features.
2024-02-05 18:55:52,107:INFO:Finished creating preprocessing pipeline.
2024-02-05 18:55:52,124:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))])
2024-02-05 18:55:52,125:INFO:Creating final display dataframe.
2024-02-05 18:55:52,232:INFO:Setup _display_container:                     Description         Value
0                    Session id           123
1                        Target     SalePrice
2                   Target type    Regression
3           Original data shape    (1456, 56)
4        Transformed data shape   (1456, 134)
5   Transformed train set shape   (1019, 134)
6    Transformed test set shape    (437, 134)
7              Ordinal features             1
8              Numeric features            40
9          Categorical features            15
10     Rows with missing values         99.7%
11                   Preprocess          True
12              Imputation type        simple
13           Numeric imputation          mean
14       Categorical imputation          mode
15     Maximum one-hot encoding            25
16              Encoding method          None
17               Fold Generator         KFold
18                  Fold Number            10
19                     CPU Jobs            -1
20                      Use GPU         False
21               Log Experiment  MlflowLogger
22              Experiment Name   houseprice1
23                          USI          9a36
2024-02-05 18:55:52,332:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:52,332:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:52,414:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:52,415:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-05 18:55:52,415:INFO:Logging experiment in loggers
2024-02-05 18:55:52,466:INFO:SubProcess save_model() called ==================================
2024-02-05 18:55:52,497:INFO:Initializing save_model()
2024-02-05 18:55:52,498:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))]), model_name=C:\Users\joseg\AppData\Local\Temp\tmpul589mv2\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2024-02-05 18:55:52,498:INFO:Adding model into prep_pipe
2024-02-05 18:55:52,498:WARNING:Only Model saved as it was a pipeline.
2024-02-05 18:55:52,509:INFO:C:\Users\joseg\AppData\Local\Temp\tmpul589mv2\Transformation Pipeline.pkl saved in current working directory
2024-02-05 18:55:52,523:INFO:Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))])
2024-02-05 18:55:52,523:INFO:save_model() successfully completed......................................
2024-02-05 18:55:52,666:INFO:SubProcess save_model() end ==================================
2024-02-05 18:55:52,673:INFO:setup() successfully completed in 1.51s...............
2024-02-05 19:03:13,567:INFO:Initializing create_model()
2024-02-05 19:03:13,567:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=LinearRegression(n_jobs=-1), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:13,567:INFO:Checking exceptions
2024-02-05 19:03:13,568:INFO:Importing libraries
2024-02-05 19:03:13,568:INFO:Copying training dataset
2024-02-05 19:03:13,572:INFO:Defining folds
2024-02-05 19:03:13,573:INFO:Declaring metric variables
2024-02-05 19:03:13,573:INFO:Importing untrained model
2024-02-05 19:03:13,573:INFO:Declaring custom model
2024-02-05 19:03:13,573:INFO:Linear Regression Imported successfully
2024-02-05 19:03:13,573:INFO:Starting cross validation
2024-02-05 19:03:13,575:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:17,205:INFO:Calculating mean and std
2024-02-05 19:03:17,206:INFO:Creating metrics dataframe
2024-02-05 19:03:17,208:INFO:Finalizing model
2024-02-05 19:03:17,357:INFO:Uploading results into container
2024-02-05 19:03:17,357:INFO:_master_model_container: 48
2024-02-05 19:03:17,357:INFO:_display_container: 14
2024-02-05 19:03:17,358:INFO:LinearRegression(n_jobs=-1)
2024-02-05 19:03:17,358:INFO:create_model() successfully completed......................................
2024-02-05 19:03:17,506:INFO:Initializing create_model()
2024-02-05 19:03:17,506:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=Lasso(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:17,506:INFO:Checking exceptions
2024-02-05 19:03:17,508:INFO:Importing libraries
2024-02-05 19:03:17,508:INFO:Copying training dataset
2024-02-05 19:03:17,513:INFO:Defining folds
2024-02-05 19:03:17,513:INFO:Declaring metric variables
2024-02-05 19:03:17,513:INFO:Importing untrained model
2024-02-05 19:03:17,513:INFO:Declaring custom model
2024-02-05 19:03:17,514:INFO:Lasso Regression Imported successfully
2024-02-05 19:03:17,514:INFO:Starting cross validation
2024-02-05 19:03:17,515:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:17,895:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.631e+11, tolerance: 5.696e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:17,897:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.569e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:17,904:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.241e+11, tolerance: 5.445e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:17,905:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.657e+11, tolerance: 5.662e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:17,910:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.375e+11, tolerance: 5.336e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:17,917:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.799e+11, tolerance: 5.738e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:17,928:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.563e+11, tolerance: 5.378e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:17,932:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.692e+11, tolerance: 5.599e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:18,177:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.674e+11, tolerance: 5.647e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:18,178:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.671e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:18,226:INFO:Calculating mean and std
2024-02-05 19:03:18,226:INFO:Creating metrics dataframe
2024-02-05 19:03:18,228:INFO:Finalizing model
2024-02-05 19:03:18,405:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning:

Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.963e+11, tolerance: 6.216e+08


2024-02-05 19:03:18,407:INFO:Uploading results into container
2024-02-05 19:03:18,408:INFO:_master_model_container: 48
2024-02-05 19:03:18,408:INFO:_display_container: 15
2024-02-05 19:03:18,408:INFO:Lasso(random_state=123)
2024-02-05 19:03:18,408:INFO:create_model() successfully completed......................................
2024-02-05 19:03:18,547:INFO:Initializing create_model()
2024-02-05 19:03:18,547:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=Ridge(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:18,547:INFO:Checking exceptions
2024-02-05 19:03:18,549:INFO:Importing libraries
2024-02-05 19:03:18,549:INFO:Copying training dataset
2024-02-05 19:03:18,553:INFO:Defining folds
2024-02-05 19:03:18,553:INFO:Declaring metric variables
2024-02-05 19:03:18,553:INFO:Importing untrained model
2024-02-05 19:03:18,553:INFO:Declaring custom model
2024-02-05 19:03:18,553:INFO:Ridge Regression Imported successfully
2024-02-05 19:03:18,554:INFO:Starting cross validation
2024-02-05 19:03:18,555:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:19,199:INFO:Calculating mean and std
2024-02-05 19:03:19,200:INFO:Creating metrics dataframe
2024-02-05 19:03:19,202:INFO:Finalizing model
2024-02-05 19:03:19,339:INFO:Uploading results into container
2024-02-05 19:03:19,340:INFO:_master_model_container: 48
2024-02-05 19:03:19,340:INFO:_display_container: 16
2024-02-05 19:03:19,340:INFO:Ridge(random_state=123)
2024-02-05 19:03:19,340:INFO:create_model() successfully completed......................................
2024-02-05 19:03:19,481:INFO:Initializing create_model()
2024-02-05 19:03:19,482:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=ElasticNet(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:19,482:INFO:Checking exceptions
2024-02-05 19:03:19,484:INFO:Importing libraries
2024-02-05 19:03:19,484:INFO:Copying training dataset
2024-02-05 19:03:19,488:INFO:Defining folds
2024-02-05 19:03:19,488:INFO:Declaring metric variables
2024-02-05 19:03:19,489:INFO:Importing untrained model
2024-02-05 19:03:19,489:INFO:Declaring custom model
2024-02-05 19:03:19,489:INFO:Elastic Net Imported successfully
2024-02-05 19:03:19,489:INFO:Starting cross validation
2024-02-05 19:03:19,491:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:19,866:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.761e+11, tolerance: 5.445e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:19,887:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.013e+11, tolerance: 5.738e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:19,888:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.858e+11, tolerance: 5.662e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:19,912:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.863e+11, tolerance: 5.599e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:19,912:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.889e+11, tolerance: 5.696e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:19,921:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.602e+11, tolerance: 5.336e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:19,944:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.721e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:19,967:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+11, tolerance: 5.378e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:20,182:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.964e+11, tolerance: 5.647e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:20,183:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.975e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:20,248:INFO:Calculating mean and std
2024-02-05 19:03:20,248:INFO:Creating metrics dataframe
2024-02-05 19:03:20,250:INFO:Finalizing model
2024-02-05 19:03:20,462:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning:

Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.283e+11, tolerance: 6.216e+08


2024-02-05 19:03:20,464:INFO:Uploading results into container
2024-02-05 19:03:20,465:INFO:_master_model_container: 48
2024-02-05 19:03:20,465:INFO:_display_container: 17
2024-02-05 19:03:20,465:INFO:ElasticNet(random_state=123)
2024-02-05 19:03:20,465:INFO:create_model() successfully completed......................................
2024-02-05 19:03:20,615:INFO:Initializing create_model()
2024-02-05 19:03:20,615:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=Lars(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:20,615:INFO:Checking exceptions
2024-02-05 19:03:20,618:INFO:Importing libraries
2024-02-05 19:03:20,619:INFO:Copying training dataset
2024-02-05 19:03:20,623:INFO:Defining folds
2024-02-05 19:03:20,623:INFO:Declaring metric variables
2024-02-05 19:03:20,623:INFO:Importing untrained model
2024-02-05 19:03:20,623:INFO:Declaring custom model
2024-02-05 19:03:20,623:INFO:Least Angle Regression Imported successfully
2024-02-05 19:03:20,625:INFO:Starting cross validation
2024-02-05 19:03:20,626:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:20,954:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=3.936e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,956:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.412e+02, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,957:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.718e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,959:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.851e+02, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,967:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=3.621e+02, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,969:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.911e+02, with an active set of 99 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,969:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.157e+03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,971:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.149e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,971:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.555e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,972:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.731e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,973:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=1.924e+05, with an active set of 109 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,974:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.519e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,974:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 160 iterations, i.e. alpha=1.780e+05, with an active set of 112 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,978:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=1.562e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,980:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.496e+09, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,980:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=5.167e+07, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,981:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=3.422e+07, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,981:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=2.687e+07, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,981:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=2.606e+07, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,981:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=2.275e+07, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,982:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.908e+07, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,982:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=1.562e+07, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,985:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.264e+07, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,985:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.442e+07, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,986:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.173e+07, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,986:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.137e+07, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,986:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.059e+07, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,987:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=1.029e+07, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,987:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=9.833e+06, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,988:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=9.103e+06, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,988:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=3.774e+05, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,988:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=3.759e+05, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,988:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=8.950e+06, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,988:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=8.153e+04, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,988:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=7.986e+06, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,989:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=7.476e+06, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,989:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=7.475e+06, with an active set of 91 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,989:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.243e+06, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,989:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.242e+06, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,990:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=6.703e+06, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,990:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=5.425e+06, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,990:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=5.332e+06, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,991:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=4.831e+06, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,991:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=4.529e+06, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,992:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=3.694e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,992:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=3.221e+06, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,992:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=2.801e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,993:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 115 iterations, i.e. alpha=2.370e+06, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,994:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.244e+12, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,994:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=1.750e+11, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,994:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.543e+11, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,995:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.062e+11, with an active set of 107 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,995:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.027e+11, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,995:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=7.904e+10, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,996:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=7.308e+10, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,996:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=6.501e+10, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,996:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=5.857e+10, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,996:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=5.713e+10, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,999:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=5.810e+10, with an active set of 113 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:20,999:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=5.809e+10, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,000:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=3.100e+10, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,007:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=9.162e+13, with an active set of 131 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,007:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=7.581e+13, with an active set of 131 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,020:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.051e+02, with an active set of 72 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,030:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=2.116e+02, with an active set of 116 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,031:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=1.924e+02, with an active set of 116 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,033:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 146 iterations, i.e. alpha=1.674e+02, with an active set of 121 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,034:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=1.569e+02, with an active set of 124 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,035:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=1.388e+03, with an active set of 128 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,035:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=1.232e+03, with an active set of 128 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,036:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=1.900e+02, with an active set of 128 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,036:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=7.168e+00, with an active set of 128 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,240:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=3.745e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,244:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.739e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,245:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.029e+02, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,246:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=2.546e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,248:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.190e+02, with an active set of 73 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,250:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=6.923e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,250:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=5.772e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,251:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.716e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,252:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=3.202e+01, with an active set of 98 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,252:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.832e+01, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,252:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.695e+01, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,253:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=1.943e+01, with an active set of 107 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,253:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=1.431e+01, with an active set of 108 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,253:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=5.669e+03, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,253:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=1.773e+01, with an active set of 111 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,254:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=2.536e+03, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,254:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=1.472e+02, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,254:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=1.219e+01, with an active set of 116 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,255:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=1.102e+01, with an active set of 116 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,257:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 144 iterations, i.e. alpha=4.280e+01, with an active set of 125 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,257:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=3.768e+02, with an active set of 128 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,258:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=1.298e+04, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,258:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=7.540e+01, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,318:INFO:Calculating mean and std
2024-02-05 19:03:21,319:INFO:Creating metrics dataframe
2024-02-05 19:03:21,320:INFO:Finalizing model
2024-02-05 19:03:21,450:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning:

Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.375e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.


2024-02-05 19:03:21,465:INFO:Uploading results into container
2024-02-05 19:03:21,466:INFO:_master_model_container: 48
2024-02-05 19:03:21,466:INFO:_display_container: 18
2024-02-05 19:03:21,466:INFO:Lars(random_state=123)
2024-02-05 19:03:21,466:INFO:create_model() successfully completed......................................
2024-02-05 19:03:21,608:INFO:Initializing create_model()
2024-02-05 19:03:21,608:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=LassoLars(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:21,608:INFO:Checking exceptions
2024-02-05 19:03:21,610:INFO:Importing libraries
2024-02-05 19:03:21,610:INFO:Copying training dataset
2024-02-05 19:03:21,614:INFO:Defining folds
2024-02-05 19:03:21,614:INFO:Declaring metric variables
2024-02-05 19:03:21,614:INFO:Importing untrained model
2024-02-05 19:03:21,614:INFO:Declaring custom model
2024-02-05 19:03:21,615:INFO:Lasso Least Angle Regression Imported successfully
2024-02-05 19:03:21,616:INFO:Starting cross validation
2024-02-05 19:03:21,618:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:21,931:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=3.340e+02, with an active set of 46 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,932:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.955e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,936:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 70 iterations, alpha=2.179e+02, previous alpha=2.179e+02, with an active set of 57 regressors.
  warnings.warn(

2024-02-05 19:03:21,938:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.603e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,939:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.426e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,939:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 86 iterations, alpha=1.479e+02, previous alpha=1.479e+02, with an active set of 71 regressors.
  warnings.warn(

2024-02-05 19:03:21,942:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.522e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,942:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=4.005e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,944:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=3.697e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,945:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.795e+02, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,946:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.087e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,948:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=4.102e+02, with an active set of 45 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,948:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=7.153e+03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,949:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=6.685e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,950:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=6.615e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,951:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.274e+03, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,952:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 103 iterations, alpha=5.927e+01, previous alpha=5.849e+01, with an active set of 82 regressors.
  warnings.warn(

2024-02-05 19:03:21,952:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.353e+03, previous alpha=1.302e+03, with an active set of 29 regressors.
  warnings.warn(

2024-02-05 19:03:21,954:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=8.289e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,954:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=8.289e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,955:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=1.416e+02, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,955:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 101 iterations, alpha=8.102e+01, previous alpha=8.102e+01, with an active set of 80 regressors.
  warnings.warn(

2024-02-05 19:03:21,963:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 111 iterations, alpha=7.889e+01, previous alpha=5.351e+01, with an active set of 88 regressors.
  warnings.warn(

2024-02-05 19:03:21,966:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=4.274e+00, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,966:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.234e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,967:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 146 iterations, i.e. alpha=1.586e+00, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,968:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 147 iterations, i.e. alpha=1.586e+00, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:21,972:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 151 iterations, alpha=1.419e+00, previous alpha=1.419e+00, with an active set of 118 regressors.
  warnings.warn(

2024-02-05 19:03:21,980:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 140 iterations, alpha=2.414e+01, previous alpha=2.414e+01, with an active set of 95 regressors.
  warnings.warn(

2024-02-05 19:03:22,190:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.325e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:22,192:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.167e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:22,194:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.168e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:22,194:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=4.459e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:22,194:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.380e+02, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:22,195:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=9.583e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:22,195:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=9.092e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:22,195:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 89 iterations, alpha=8.936e+01, previous alpha=8.854e+01, with an active set of 78 regressors.
  warnings.warn(

2024-02-05 19:03:22,197:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.956e+02, with an active set of 57 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:22,203:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 129 iterations, alpha=4.342e+01, previous alpha=3.885e+01, with an active set of 96 regressors.
  warnings.warn(

2024-02-05 19:03:22,248:INFO:Calculating mean and std
2024-02-05 19:03:22,248:INFO:Creating metrics dataframe
2024-02-05 19:03:22,250:INFO:Finalizing model
2024-02-05 19:03:22,385:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning:

Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.856e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.


2024-02-05 19:03:22,388:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning:

Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=2.148e+02, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.


2024-02-05 19:03:22,390:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning:

Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.193e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.


2024-02-05 19:03:22,391:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning:

Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=5.964e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.


2024-02-05 19:03:22,392:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning:

Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 99 iterations, alpha=5.964e+01, previous alpha=5.964e+01, with an active set of 84 regressors.


2024-02-05 19:03:22,394:INFO:Uploading results into container
2024-02-05 19:03:22,395:INFO:_master_model_container: 48
2024-02-05 19:03:22,396:INFO:_display_container: 19
2024-02-05 19:03:22,396:INFO:LassoLars(random_state=123)
2024-02-05 19:03:22,396:INFO:create_model() successfully completed......................................
2024-02-05 19:03:22,536:INFO:Initializing create_model()
2024-02-05 19:03:22,536:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=OrthogonalMatchingPursuit(), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:22,536:INFO:Checking exceptions
2024-02-05 19:03:22,537:INFO:Importing libraries
2024-02-05 19:03:22,538:INFO:Copying training dataset
2024-02-05 19:03:22,544:INFO:Defining folds
2024-02-05 19:03:22,544:INFO:Declaring metric variables
2024-02-05 19:03:22,544:INFO:Importing untrained model
2024-02-05 19:03:22,544:INFO:Declaring custom model
2024-02-05 19:03:22,544:INFO:Orthogonal Matching Pursuit Imported successfully
2024-02-05 19:03:22,545:INFO:Starting cross validation
2024-02-05 19:03:22,546:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:23,131:INFO:Calculating mean and std
2024-02-05 19:03:23,131:INFO:Creating metrics dataframe
2024-02-05 19:03:23,133:INFO:Finalizing model
2024-02-05 19:03:23,264:INFO:Uploading results into container
2024-02-05 19:03:23,264:INFO:_master_model_container: 48
2024-02-05 19:03:23,264:INFO:_display_container: 20
2024-02-05 19:03:23,264:INFO:OrthogonalMatchingPursuit()
2024-02-05 19:03:23,265:INFO:create_model() successfully completed......................................
2024-02-05 19:03:23,405:INFO:Initializing create_model()
2024-02-05 19:03:23,405:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=BayesianRidge(), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:23,405:INFO:Checking exceptions
2024-02-05 19:03:23,408:INFO:Importing libraries
2024-02-05 19:03:23,408:INFO:Copying training dataset
2024-02-05 19:03:23,413:INFO:Defining folds
2024-02-05 19:03:23,413:INFO:Declaring metric variables
2024-02-05 19:03:23,413:INFO:Importing untrained model
2024-02-05 19:03:23,414:INFO:Declaring custom model
2024-02-05 19:03:23,414:INFO:Bayesian Ridge Imported successfully
2024-02-05 19:03:23,414:INFO:Starting cross validation
2024-02-05 19:03:23,416:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:24,015:INFO:Calculating mean and std
2024-02-05 19:03:24,015:INFO:Creating metrics dataframe
2024-02-05 19:03:24,017:INFO:Finalizing model
2024-02-05 19:03:24,173:INFO:Uploading results into container
2024-02-05 19:03:24,174:INFO:_master_model_container: 48
2024-02-05 19:03:24,174:INFO:_display_container: 21
2024-02-05 19:03:24,174:INFO:BayesianRidge()
2024-02-05 19:03:24,174:INFO:create_model() successfully completed......................................
2024-02-05 19:03:24,300:INFO:Initializing create_model()
2024-02-05 19:03:24,300:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=PassiveAggressiveRegressor(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:24,300:INFO:Checking exceptions
2024-02-05 19:03:24,302:INFO:Importing libraries
2024-02-05 19:03:24,302:INFO:Copying training dataset
2024-02-05 19:03:24,307:INFO:Defining folds
2024-02-05 19:03:24,307:INFO:Declaring metric variables
2024-02-05 19:03:24,307:INFO:Importing untrained model
2024-02-05 19:03:24,307:INFO:Declaring custom model
2024-02-05 19:03:24,307:INFO:Passive Aggressive Regressor Imported successfully
2024-02-05 19:03:24,308:INFO:Starting cross validation
2024-02-05 19:03:24,309:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:24,867:INFO:Calculating mean and std
2024-02-05 19:03:24,867:INFO:Creating metrics dataframe
2024-02-05 19:03:24,869:INFO:Finalizing model
2024-02-05 19:03:24,992:INFO:Uploading results into container
2024-02-05 19:03:24,993:INFO:_master_model_container: 48
2024-02-05 19:03:24,993:INFO:_display_container: 22
2024-02-05 19:03:24,993:INFO:PassiveAggressiveRegressor(random_state=123)
2024-02-05 19:03:24,993:INFO:create_model() successfully completed......................................
2024-02-05 19:03:25,119:INFO:Initializing create_model()
2024-02-05 19:03:25,119:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=HuberRegressor(), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:25,119:INFO:Checking exceptions
2024-02-05 19:03:25,120:INFO:Importing libraries
2024-02-05 19:03:25,121:INFO:Copying training dataset
2024-02-05 19:03:25,125:INFO:Defining folds
2024-02-05 19:03:25,125:INFO:Declaring metric variables
2024-02-05 19:03:25,125:INFO:Importing untrained model
2024-02-05 19:03:25,125:INFO:Declaring custom model
2024-02-05 19:03:25,125:INFO:Huber Regressor Imported successfully
2024-02-05 19:03:25,125:INFO:Starting cross validation
2024-02-05 19:03:25,127:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:25,447:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:25,460:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:25,497:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:25,525:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:25,532:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:25,538:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:25,539:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:25,578:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:25,754:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:25,754:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:25,810:INFO:Calculating mean and std
2024-02-05 19:03:25,810:INFO:Creating metrics dataframe
2024-02-05 19:03:25,811:INFO:Finalizing model
2024-02-05 19:03:26,014:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning:

lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html


2024-02-05 19:03:26,015:INFO:Uploading results into container
2024-02-05 19:03:26,016:INFO:_master_model_container: 48
2024-02-05 19:03:26,016:INFO:_display_container: 23
2024-02-05 19:03:26,016:INFO:HuberRegressor()
2024-02-05 19:03:26,016:INFO:create_model() successfully completed......................................
2024-02-05 19:03:26,144:INFO:Initializing create_model()
2024-02-05 19:03:26,144:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=KNeighborsRegressor(n_jobs=-1), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:26,144:INFO:Checking exceptions
2024-02-05 19:03:26,146:INFO:Importing libraries
2024-02-05 19:03:26,146:INFO:Copying training dataset
2024-02-05 19:03:26,150:INFO:Defining folds
2024-02-05 19:03:26,150:INFO:Declaring metric variables
2024-02-05 19:03:26,150:INFO:Importing untrained model
2024-02-05 19:03:26,150:INFO:Declaring custom model
2024-02-05 19:03:26,150:INFO:K Neighbors Regressor Imported successfully
2024-02-05 19:03:26,150:INFO:Starting cross validation
2024-02-05 19:03:26,151:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:26,807:INFO:Calculating mean and std
2024-02-05 19:03:26,807:INFO:Creating metrics dataframe
2024-02-05 19:03:26,809:INFO:Finalizing model
2024-02-05 19:03:26,928:INFO:Uploading results into container
2024-02-05 19:03:26,930:INFO:_master_model_container: 48
2024-02-05 19:03:26,930:INFO:_display_container: 24
2024-02-05 19:03:26,930:INFO:KNeighborsRegressor(n_jobs=-1)
2024-02-05 19:03:26,930:INFO:create_model() successfully completed......................................
2024-02-05 19:03:27,056:INFO:Initializing create_model()
2024-02-05 19:03:27,056:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=DecisionTreeRegressor(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:27,056:INFO:Checking exceptions
2024-02-05 19:03:27,058:INFO:Importing libraries
2024-02-05 19:03:27,058:INFO:Copying training dataset
2024-02-05 19:03:27,063:INFO:Defining folds
2024-02-05 19:03:27,063:INFO:Declaring metric variables
2024-02-05 19:03:27,063:INFO:Importing untrained model
2024-02-05 19:03:27,063:INFO:Declaring custom model
2024-02-05 19:03:27,064:INFO:Decision Tree Regressor Imported successfully
2024-02-05 19:03:27,064:INFO:Starting cross validation
2024-02-05 19:03:27,065:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:27,622:INFO:Calculating mean and std
2024-02-05 19:03:27,622:INFO:Creating metrics dataframe
2024-02-05 19:03:27,624:INFO:Finalizing model
2024-02-05 19:03:27,759:INFO:Uploading results into container
2024-02-05 19:03:27,760:INFO:_master_model_container: 48
2024-02-05 19:03:27,760:INFO:_display_container: 25
2024-02-05 19:03:27,760:INFO:DecisionTreeRegressor(random_state=123)
2024-02-05 19:03:27,760:INFO:create_model() successfully completed......................................
2024-02-05 19:03:27,886:INFO:Initializing create_model()
2024-02-05 19:03:27,886:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=RandomForestRegressor(n_jobs=-1, random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:27,886:INFO:Checking exceptions
2024-02-05 19:03:27,887:INFO:Importing libraries
2024-02-05 19:03:27,887:INFO:Copying training dataset
2024-02-05 19:03:27,892:INFO:Defining folds
2024-02-05 19:03:27,892:INFO:Declaring metric variables
2024-02-05 19:03:27,892:INFO:Importing untrained model
2024-02-05 19:03:27,892:INFO:Declaring custom model
2024-02-05 19:03:27,892:INFO:Random Forest Regressor Imported successfully
2024-02-05 19:03:27,893:INFO:Starting cross validation
2024-02-05 19:03:27,894:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:31,020:INFO:Calculating mean and std
2024-02-05 19:03:31,020:INFO:Creating metrics dataframe
2024-02-05 19:03:31,023:INFO:Finalizing model
2024-02-05 19:03:31,483:INFO:Uploading results into container
2024-02-05 19:03:31,484:INFO:_master_model_container: 48
2024-02-05 19:03:31,484:INFO:_display_container: 26
2024-02-05 19:03:31,485:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2024-02-05 19:03:31,485:INFO:create_model() successfully completed......................................
2024-02-05 19:03:31,610:INFO:Initializing create_model()
2024-02-05 19:03:31,611:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:31,611:INFO:Checking exceptions
2024-02-05 19:03:31,613:INFO:Importing libraries
2024-02-05 19:03:31,613:INFO:Copying training dataset
2024-02-05 19:03:31,619:INFO:Defining folds
2024-02-05 19:03:31,619:INFO:Declaring metric variables
2024-02-05 19:03:31,619:INFO:Importing untrained model
2024-02-05 19:03:31,619:INFO:Declaring custom model
2024-02-05 19:03:31,619:INFO:Extra Trees Regressor Imported successfully
2024-02-05 19:03:31,619:INFO:Starting cross validation
2024-02-05 19:03:31,621:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:33,806:INFO:Calculating mean and std
2024-02-05 19:03:33,807:INFO:Creating metrics dataframe
2024-02-05 19:03:33,809:INFO:Finalizing model
2024-02-05 19:03:34,162:INFO:Uploading results into container
2024-02-05 19:03:34,163:INFO:_master_model_container: 48
2024-02-05 19:03:34,163:INFO:_display_container: 27
2024-02-05 19:03:34,163:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2024-02-05 19:03:34,163:INFO:create_model() successfully completed......................................
2024-02-05 19:03:34,292:INFO:Initializing create_model()
2024-02-05 19:03:34,292:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=AdaBoostRegressor(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:34,292:INFO:Checking exceptions
2024-02-05 19:03:34,294:INFO:Importing libraries
2024-02-05 19:03:34,294:INFO:Copying training dataset
2024-02-05 19:03:34,301:INFO:Defining folds
2024-02-05 19:03:34,301:INFO:Declaring metric variables
2024-02-05 19:03:34,301:INFO:Importing untrained model
2024-02-05 19:03:34,301:INFO:Declaring custom model
2024-02-05 19:03:34,301:INFO:AdaBoost Regressor Imported successfully
2024-02-05 19:03:34,301:INFO:Starting cross validation
2024-02-05 19:03:34,303:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:35,324:INFO:Calculating mean and std
2024-02-05 19:03:35,324:INFO:Creating metrics dataframe
2024-02-05 19:03:35,324:INFO:Finalizing model
2024-02-05 19:03:35,683:INFO:Uploading results into container
2024-02-05 19:03:35,684:INFO:_master_model_container: 48
2024-02-05 19:03:35,684:INFO:_display_container: 28
2024-02-05 19:03:35,684:INFO:AdaBoostRegressor(random_state=123)
2024-02-05 19:03:35,684:INFO:create_model() successfully completed......................................
2024-02-05 19:03:35,812:INFO:Initializing create_model()
2024-02-05 19:03:35,812:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=GradientBoostingRegressor(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:35,812:INFO:Checking exceptions
2024-02-05 19:03:35,814:INFO:Importing libraries
2024-02-05 19:03:35,814:INFO:Copying training dataset
2024-02-05 19:03:35,820:INFO:Defining folds
2024-02-05 19:03:35,820:INFO:Declaring metric variables
2024-02-05 19:03:35,820:INFO:Importing untrained model
2024-02-05 19:03:35,820:INFO:Declaring custom model
2024-02-05 19:03:35,821:INFO:Gradient Boosting Regressor Imported successfully
2024-02-05 19:03:35,821:INFO:Starting cross validation
2024-02-05 19:03:35,822:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:37,373:INFO:Calculating mean and std
2024-02-05 19:03:37,373:INFO:Creating metrics dataframe
2024-02-05 19:03:37,375:INFO:Finalizing model
2024-02-05 19:03:37,910:INFO:Uploading results into container
2024-02-05 19:03:37,910:INFO:_master_model_container: 48
2024-02-05 19:03:37,910:INFO:_display_container: 29
2024-02-05 19:03:37,911:INFO:GradientBoostingRegressor(random_state=123)
2024-02-05 19:03:37,911:INFO:create_model() successfully completed......................................
2024-02-05 19:03:38,040:INFO:Initializing create_model()
2024-02-05 19:03:38,040:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:38,040:INFO:Checking exceptions
2024-02-05 19:03:38,043:INFO:Importing libraries
2024-02-05 19:03:38,043:INFO:Copying training dataset
2024-02-05 19:03:38,047:INFO:Defining folds
2024-02-05 19:03:38,048:INFO:Declaring metric variables
2024-02-05 19:03:38,048:INFO:Importing untrained model
2024-02-05 19:03:38,048:INFO:Declaring custom model
2024-02-05 19:03:38,048:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-05 19:03:38,048:INFO:Starting cross validation
2024-02-05 19:03:38,050:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:39,453:INFO:Calculating mean and std
2024-02-05 19:03:39,453:INFO:Creating metrics dataframe
2024-02-05 19:03:39,455:INFO:Finalizing model
2024-02-05 19:03:39,598:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-02-05 19:03:39,600:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000715 seconds.
2024-02-05 19:03:39,600:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-05 19:03:39,600:INFO:[LightGBM] [Info] Total Bins 2881
2024-02-05 19:03:39,600:INFO:[LightGBM] [Info] Number of data points in the train set: 1019, number of used features: 93
2024-02-05 19:03:39,602:INFO:[LightGBM] [Info] Start training from score 180679.625123
2024-02-05 19:03:39,676:INFO:Uploading results into container
2024-02-05 19:03:39,677:INFO:_master_model_container: 48
2024-02-05 19:03:39,677:INFO:_display_container: 30
2024-02-05 19:03:39,677:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-05 19:03:39,677:INFO:create_model() successfully completed......................................
2024-02-05 19:03:39,823:INFO:Initializing create_model()
2024-02-05 19:03:39,823:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=DummyRegressor(), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:39,823:INFO:Checking exceptions
2024-02-05 19:03:39,824:INFO:Importing libraries
2024-02-05 19:03:39,824:INFO:Copying training dataset
2024-02-05 19:03:39,829:INFO:Defining folds
2024-02-05 19:03:39,829:INFO:Declaring metric variables
2024-02-05 19:03:39,829:INFO:Importing untrained model
2024-02-05 19:03:39,829:INFO:Declaring custom model
2024-02-05 19:03:39,829:INFO:Dummy Regressor Imported successfully
2024-02-05 19:03:39,830:INFO:Starting cross validation
2024-02-05 19:03:39,831:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:40,348:INFO:Calculating mean and std
2024-02-05 19:03:40,348:INFO:Creating metrics dataframe
2024-02-05 19:03:40,351:INFO:Finalizing model
2024-02-05 19:03:40,473:INFO:Uploading results into container
2024-02-05 19:03:40,473:INFO:_master_model_container: 48
2024-02-05 19:03:40,473:INFO:_display_container: 31
2024-02-05 19:03:40,474:INFO:DummyRegressor()
2024-02-05 19:03:40,474:INFO:create_model() successfully completed......................................
2024-02-05 19:03:40,627:INFO:Initializing create_model()
2024-02-05 19:03:40,627:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:40,627:INFO:Checking exceptions
2024-02-05 19:03:40,629:INFO:Importing libraries
2024-02-05 19:03:40,629:INFO:Copying training dataset
2024-02-05 19:03:40,633:INFO:Defining folds
2024-02-05 19:03:40,634:INFO:Declaring metric variables
2024-02-05 19:03:40,634:INFO:Importing untrained model
2024-02-05 19:03:40,634:INFO:Declaring custom model
2024-02-05 19:03:40,634:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-05 19:03:40,634:INFO:Starting cross validation
2024-02-05 19:03:40,637:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:42,084:INFO:Calculating mean and std
2024-02-05 19:03:42,084:INFO:Creating metrics dataframe
2024-02-05 19:03:42,086:INFO:Finalizing model
2024-02-05 19:03:42,231:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-02-05 19:03:42,233:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000768 seconds.
2024-02-05 19:03:42,233:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-05 19:03:42,233:INFO:[LightGBM] [Info] Total Bins 2881
2024-02-05 19:03:42,233:INFO:[LightGBM] [Info] Number of data points in the train set: 1019, number of used features: 93
2024-02-05 19:03:42,234:INFO:[LightGBM] [Info] Start training from score 180679.625123
2024-02-05 19:03:42,303:INFO:Uploading results into container
2024-02-05 19:03:42,305:INFO:_master_model_container: 48
2024-02-05 19:03:42,305:INFO:_display_container: 32
2024-02-05 19:03:42,305:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-05 19:03:42,305:INFO:create_model() successfully completed......................................
2024-02-05 19:03:42,452:INFO:Initializing create_model()
2024-02-05 19:03:42,452:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=GradientBoostingRegressor(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:42,453:INFO:Checking exceptions
2024-02-05 19:03:42,454:INFO:Importing libraries
2024-02-05 19:03:42,454:INFO:Copying training dataset
2024-02-05 19:03:42,459:INFO:Defining folds
2024-02-05 19:03:42,459:INFO:Declaring metric variables
2024-02-05 19:03:42,459:INFO:Importing untrained model
2024-02-05 19:03:42,459:INFO:Declaring custom model
2024-02-05 19:03:42,460:INFO:Gradient Boosting Regressor Imported successfully
2024-02-05 19:03:42,460:INFO:Starting cross validation
2024-02-05 19:03:42,461:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:44,036:INFO:Calculating mean and std
2024-02-05 19:03:44,036:INFO:Creating metrics dataframe
2024-02-05 19:03:44,038:INFO:Finalizing model
2024-02-05 19:03:44,574:INFO:Uploading results into container
2024-02-05 19:03:44,574:INFO:_master_model_container: 48
2024-02-05 19:03:44,574:INFO:_display_container: 33
2024-02-05 19:03:44,574:INFO:GradientBoostingRegressor(random_state=123)
2024-02-05 19:03:44,574:INFO:create_model() successfully completed......................................
2024-02-05 19:03:44,706:INFO:Initializing create_model()
2024-02-05 19:03:44,706:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=BayesianRidge(), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:44,706:INFO:Checking exceptions
2024-02-05 19:03:44,708:INFO:Importing libraries
2024-02-05 19:03:44,708:INFO:Copying training dataset
2024-02-05 19:03:44,713:INFO:Defining folds
2024-02-05 19:03:44,713:INFO:Declaring metric variables
2024-02-05 19:03:44,714:INFO:Importing untrained model
2024-02-05 19:03:44,714:INFO:Declaring custom model
2024-02-05 19:03:44,714:INFO:Bayesian Ridge Imported successfully
2024-02-05 19:03:44,714:INFO:Starting cross validation
2024-02-05 19:03:44,716:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:45,270:INFO:Calculating mean and std
2024-02-05 19:03:45,270:INFO:Creating metrics dataframe
2024-02-05 19:03:45,272:INFO:Finalizing model
2024-02-05 19:03:45,424:INFO:Uploading results into container
2024-02-05 19:03:45,425:INFO:_master_model_container: 48
2024-02-05 19:03:45,425:INFO:_display_container: 34
2024-02-05 19:03:45,425:INFO:BayesianRidge()
2024-02-05 19:03:45,425:INFO:create_model() successfully completed......................................
2024-02-05 19:03:45,554:INFO:Initializing create_model()
2024-02-05 19:03:45,554:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=LinearRegression(n_jobs=-1), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:45,554:INFO:Checking exceptions
2024-02-05 19:03:45,556:INFO:Importing libraries
2024-02-05 19:03:45,556:INFO:Copying training dataset
2024-02-05 19:03:45,562:INFO:Defining folds
2024-02-05 19:03:45,562:INFO:Declaring metric variables
2024-02-05 19:03:45,562:INFO:Importing untrained model
2024-02-05 19:03:45,562:INFO:Declaring custom model
2024-02-05 19:03:45,563:INFO:Linear Regression Imported successfully
2024-02-05 19:03:45,563:INFO:Starting cross validation
2024-02-05 19:03:45,564:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:46,107:INFO:Calculating mean and std
2024-02-05 19:03:46,107:INFO:Creating metrics dataframe
2024-02-05 19:03:46,108:INFO:Finalizing model
2024-02-05 19:03:46,250:INFO:Uploading results into container
2024-02-05 19:03:46,250:INFO:_master_model_container: 48
2024-02-05 19:03:46,250:INFO:_display_container: 35
2024-02-05 19:03:46,250:INFO:LinearRegression(n_jobs=-1)
2024-02-05 19:03:46,251:INFO:create_model() successfully completed......................................
2024-02-05 19:03:46,376:INFO:Initializing create_model()
2024-02-05 19:03:46,376:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=Lasso(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:46,376:INFO:Checking exceptions
2024-02-05 19:03:46,378:INFO:Importing libraries
2024-02-05 19:03:46,378:INFO:Copying training dataset
2024-02-05 19:03:46,383:INFO:Defining folds
2024-02-05 19:03:46,383:INFO:Declaring metric variables
2024-02-05 19:03:46,383:INFO:Importing untrained model
2024-02-05 19:03:46,383:INFO:Declaring custom model
2024-02-05 19:03:46,383:INFO:Lasso Regression Imported successfully
2024-02-05 19:03:46,384:INFO:Starting cross validation
2024-02-05 19:03:46,385:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:46,705:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.631e+11, tolerance: 5.696e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:46,722:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.799e+11, tolerance: 5.738e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:46,724:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.241e+11, tolerance: 5.445e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:46,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.569e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:46,741:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.563e+11, tolerance: 5.378e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:46,743:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.692e+11, tolerance: 5.599e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:46,752:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.375e+11, tolerance: 5.336e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:46,989:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.674e+11, tolerance: 5.647e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:46,999:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.671e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:47,049:INFO:Calculating mean and std
2024-02-05 19:03:47,049:INFO:Creating metrics dataframe
2024-02-05 19:03:47,050:INFO:Finalizing model
2024-02-05 19:03:47,217:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning:

Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.963e+11, tolerance: 6.216e+08


2024-02-05 19:03:47,219:INFO:Uploading results into container
2024-02-05 19:03:47,220:INFO:_master_model_container: 48
2024-02-05 19:03:47,220:INFO:_display_container: 36
2024-02-05 19:03:47,220:INFO:Lasso(random_state=123)
2024-02-05 19:03:47,220:INFO:create_model() successfully completed......................................
2024-02-05 19:03:47,348:INFO:Initializing create_model()
2024-02-05 19:03:47,349:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=Ridge(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:47,349:INFO:Checking exceptions
2024-02-05 19:03:47,350:INFO:Importing libraries
2024-02-05 19:03:47,350:INFO:Copying training dataset
2024-02-05 19:03:47,354:INFO:Defining folds
2024-02-05 19:03:47,355:INFO:Declaring metric variables
2024-02-05 19:03:47,355:INFO:Importing untrained model
2024-02-05 19:03:47,355:INFO:Declaring custom model
2024-02-05 19:03:47,355:INFO:Ridge Regression Imported successfully
2024-02-05 19:03:47,355:INFO:Starting cross validation
2024-02-05 19:03:47,356:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:47,873:INFO:Calculating mean and std
2024-02-05 19:03:47,873:INFO:Creating metrics dataframe
2024-02-05 19:03:47,874:INFO:Finalizing model
2024-02-05 19:03:48,000:INFO:Uploading results into container
2024-02-05 19:03:48,001:INFO:_master_model_container: 48
2024-02-05 19:03:48,001:INFO:_display_container: 37
2024-02-05 19:03:48,001:INFO:Ridge(random_state=123)
2024-02-05 19:03:48,002:INFO:create_model() successfully completed......................................
2024-02-05 19:03:48,130:INFO:Initializing create_model()
2024-02-05 19:03:48,130:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=ElasticNet(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:48,130:INFO:Checking exceptions
2024-02-05 19:03:48,132:INFO:Importing libraries
2024-02-05 19:03:48,132:INFO:Copying training dataset
2024-02-05 19:03:48,138:INFO:Defining folds
2024-02-05 19:03:48,138:INFO:Declaring metric variables
2024-02-05 19:03:48,138:INFO:Importing untrained model
2024-02-05 19:03:48,138:INFO:Declaring custom model
2024-02-05 19:03:48,138:INFO:Elastic Net Imported successfully
2024-02-05 19:03:48,139:INFO:Starting cross validation
2024-02-05 19:03:48,140:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:48,502:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.761e+11, tolerance: 5.445e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:48,503:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.889e+11, tolerance: 5.696e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:48,515:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.013e+11, tolerance: 5.738e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:48,516:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.602e+11, tolerance: 5.336e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:48,520:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.863e+11, tolerance: 5.599e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:48,520:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.721e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:48,526:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+11, tolerance: 5.378e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:48,776:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.964e+11, tolerance: 5.647e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:48,786:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.975e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-05 19:03:48,831:INFO:Calculating mean and std
2024-02-05 19:03:48,831:INFO:Creating metrics dataframe
2024-02-05 19:03:48,833:INFO:Finalizing model
2024-02-05 19:03:49,036:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning:

Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.283e+11, tolerance: 6.216e+08


2024-02-05 19:03:49,038:INFO:Uploading results into container
2024-02-05 19:03:49,038:INFO:_master_model_container: 48
2024-02-05 19:03:49,038:INFO:_display_container: 38
2024-02-05 19:03:49,040:INFO:ElasticNet(random_state=123)
2024-02-05 19:03:49,040:INFO:create_model() successfully completed......................................
2024-02-05 19:03:49,168:INFO:Initializing create_model()
2024-02-05 19:03:49,168:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=Lars(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:49,169:INFO:Checking exceptions
2024-02-05 19:03:49,171:INFO:Importing libraries
2024-02-05 19:03:49,171:INFO:Copying training dataset
2024-02-05 19:03:49,176:INFO:Defining folds
2024-02-05 19:03:49,176:INFO:Declaring metric variables
2024-02-05 19:03:49,177:INFO:Importing untrained model
2024-02-05 19:03:49,177:INFO:Declaring custom model
2024-02-05 19:03:49,177:INFO:Least Angle Regression Imported successfully
2024-02-05 19:03:49,178:INFO:Starting cross validation
2024-02-05 19:03:49,179:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:49,472:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=3.936e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,474:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.412e+02, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,478:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.157e+03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,480:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.555e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,480:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.731e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,482:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.519e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,486:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=1.562e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,489:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.496e+09, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,489:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=5.167e+07, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,489:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=3.422e+07, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,490:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=2.687e+07, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,490:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=2.606e+07, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,490:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=2.275e+07, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,491:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.908e+07, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,491:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=1.562e+07, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,491:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.264e+07, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,492:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.442e+07, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,492:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.718e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,492:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.173e+07, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,493:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.137e+07, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,493:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.059e+07, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,493:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=1.029e+07, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,494:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=9.833e+06, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,494:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=9.103e+06, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,494:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.851e+02, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,495:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=3.621e+02, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,495:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=8.950e+06, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,495:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=7.986e+06, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,495:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=7.476e+06, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,495:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=7.475e+06, with an active set of 91 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,496:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.243e+06, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,496:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.242e+06, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,497:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=6.703e+06, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,497:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=5.425e+06, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,498:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=5.332e+06, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,498:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=4.831e+06, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,498:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=4.529e+06, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,498:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.149e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=3.694e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=3.221e+06, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=2.801e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,501:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 115 iterations, i.e. alpha=2.370e+06, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,504:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.911e+02, with an active set of 99 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,505:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.244e+12, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,505:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=1.750e+11, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,506:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.543e+11, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,506:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.062e+11, with an active set of 107 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,506:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.027e+11, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,507:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=7.904e+10, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,507:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=7.308e+10, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,507:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=6.501e+10, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,508:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=5.857e+10, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,508:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=5.713e+10, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,509:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=5.810e+10, with an active set of 113 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,509:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=5.809e+10, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,509:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=3.100e+10, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,510:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=1.924e+05, with an active set of 109 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,511:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 160 iterations, i.e. alpha=1.780e+05, with an active set of 112 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,513:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=3.774e+05, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,513:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=3.759e+05, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,513:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=8.153e+04, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,515:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=9.162e+13, with an active set of 131 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,515:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=7.581e+13, with an active set of 131 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,526:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.051e+02, with an active set of 72 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,538:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=2.116e+02, with an active set of 116 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,538:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=1.924e+02, with an active set of 116 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,541:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 146 iterations, i.e. alpha=1.674e+02, with an active set of 121 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,542:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=1.569e+02, with an active set of 124 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,544:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=1.388e+03, with an active set of 128 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,544:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=1.232e+03, with an active set of 128 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,544:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=1.900e+02, with an active set of 128 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,544:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=7.168e+00, with an active set of 128 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,737:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=3.745e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,743:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.029e+02, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,751:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.739e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=2.546e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,755:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=5.669e+03, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,755:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=2.536e+03, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,755:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=1.472e+02, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,755:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.190e+02, with an active set of 73 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,756:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=6.923e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,757:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=5.772e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,758:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.716e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,758:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=3.202e+01, with an active set of 98 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,759:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.832e+01, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,759:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.695e+01, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,759:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=1.943e+01, with an active set of 107 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,759:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=1.431e+01, with an active set of 108 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,760:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=1.773e+01, with an active set of 111 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,761:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=1.219e+01, with an active set of 116 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,761:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=1.102e+01, with an active set of 116 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,762:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 144 iterations, i.e. alpha=4.280e+01, with an active set of 125 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,763:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=3.768e+02, with an active set of 128 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,764:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=1.298e+04, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,764:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=7.540e+01, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:49,812:INFO:Calculating mean and std
2024-02-05 19:03:49,812:INFO:Creating metrics dataframe
2024-02-05 19:03:49,814:INFO:Finalizing model
2024-02-05 19:03:49,938:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning:

Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.375e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.


2024-02-05 19:03:49,950:INFO:Uploading results into container
2024-02-05 19:03:49,951:INFO:_master_model_container: 48
2024-02-05 19:03:49,951:INFO:_display_container: 39
2024-02-05 19:03:49,952:INFO:Lars(random_state=123)
2024-02-05 19:03:49,952:INFO:create_model() successfully completed......................................
2024-02-05 19:03:50,081:INFO:Initializing create_model()
2024-02-05 19:03:50,082:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=LassoLars(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:50,082:INFO:Checking exceptions
2024-02-05 19:03:50,084:INFO:Importing libraries
2024-02-05 19:03:50,084:INFO:Copying training dataset
2024-02-05 19:03:50,089:INFO:Defining folds
2024-02-05 19:03:50,089:INFO:Declaring metric variables
2024-02-05 19:03:50,090:INFO:Importing untrained model
2024-02-05 19:03:50,090:INFO:Declaring custom model
2024-02-05 19:03:50,090:INFO:Lasso Least Angle Regression Imported successfully
2024-02-05 19:03:50,090:INFO:Starting cross validation
2024-02-05 19:03:50,092:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:50,345:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=3.340e+02, with an active set of 46 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,350:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.603e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,352:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 86 iterations, alpha=1.479e+02, previous alpha=1.479e+02, with an active set of 71 regressors.
  warnings.warn(

2024-02-05 19:03:50,362:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=7.153e+03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,363:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.274e+03, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,364:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.955e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,365:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.353e+03, previous alpha=1.302e+03, with an active set of 29 regressors.
  warnings.warn(

2024-02-05 19:03:50,368:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 70 iterations, alpha=2.179e+02, previous alpha=2.179e+02, with an active set of 57 regressors.
  warnings.warn(

2024-02-05 19:03:50,380:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=4.005e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,381:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=3.697e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,383:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.087e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,385:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.234e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,390:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.426e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,391:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=8.289e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,392:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=8.289e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,392:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=4.274e+00, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,392:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.522e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,392:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 101 iterations, alpha=8.102e+01, previous alpha=8.102e+01, with an active set of 80 regressors.
  warnings.warn(

2024-02-05 19:03:50,394:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 146 iterations, i.e. alpha=1.586e+00, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,394:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 147 iterations, i.e. alpha=1.586e+00, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,394:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.795e+02, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,396:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 151 iterations, alpha=1.419e+00, previous alpha=1.419e+00, with an active set of 118 regressors.
  warnings.warn(

2024-02-05 19:03:50,396:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 140 iterations, alpha=2.414e+01, previous alpha=2.414e+01, with an active set of 95 regressors.
  warnings.warn(

2024-02-05 19:03:50,397:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=4.102e+02, with an active set of 45 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,399:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=6.685e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,399:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=6.615e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,402:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 103 iterations, alpha=5.927e+01, previous alpha=5.849e+01, with an active set of 82 regressors.
  warnings.warn(

2024-02-05 19:03:50,402:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=1.416e+02, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,410:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 111 iterations, alpha=7.889e+01, previous alpha=5.351e+01, with an active set of 88 regressors.
  warnings.warn(

2024-02-05 19:03:50,588:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.325e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,589:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.167e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,592:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.380e+02, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,592:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=9.583e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,592:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=9.092e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,593:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 89 iterations, alpha=8.936e+01, previous alpha=8.854e+01, with an active set of 78 regressors.
  warnings.warn(

2024-02-05 19:03:50,594:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.168e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,595:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=4.459e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,597:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.956e+02, with an active set of 57 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-05 19:03:50,601:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 129 iterations, alpha=4.342e+01, previous alpha=3.885e+01, with an active set of 96 regressors.
  warnings.warn(

2024-02-05 19:03:50,655:INFO:Calculating mean and std
2024-02-05 19:03:50,655:INFO:Creating metrics dataframe
2024-02-05 19:03:50,657:INFO:Finalizing model
2024-02-05 19:03:50,782:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning:

Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=3.856e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.


2024-02-05 19:03:50,783:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning:

Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=2.148e+02, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.


2024-02-05 19:03:50,786:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning:

Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.193e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.


2024-02-05 19:03:50,787:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning:

Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=5.964e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.


2024-02-05 19:03:50,788:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning:

Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 99 iterations, alpha=5.964e+01, previous alpha=5.964e+01, with an active set of 84 regressors.


2024-02-05 19:03:50,790:INFO:Uploading results into container
2024-02-05 19:03:50,791:INFO:_master_model_container: 48
2024-02-05 19:03:50,791:INFO:_display_container: 40
2024-02-05 19:03:50,791:INFO:LassoLars(random_state=123)
2024-02-05 19:03:50,791:INFO:create_model() successfully completed......................................
2024-02-05 19:03:50,918:INFO:Initializing create_model()
2024-02-05 19:03:50,918:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=OrthogonalMatchingPursuit(), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:50,918:INFO:Checking exceptions
2024-02-05 19:03:50,920:INFO:Importing libraries
2024-02-05 19:03:50,920:INFO:Copying training dataset
2024-02-05 19:03:50,924:INFO:Defining folds
2024-02-05 19:03:50,924:INFO:Declaring metric variables
2024-02-05 19:03:50,924:INFO:Importing untrained model
2024-02-05 19:03:50,924:INFO:Declaring custom model
2024-02-05 19:03:50,925:INFO:Orthogonal Matching Pursuit Imported successfully
2024-02-05 19:03:50,925:INFO:Starting cross validation
2024-02-05 19:03:50,926:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:51,458:INFO:Calculating mean and std
2024-02-05 19:03:51,458:INFO:Creating metrics dataframe
2024-02-05 19:03:51,460:INFO:Finalizing model
2024-02-05 19:03:51,582:INFO:Uploading results into container
2024-02-05 19:03:51,583:INFO:_master_model_container: 48
2024-02-05 19:03:51,583:INFO:_display_container: 41
2024-02-05 19:03:51,584:INFO:OrthogonalMatchingPursuit()
2024-02-05 19:03:51,584:INFO:create_model() successfully completed......................................
2024-02-05 19:03:51,712:INFO:Initializing create_model()
2024-02-05 19:03:51,713:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=BayesianRidge(), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:51,713:INFO:Checking exceptions
2024-02-05 19:03:51,715:INFO:Importing libraries
2024-02-05 19:03:51,715:INFO:Copying training dataset
2024-02-05 19:03:51,719:INFO:Defining folds
2024-02-05 19:03:51,719:INFO:Declaring metric variables
2024-02-05 19:03:51,719:INFO:Importing untrained model
2024-02-05 19:03:51,719:INFO:Declaring custom model
2024-02-05 19:03:51,720:INFO:Bayesian Ridge Imported successfully
2024-02-05 19:03:51,720:INFO:Starting cross validation
2024-02-05 19:03:51,722:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:52,270:INFO:Calculating mean and std
2024-02-05 19:03:52,270:INFO:Creating metrics dataframe
2024-02-05 19:03:52,272:INFO:Finalizing model
2024-02-05 19:03:52,429:INFO:Uploading results into container
2024-02-05 19:03:52,429:INFO:_master_model_container: 48
2024-02-05 19:03:52,429:INFO:_display_container: 42
2024-02-05 19:03:52,429:INFO:BayesianRidge()
2024-02-05 19:03:52,429:INFO:create_model() successfully completed......................................
2024-02-05 19:03:52,559:INFO:Initializing create_model()
2024-02-05 19:03:52,560:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=PassiveAggressiveRegressor(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:52,560:INFO:Checking exceptions
2024-02-05 19:03:52,562:INFO:Importing libraries
2024-02-05 19:03:52,562:INFO:Copying training dataset
2024-02-05 19:03:52,568:INFO:Defining folds
2024-02-05 19:03:52,568:INFO:Declaring metric variables
2024-02-05 19:03:52,569:INFO:Importing untrained model
2024-02-05 19:03:52,569:INFO:Declaring custom model
2024-02-05 19:03:52,569:INFO:Passive Aggressive Regressor Imported successfully
2024-02-05 19:03:52,569:INFO:Starting cross validation
2024-02-05 19:03:52,571:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:53,114:INFO:Calculating mean and std
2024-02-05 19:03:53,114:INFO:Creating metrics dataframe
2024-02-05 19:03:53,116:INFO:Finalizing model
2024-02-05 19:03:53,239:INFO:Uploading results into container
2024-02-05 19:03:53,240:INFO:_master_model_container: 48
2024-02-05 19:03:53,240:INFO:_display_container: 43
2024-02-05 19:03:53,240:INFO:PassiveAggressiveRegressor(random_state=123)
2024-02-05 19:03:53,240:INFO:create_model() successfully completed......................................
2024-02-05 19:03:53,369:INFO:Initializing create_model()
2024-02-05 19:03:53,370:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=HuberRegressor(), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:53,370:INFO:Checking exceptions
2024-02-05 19:03:53,371:INFO:Importing libraries
2024-02-05 19:03:53,371:INFO:Copying training dataset
2024-02-05 19:03:53,377:INFO:Defining folds
2024-02-05 19:03:53,377:INFO:Declaring metric variables
2024-02-05 19:03:53,377:INFO:Importing untrained model
2024-02-05 19:03:53,377:INFO:Declaring custom model
2024-02-05 19:03:53,377:INFO:Huber Regressor Imported successfully
2024-02-05 19:03:53,378:INFO:Starting cross validation
2024-02-05 19:03:53,379:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:53,730:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:53,790:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:53,793:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:53,795:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:53,801:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:53,803:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:53,817:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:53,823:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:54,043:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:54,044:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-05 19:03:54,099:INFO:Calculating mean and std
2024-02-05 19:03:54,099:INFO:Creating metrics dataframe
2024-02-05 19:03:54,101:INFO:Finalizing model
2024-02-05 19:03:54,305:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning:

lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html


2024-02-05 19:03:54,307:INFO:Uploading results into container
2024-02-05 19:03:54,307:INFO:_master_model_container: 48
2024-02-05 19:03:54,307:INFO:_display_container: 44
2024-02-05 19:03:54,308:INFO:HuberRegressor()
2024-02-05 19:03:54,308:INFO:create_model() successfully completed......................................
2024-02-05 19:03:54,441:INFO:Initializing create_model()
2024-02-05 19:03:54,442:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=KNeighborsRegressor(n_jobs=-1), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:54,442:INFO:Checking exceptions
2024-02-05 19:03:54,444:INFO:Importing libraries
2024-02-05 19:03:54,444:INFO:Copying training dataset
2024-02-05 19:03:54,455:INFO:Defining folds
2024-02-05 19:03:54,455:INFO:Declaring metric variables
2024-02-05 19:03:54,455:INFO:Importing untrained model
2024-02-05 19:03:54,455:INFO:Declaring custom model
2024-02-05 19:03:54,456:INFO:K Neighbors Regressor Imported successfully
2024-02-05 19:03:54,456:INFO:Starting cross validation
2024-02-05 19:03:54,484:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:55,017:INFO:Calculating mean and std
2024-02-05 19:03:55,017:INFO:Creating metrics dataframe
2024-02-05 19:03:55,019:INFO:Finalizing model
2024-02-05 19:03:55,141:INFO:Uploading results into container
2024-02-05 19:03:55,142:INFO:_master_model_container: 48
2024-02-05 19:03:55,142:INFO:_display_container: 45
2024-02-05 19:03:55,142:INFO:KNeighborsRegressor(n_jobs=-1)
2024-02-05 19:03:55,142:INFO:create_model() successfully completed......................................
2024-02-05 19:03:55,270:INFO:Initializing create_model()
2024-02-05 19:03:55,270:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=DecisionTreeRegressor(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:55,270:INFO:Checking exceptions
2024-02-05 19:03:55,273:INFO:Importing libraries
2024-02-05 19:03:55,273:INFO:Copying training dataset
2024-02-05 19:03:55,278:INFO:Defining folds
2024-02-05 19:03:55,278:INFO:Declaring metric variables
2024-02-05 19:03:55,278:INFO:Importing untrained model
2024-02-05 19:03:55,278:INFO:Declaring custom model
2024-02-05 19:03:55,279:INFO:Decision Tree Regressor Imported successfully
2024-02-05 19:03:55,279:INFO:Starting cross validation
2024-02-05 19:03:55,280:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:55,825:INFO:Calculating mean and std
2024-02-05 19:03:55,825:INFO:Creating metrics dataframe
2024-02-05 19:03:55,826:INFO:Finalizing model
2024-02-05 19:03:55,961:INFO:Uploading results into container
2024-02-05 19:03:55,961:INFO:_master_model_container: 48
2024-02-05 19:03:55,962:INFO:_display_container: 46
2024-02-05 19:03:55,962:INFO:DecisionTreeRegressor(random_state=123)
2024-02-05 19:03:55,962:INFO:create_model() successfully completed......................................
2024-02-05 19:03:56,093:INFO:Initializing create_model()
2024-02-05 19:03:56,093:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=RandomForestRegressor(n_jobs=-1, random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:56,093:INFO:Checking exceptions
2024-02-05 19:03:56,095:INFO:Importing libraries
2024-02-05 19:03:56,095:INFO:Copying training dataset
2024-02-05 19:03:56,100:INFO:Defining folds
2024-02-05 19:03:56,100:INFO:Declaring metric variables
2024-02-05 19:03:56,100:INFO:Importing untrained model
2024-02-05 19:03:56,100:INFO:Declaring custom model
2024-02-05 19:03:56,101:INFO:Random Forest Regressor Imported successfully
2024-02-05 19:03:56,101:INFO:Starting cross validation
2024-02-05 19:03:56,102:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:03:59,168:INFO:Calculating mean and std
2024-02-05 19:03:59,169:INFO:Creating metrics dataframe
2024-02-05 19:03:59,172:INFO:Finalizing model
2024-02-05 19:03:59,659:INFO:Uploading results into container
2024-02-05 19:03:59,660:INFO:_master_model_container: 48
2024-02-05 19:03:59,660:INFO:_display_container: 47
2024-02-05 19:03:59,660:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2024-02-05 19:03:59,660:INFO:create_model() successfully completed......................................
2024-02-05 19:03:59,789:INFO:Initializing create_model()
2024-02-05 19:03:59,789:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:03:59,789:INFO:Checking exceptions
2024-02-05 19:03:59,792:INFO:Importing libraries
2024-02-05 19:03:59,792:INFO:Copying training dataset
2024-02-05 19:03:59,798:INFO:Defining folds
2024-02-05 19:03:59,798:INFO:Declaring metric variables
2024-02-05 19:03:59,798:INFO:Importing untrained model
2024-02-05 19:03:59,798:INFO:Declaring custom model
2024-02-05 19:03:59,799:INFO:Extra Trees Regressor Imported successfully
2024-02-05 19:03:59,799:INFO:Starting cross validation
2024-02-05 19:03:59,800:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:04:01,993:INFO:Calculating mean and std
2024-02-05 19:04:01,993:INFO:Creating metrics dataframe
2024-02-05 19:04:01,994:INFO:Finalizing model
2024-02-05 19:04:02,349:INFO:Uploading results into container
2024-02-05 19:04:02,350:INFO:_master_model_container: 48
2024-02-05 19:04:02,350:INFO:_display_container: 48
2024-02-05 19:04:02,350:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2024-02-05 19:04:02,350:INFO:create_model() successfully completed......................................
2024-02-05 19:04:02,481:INFO:Initializing create_model()
2024-02-05 19:04:02,481:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=AdaBoostRegressor(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:04:02,481:INFO:Checking exceptions
2024-02-05 19:04:02,483:INFO:Importing libraries
2024-02-05 19:04:02,483:INFO:Copying training dataset
2024-02-05 19:04:02,489:INFO:Defining folds
2024-02-05 19:04:02,489:INFO:Declaring metric variables
2024-02-05 19:04:02,490:INFO:Importing untrained model
2024-02-05 19:04:02,490:INFO:Declaring custom model
2024-02-05 19:04:02,490:INFO:AdaBoost Regressor Imported successfully
2024-02-05 19:04:02,490:INFO:Starting cross validation
2024-02-05 19:04:02,492:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:04:03,549:INFO:Calculating mean and std
2024-02-05 19:04:03,549:INFO:Creating metrics dataframe
2024-02-05 19:04:03,550:INFO:Finalizing model
2024-02-05 19:04:03,898:INFO:Uploading results into container
2024-02-05 19:04:03,899:INFO:_master_model_container: 48
2024-02-05 19:04:03,899:INFO:_display_container: 49
2024-02-05 19:04:03,899:INFO:AdaBoostRegressor(random_state=123)
2024-02-05 19:04:03,899:INFO:create_model() successfully completed......................................
2024-02-05 19:04:04,029:INFO:Initializing create_model()
2024-02-05 19:04:04,029:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=GradientBoostingRegressor(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:04:04,030:INFO:Checking exceptions
2024-02-05 19:04:04,033:INFO:Importing libraries
2024-02-05 19:04:04,033:INFO:Copying training dataset
2024-02-05 19:04:04,037:INFO:Defining folds
2024-02-05 19:04:04,037:INFO:Declaring metric variables
2024-02-05 19:04:04,038:INFO:Importing untrained model
2024-02-05 19:04:04,038:INFO:Declaring custom model
2024-02-05 19:04:04,038:INFO:Gradient Boosting Regressor Imported successfully
2024-02-05 19:04:04,039:INFO:Starting cross validation
2024-02-05 19:04:04,041:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:04:05,630:INFO:Calculating mean and std
2024-02-05 19:04:05,630:INFO:Creating metrics dataframe
2024-02-05 19:04:05,632:INFO:Finalizing model
2024-02-05 19:04:06,161:INFO:Uploading results into container
2024-02-05 19:04:06,162:INFO:_master_model_container: 48
2024-02-05 19:04:06,162:INFO:_display_container: 50
2024-02-05 19:04:06,162:INFO:GradientBoostingRegressor(random_state=123)
2024-02-05 19:04:06,162:INFO:create_model() successfully completed......................................
2024-02-05 19:04:06,294:INFO:Initializing create_model()
2024-02-05 19:04:06,295:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:04:06,295:INFO:Checking exceptions
2024-02-05 19:04:06,297:INFO:Importing libraries
2024-02-05 19:04:06,297:INFO:Copying training dataset
2024-02-05 19:04:06,302:INFO:Defining folds
2024-02-05 19:04:06,302:INFO:Declaring metric variables
2024-02-05 19:04:06,302:INFO:Importing untrained model
2024-02-05 19:04:06,302:INFO:Declaring custom model
2024-02-05 19:04:06,302:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-05 19:04:06,302:INFO:Starting cross validation
2024-02-05 19:04:06,304:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:04:07,723:INFO:Calculating mean and std
2024-02-05 19:04:07,723:INFO:Creating metrics dataframe
2024-02-05 19:04:07,725:INFO:Finalizing model
2024-02-05 19:04:07,869:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-02-05 19:04:07,870:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000799 seconds.
2024-02-05 19:04:07,870:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-05 19:04:07,870:INFO:[LightGBM] [Info] Total Bins 2881
2024-02-05 19:04:07,871:INFO:[LightGBM] [Info] Number of data points in the train set: 1019, number of used features: 93
2024-02-05 19:04:07,871:INFO:[LightGBM] [Info] Start training from score 180679.625123
2024-02-05 19:04:07,942:INFO:Uploading results into container
2024-02-05 19:04:07,944:INFO:_master_model_container: 48
2024-02-05 19:04:07,944:INFO:_display_container: 51
2024-02-05 19:04:07,944:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-05 19:04:07,944:INFO:create_model() successfully completed......................................
2024-02-05 19:04:08,092:INFO:Initializing create_model()
2024-02-05 19:04:08,092:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001D7B322A2B0>, estimator=DummyRegressor(), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-05 19:04:08,093:INFO:Checking exceptions
2024-02-05 19:04:08,094:INFO:Importing libraries
2024-02-05 19:04:08,094:INFO:Copying training dataset
2024-02-05 19:04:08,100:INFO:Defining folds
2024-02-05 19:04:08,100:INFO:Declaring metric variables
2024-02-05 19:04:08,100:INFO:Importing untrained model
2024-02-05 19:04:08,100:INFO:Declaring custom model
2024-02-05 19:04:08,100:INFO:Dummy Regressor Imported successfully
2024-02-05 19:04:08,100:INFO:Starting cross validation
2024-02-05 19:04:08,101:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-05 19:04:08,619:INFO:Calculating mean and std
2024-02-05 19:04:08,619:INFO:Creating metrics dataframe
2024-02-05 19:04:08,620:INFO:Finalizing model
2024-02-05 19:04:08,739:INFO:Uploading results into container
2024-02-05 19:04:08,740:INFO:_master_model_container: 48
2024-02-05 19:04:08,740:INFO:_display_container: 52
2024-02-05 19:04:08,740:INFO:DummyRegressor()
2024-02-05 19:04:08,740:INFO:create_model() successfully completed......................................
