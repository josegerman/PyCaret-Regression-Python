2024-02-04 19:05:19,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:05:19,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:05:19,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:05:19,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:05:19,901:INFO:PyCaret RegressionExperiment
2024-02-04 19:05:19,901:INFO:Logging name: houseprice1
2024-02-04 19:05:19,901:INFO:ML Usecase: MLUsecase.REGRESSION
2024-02-04 19:05:19,901:INFO:version 3.2.0
2024-02-04 19:05:19,901:INFO:Initializing setup()
2024-02-04 19:05:19,901:INFO:self.USI: 027c
2024-02-04 19:05:19,901:INFO:self._variable_keys: {'X_test', 'idx', 'gpu_n_jobs_param', 'y_test', 'memory', 'y_train', 'fold_generator', 'exp_id', 'y', 'fold_shuffle_param', 'data', 'pipeline', 'logging_param', 'n_jobs_param', 'gpu_param', 'seed', 'X_train', 'X', 'html_param', '_available_plots', 'target_param', 'transform_target_param', 'USI', 'fold_groups_param', 'exp_name_log', '_ml_usecase', 'log_plots_param'}
2024-02-04 19:05:19,901:INFO:Checking environment
2024-02-04 19:05:19,901:INFO:python_version: 3.9.9
2024-02-04 19:05:19,901:INFO:python_build: ('tags/v3.9.9:ccb0e6a', 'Nov 15 2021 18:08:50')
2024-02-04 19:05:19,901:INFO:machine: AMD64
2024-02-04 19:05:19,901:INFO:platform: Windows-10-10.0.22621-SP0
2024-02-04 19:05:19,901:INFO:Memory: svmem(total=16856203264, available=7707590656, percent=54.3, used=9148612608, free=7707590656)
2024-02-04 19:05:19,901:INFO:Physical Core: 4
2024-02-04 19:05:19,901:INFO:Logical Core: 8
2024-02-04 19:05:19,901:INFO:Checking libraries
2024-02-04 19:05:19,901:INFO:System:
2024-02-04 19:05:19,901:INFO:    python: 3.9.9 (tags/v3.9.9:ccb0e6a, Nov 15 2021, 18:08:50) [MSC v.1929 64 bit (AMD64)]
2024-02-04 19:05:19,901:INFO:executable: c:\Users\joseg\AppData\Local\Programs\Python\Python39\python.exe
2024-02-04 19:05:19,901:INFO:   machine: Windows-10-10.0.22621-SP0
2024-02-04 19:05:19,901:INFO:PyCaret required dependencies:
2024-02-04 19:05:19,951:INFO:                 pip: 21.2.4
2024-02-04 19:05:19,951:INFO:          setuptools: 58.1.0
2024-02-04 19:05:19,951:INFO:             pycaret: 3.2.0
2024-02-04 19:05:19,951:INFO:             IPython: 8.18.1
2024-02-04 19:05:19,951:INFO:          ipywidgets: 8.1.1
2024-02-04 19:05:19,951:INFO:                tqdm: 4.66.1
2024-02-04 19:05:19,951:INFO:               numpy: 1.23.5
2024-02-04 19:05:19,951:INFO:              pandas: 1.5.3
2024-02-04 19:05:19,951:INFO:              jinja2: 3.1.3
2024-02-04 19:05:19,951:INFO:               scipy: 1.10.1
2024-02-04 19:05:19,951:INFO:              joblib: 1.3.2
2024-02-04 19:05:19,951:INFO:             sklearn: 1.2.2
2024-02-04 19:05:19,951:INFO:                pyod: 1.1.2
2024-02-04 19:05:19,951:INFO:            imblearn: 0.12.0
2024-02-04 19:05:19,951:INFO:   category_encoders: 2.6.3
2024-02-04 19:05:19,951:INFO:            lightgbm: 4.3.0
2024-02-04 19:05:19,951:INFO:               numba: 0.59.0
2024-02-04 19:05:19,951:INFO:            requests: 2.31.0
2024-02-04 19:05:19,951:INFO:          matplotlib: 3.6.0
2024-02-04 19:05:19,951:INFO:          scikitplot: 0.3.7
2024-02-04 19:05:19,951:INFO:         yellowbrick: 1.5
2024-02-04 19:05:19,951:INFO:              plotly: 5.18.0
2024-02-04 19:05:19,951:INFO:    plotly-resampler: Not installed
2024-02-04 19:05:19,951:INFO:             kaleido: 0.2.1
2024-02-04 19:05:19,951:INFO:           schemdraw: 0.15
2024-02-04 19:05:19,951:INFO:         statsmodels: 0.14.1
2024-02-04 19:05:19,951:INFO:              sktime: 0.21.1
2024-02-04 19:05:19,951:INFO:               tbats: 1.1.3
2024-02-04 19:05:19,951:INFO:            pmdarima: 2.0.4
2024-02-04 19:05:19,951:INFO:              psutil: 5.9.8
2024-02-04 19:05:19,951:INFO:          markupsafe: 2.1.5
2024-02-04 19:05:19,951:INFO:             pickle5: Not installed
2024-02-04 19:05:19,951:INFO:         cloudpickle: 3.0.0
2024-02-04 19:05:19,951:INFO:         deprecation: 2.1.0
2024-02-04 19:05:19,951:INFO:              xxhash: 3.4.1
2024-02-04 19:05:19,951:INFO:           wurlitzer: Not installed
2024-02-04 19:05:19,951:INFO:PyCaret optional dependencies:
2024-02-04 19:05:21,017:INFO:                shap: 0.44.1
2024-02-04 19:05:21,017:INFO:           interpret: Not installed
2024-02-04 19:05:21,017:INFO:                umap: Not installed
2024-02-04 19:05:21,017:INFO:     ydata_profiling: Not installed
2024-02-04 19:05:21,017:INFO:  explainerdashboard: 0.4.5
2024-02-04 19:05:21,017:INFO:             autoviz: Not installed
2024-02-04 19:05:21,017:INFO:           fairlearn: Not installed
2024-02-04 19:05:21,017:INFO:          deepchecks: Not installed
2024-02-04 19:05:21,017:INFO:             xgboost: Not installed
2024-02-04 19:05:21,017:INFO:            catboost: Not installed
2024-02-04 19:05:21,017:INFO:              kmodes: Not installed
2024-02-04 19:05:21,017:INFO:             mlxtend: Not installed
2024-02-04 19:05:21,017:INFO:       statsforecast: Not installed
2024-02-04 19:05:21,017:INFO:        tune_sklearn: Not installed
2024-02-04 19:05:21,017:INFO:                 ray: Not installed
2024-02-04 19:05:21,017:INFO:            hyperopt: Not installed
2024-02-04 19:05:21,017:INFO:              optuna: Not installed
2024-02-04 19:05:21,017:INFO:               skopt: Not installed
2024-02-04 19:05:21,017:INFO:              mlflow: 2.10.0
2024-02-04 19:05:21,017:INFO:              gradio: 3.50.0
2024-02-04 19:05:21,017:INFO:             fastapi: 0.109.1
2024-02-04 19:05:21,017:INFO:             uvicorn: 0.27.0.post1
2024-02-04 19:05:21,017:INFO:              m2cgen: Not installed
2024-02-04 19:05:21,017:INFO:           evidently: Not installed
2024-02-04 19:05:21,017:INFO:               fugue: Not installed
2024-02-04 19:05:21,017:INFO:           streamlit: Not installed
2024-02-04 19:05:21,017:INFO:             prophet: Not installed
2024-02-04 19:05:21,017:INFO:None
2024-02-04 19:05:21,017:INFO:Set up data.
2024-02-04 19:05:21,034:INFO:Set up folding strategy.
2024-02-04 19:05:21,034:INFO:Set up train/test split.
2024-02-04 19:05:21,044:INFO:Set up index.
2024-02-04 19:05:21,044:INFO:Assigning column types.
2024-02-04 19:05:21,051:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-02-04 19:05:21,051:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,051:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,051:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,084:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,117:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,117:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,117:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,117:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,117:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,134:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,167:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,201:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,201:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,201:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,201:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2024-02-04 19:05:21,201:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,201:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,234:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,267:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,267:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,267:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,281:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,284:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,320:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,351:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,351:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,351:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,351:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2024-02-04 19:05:21,351:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,384:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,420:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,420:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,420:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,432:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,467:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,495:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,495:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,495:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,500:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2024-02-04 19:05:21,537:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,567:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,567:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,567:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,617:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,645:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,645:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,645:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,645:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-02-04 19:05:21,692:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,717:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,717:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,767:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2024-02-04 19:05:21,796:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,796:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,796:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2024-02-04 19:05:21,867:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,867:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,934:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,934:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:21,946:INFO:Preparing preprocessing pipeline...
2024-02-04 19:05:21,946:INFO:Set up simple imputation.
2024-02-04 19:05:21,950:INFO:Set up encoding of ordinal features.
2024-02-04 19:05:21,950:INFO:Set up encoding of categorical features.
2024-02-04 19:05:22,081:INFO:Finished creating preprocessing pipeline.
2024-02-04 19:05:22,097:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))])
2024-02-04 19:05:22,097:INFO:Creating final display dataframe.
2024-02-04 19:05:22,448:INFO:Setup _display_container:                     Description         Value
0                    Session id           123
1                        Target     SalePrice
2                   Target type    Regression
3           Original data shape    (1456, 56)
4        Transformed data shape   (1456, 134)
5   Transformed train set shape   (1019, 134)
6    Transformed test set shape    (437, 134)
7              Ordinal features             1
8              Numeric features            40
9          Categorical features            15
10     Rows with missing values         99.7%
11                   Preprocess          True
12              Imputation type        simple
13           Numeric imputation          mean
14       Categorical imputation          mode
15     Maximum one-hot encoding            25
16              Encoding method          None
17               Fold Generator         KFold
18                  Fold Number            10
19                     CPU Jobs            -1
20                      Use GPU         False
21               Log Experiment  MlflowLogger
22              Experiment Name   houseprice1
23                          USI          027c
2024-02-04 19:05:22,541:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:22,541:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:22,617:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:22,617:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-02-04 19:05:22,617:INFO:Logging experiment in loggers
2024-02-04 19:05:22,783:INFO:SubProcess save_model() called ==================================
2024-02-04 19:05:22,807:INFO:Initializing save_model()
2024-02-04 19:05:22,807:INFO:save_model(model=Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))]), model_name=C:\Users\joseg\AppData\Local\Temp\tmpf6nvxbbn\Transformation Pipeline, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2024-02-04 19:05:22,807:INFO:Adding model into prep_pipe
2024-02-04 19:05:22,807:WARNING:Only Model saved as it was a pipeline.
2024-02-04 19:05:22,817:INFO:C:\Users\joseg\AppData\Local\Temp\tmpf6nvxbbn\Transformation Pipeline.pkl saved in current working directory
2024-02-04 19:05:22,831:INFO:Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))])
2024-02-04 19:05:22,831:INFO:save_model() successfully completed......................................
2024-02-04 19:05:22,918:INFO:SubProcess save_model() end ==================================
2024-02-04 19:05:22,918:INFO:setup() successfully completed in 2.72s...............
2024-02-04 19:05:59,243:INFO:Initializing compare_models()
2024-02-04 19:05:59,243:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, include=None, fold=5, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, 'include': None, 'exclude': None, 'fold': 5, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2024-02-04 19:05:59,243:INFO:Checking exceptions
2024-02-04 19:05:59,243:INFO:Preparing display monitor
2024-02-04 19:05:59,271:INFO:Initializing Linear Regression
2024-02-04 19:05:59,271:INFO:Total runtime is 0.0 minutes
2024-02-04 19:05:59,271:INFO:SubProcess create_model() called ==================================
2024-02-04 19:05:59,271:INFO:Initializing create_model()
2024-02-04 19:05:59,271:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lr, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:05:59,271:INFO:Checking exceptions
2024-02-04 19:05:59,271:INFO:Importing libraries
2024-02-04 19:05:59,271:INFO:Copying training dataset
2024-02-04 19:05:59,271:INFO:Defining folds
2024-02-04 19:05:59,271:INFO:Declaring metric variables
2024-02-04 19:05:59,279:INFO:Importing untrained model
2024-02-04 19:05:59,279:INFO:Linear Regression Imported successfully
2024-02-04 19:05:59,287:INFO:Starting cross validation
2024-02-04 19:05:59,296:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:01,994:INFO:Calculating mean and std
2024-02-04 19:06:01,996:INFO:Creating metrics dataframe
2024-02-04 19:06:01,996:INFO:Uploading results into container
2024-02-04 19:06:01,996:INFO:Uploading model into container now
2024-02-04 19:06:01,996:INFO:_master_model_container: 1
2024-02-04 19:06:01,996:INFO:_display_container: 2
2024-02-04 19:06:01,996:INFO:LinearRegression(n_jobs=-1)
2024-02-04 19:06:01,996:INFO:create_model() successfully completed......................................
2024-02-04 19:06:02,096:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:02,096:INFO:Creating metrics dataframe
2024-02-04 19:06:02,096:INFO:Initializing Lasso Regression
2024-02-04 19:06:02,096:INFO:Total runtime is 0.04709691603978475 minutes
2024-02-04 19:06:02,096:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:02,096:INFO:Initializing create_model()
2024-02-04 19:06:02,096:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lasso, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:02,096:INFO:Checking exceptions
2024-02-04 19:06:02,110:INFO:Importing libraries
2024-02-04 19:06:02,110:INFO:Copying training dataset
2024-02-04 19:06:02,113:INFO:Defining folds
2024-02-04 19:06:02,113:INFO:Declaring metric variables
2024-02-04 19:06:02,113:INFO:Importing untrained model
2024-02-04 19:06:02,120:INFO:Lasso Regression Imported successfully
2024-02-04 19:06:02,120:INFO:Starting cross validation
2024-02-04 19:06:02,120:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:02,380:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.451e+11, tolerance: 5.149e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:02,396:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.972e+11, tolerance: 4.496e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:03,813:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.206e+11, tolerance: 5.100e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:03,846:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.484e+11, tolerance: 5.182e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:03,846:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.138e+11, tolerance: 4.925e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:03,928:INFO:Calculating mean and std
2024-02-04 19:06:03,929:INFO:Creating metrics dataframe
2024-02-04 19:06:03,935:INFO:Uploading results into container
2024-02-04 19:06:03,935:INFO:Uploading model into container now
2024-02-04 19:06:03,937:INFO:_master_model_container: 2
2024-02-04 19:06:03,937:INFO:_display_container: 2
2024-02-04 19:06:03,937:INFO:Lasso(random_state=123)
2024-02-04 19:06:03,937:INFO:create_model() successfully completed......................................
2024-02-04 19:06:04,029:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:04,029:INFO:Creating metrics dataframe
2024-02-04 19:06:04,034:INFO:Initializing Ridge Regression
2024-02-04 19:06:04,034:INFO:Total runtime is 0.07938300768534343 minutes
2024-02-04 19:06:04,042:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:04,042:INFO:Initializing create_model()
2024-02-04 19:06:04,042:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=ridge, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:04,042:INFO:Checking exceptions
2024-02-04 19:06:04,042:INFO:Importing libraries
2024-02-04 19:06:04,042:INFO:Copying training dataset
2024-02-04 19:06:04,046:INFO:Defining folds
2024-02-04 19:06:04,046:INFO:Declaring metric variables
2024-02-04 19:06:04,051:INFO:Importing untrained model
2024-02-04 19:06:04,051:INFO:Ridge Regression Imported successfully
2024-02-04 19:06:04,051:INFO:Starting cross validation
2024-02-04 19:06:04,051:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:04,359:INFO:Calculating mean and std
2024-02-04 19:06:04,359:INFO:Creating metrics dataframe
2024-02-04 19:06:04,362:INFO:Uploading results into container
2024-02-04 19:06:04,362:INFO:Uploading model into container now
2024-02-04 19:06:04,362:INFO:_master_model_container: 3
2024-02-04 19:06:04,362:INFO:_display_container: 2
2024-02-04 19:06:04,362:INFO:Ridge(random_state=123)
2024-02-04 19:06:04,362:INFO:create_model() successfully completed......................................
2024-02-04 19:06:04,446:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:04,446:INFO:Creating metrics dataframe
2024-02-04 19:06:04,458:INFO:Initializing Elastic Net
2024-02-04 19:06:04,458:INFO:Total runtime is 0.08645121256510417 minutes
2024-02-04 19:06:04,464:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:04,464:INFO:Initializing create_model()
2024-02-04 19:06:04,464:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=en, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:04,464:INFO:Checking exceptions
2024-02-04 19:06:04,464:INFO:Importing libraries
2024-02-04 19:06:04,464:INFO:Copying training dataset
2024-02-04 19:06:04,464:INFO:Defining folds
2024-02-04 19:06:04,464:INFO:Declaring metric variables
2024-02-04 19:06:04,473:INFO:Importing untrained model
2024-02-04 19:06:04,477:INFO:Elastic Net Imported successfully
2024-02-04 19:06:04,481:INFO:Starting cross validation
2024-02-04 19:06:04,481:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:04,806:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.303e+11, tolerance: 5.100e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:04,813:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.375e+11, tolerance: 4.925e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:04,826:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.038e+11, tolerance: 4.496e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:04,827:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.592e+11, tolerance: 5.182e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:04,829:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.659e+11, tolerance: 5.149e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:06:04,913:INFO:Calculating mean and std
2024-02-04 19:06:04,914:INFO:Creating metrics dataframe
2024-02-04 19:06:04,918:INFO:Uploading results into container
2024-02-04 19:06:04,918:INFO:Uploading model into container now
2024-02-04 19:06:04,918:INFO:_master_model_container: 4
2024-02-04 19:06:04,918:INFO:_display_container: 2
2024-02-04 19:06:04,918:INFO:ElasticNet(random_state=123)
2024-02-04 19:06:04,918:INFO:create_model() successfully completed......................................
2024-02-04 19:06:05,008:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:05,008:INFO:Creating metrics dataframe
2024-02-04 19:06:05,016:INFO:Initializing Least Angle Regression
2024-02-04 19:06:05,016:INFO:Total runtime is 0.09575716257095337 minutes
2024-02-04 19:06:05,016:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:05,016:INFO:Initializing create_model()
2024-02-04 19:06:05,016:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lar, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:05,016:INFO:Checking exceptions
2024-02-04 19:06:05,016:INFO:Importing libraries
2024-02-04 19:06:05,016:INFO:Copying training dataset
2024-02-04 19:06:05,016:INFO:Defining folds
2024-02-04 19:06:05,016:INFO:Declaring metric variables
2024-02-04 19:06:05,029:INFO:Importing untrained model
2024-02-04 19:06:05,033:INFO:Least Angle Regression Imported successfully
2024-02-04 19:06:05,034:INFO:Starting cross validation
2024-02-04 19:06:05,034:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=3.120e+02, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.077e+02, with an active set of 39 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=2.921e+02, with an active set of 49 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.866e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=7.747e+01, with an active set of 84 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.845e+02, with an active set of 53 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.239e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.379e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=1.536e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.070e+02, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,247:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=8.829e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=7.866e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=1.273e+02, with an active set of 75 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=9.683e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=6.627e+01, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=5.541e+01, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=5.236e+01, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=5.124e+01, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=4.716e+01, with an active set of 95 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=4.493e+01, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=4.481e+05, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.188e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=1.683e+02, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=2.478e+06, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 167 iterations, i.e. alpha=2.516e+06, with an active set of 131 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=9.821e+05, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=7.083e+05, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=6.203e+05, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=5.911e+05, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 170 iterations, i.e. alpha=3.301e+09, with an active set of 129 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=2.670e+06, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 170 iterations, i.e. alpha=2.294e+09, with an active set of 129 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 131 iterations, i.e. alpha=5.158e+05, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=3.391e+05, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=3.311e+05, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 177 iterations, i.e. alpha=1.130e+04, with an active set of 130 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 177 iterations, i.e. alpha=2.523e+03, with an active set of 130 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=3.302e+05, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,263:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=6.480e+05, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,279:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 173 iterations, i.e. alpha=3.011e+08, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,279:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=4.104e+06, with an active set of 130 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,279:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=3.638e+06, with an active set of 130 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,358:INFO:Calculating mean and std
2024-02-04 19:06:05,358:INFO:Creating metrics dataframe
2024-02-04 19:06:05,358:INFO:Uploading results into container
2024-02-04 19:06:05,358:INFO:Uploading model into container now
2024-02-04 19:06:05,358:INFO:_master_model_container: 5
2024-02-04 19:06:05,358:INFO:_display_container: 2
2024-02-04 19:06:05,358:INFO:Lars(random_state=123)
2024-02-04 19:06:05,358:INFO:create_model() successfully completed......................................
2024-02-04 19:06:05,449:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:05,449:INFO:Creating metrics dataframe
2024-02-04 19:06:05,449:INFO:Initializing Lasso Least Angle Regression
2024-02-04 19:06:05,449:INFO:Total runtime is 0.10297880570093791 minutes
2024-02-04 19:06:05,465:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:05,465:INFO:Initializing create_model()
2024-02-04 19:06:05,465:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=llar, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:05,466:INFO:Checking exceptions
2024-02-04 19:06:05,466:INFO:Importing libraries
2024-02-04 19:06:05,466:INFO:Copying training dataset
2024-02-04 19:06:05,466:INFO:Defining folds
2024-02-04 19:06:05,466:INFO:Declaring metric variables
2024-02-04 19:06:05,473:INFO:Importing untrained model
2024-02-04 19:06:05,473:INFO:Lasso Least Angle Regression Imported successfully
2024-02-04 19:06:05,483:INFO:Starting cross validation
2024-02-04 19:06:05,483:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:05,717:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=3.367e+02, with an active set of 47 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,717:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.695e+02, with an active set of 65 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,717:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=5.136e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,727:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.452e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,729:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 79 iterations, alpha=1.116e+03, previous alpha=1.723e+02, with an active set of 60 regressors.
  warnings.warn(

2024-02-04 19:06:05,729:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=3.697e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,729:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=2.465e+01, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 132 iterations, i.e. alpha=1.866e+01, with an active set of 106 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 138 iterations, alpha=1.717e+01, previous alpha=1.717e+01, with an active set of 107 regressors.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=4.971e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=4.937e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=8.321e+01, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.873e+02, with an active set of 51 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=2.455e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=4.349e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=1.392e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=1.392e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=1.199e+02, with an active set of 71 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=2.372e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,735:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=9.636e+01, with an active set of 75 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,750:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 103 iterations, alpha=6.958e+01, previous alpha=6.958e+01, with an active set of 82 regressors.
  warnings.warn(

2024-02-04 19:06:05,750:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=1.063e+02, with an active set of 78 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,750:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 135 iterations, alpha=6.986e+01, previous alpha=2.488e+01, with an active set of 100 regressors.
  warnings.warn(

2024-02-04 19:06:05,750:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=6.564e+01, with an active set of 81 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:06:05,750:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 140 iterations, alpha=2.593e+01, previous alpha=2.593e+01, with an active set of 97 regressors.
  warnings.warn(

2024-02-04 19:06:05,828:INFO:Calculating mean and std
2024-02-04 19:06:05,828:INFO:Creating metrics dataframe
2024-02-04 19:06:05,833:INFO:Uploading results into container
2024-02-04 19:06:05,833:INFO:Uploading model into container now
2024-02-04 19:06:05,833:INFO:_master_model_container: 6
2024-02-04 19:06:05,833:INFO:_display_container: 2
2024-02-04 19:06:05,833:INFO:LassoLars(random_state=123)
2024-02-04 19:06:05,833:INFO:create_model() successfully completed......................................
2024-02-04 19:06:05,929:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:05,929:INFO:Creating metrics dataframe
2024-02-04 19:06:05,938:INFO:Initializing Orthogonal Matching Pursuit
2024-02-04 19:06:05,938:INFO:Total runtime is 0.11112848122914633 minutes
2024-02-04 19:06:05,941:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:05,941:INFO:Initializing create_model()
2024-02-04 19:06:05,942:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=omp, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:05,942:INFO:Checking exceptions
2024-02-04 19:06:05,942:INFO:Importing libraries
2024-02-04 19:06:05,942:INFO:Copying training dataset
2024-02-04 19:06:05,946:INFO:Defining folds
2024-02-04 19:06:05,947:INFO:Declaring metric variables
2024-02-04 19:06:05,951:INFO:Importing untrained model
2024-02-04 19:06:05,953:INFO:Orthogonal Matching Pursuit Imported successfully
2024-02-04 19:06:05,962:INFO:Starting cross validation
2024-02-04 19:06:05,969:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:06,278:INFO:Calculating mean and std
2024-02-04 19:06:06,278:INFO:Creating metrics dataframe
2024-02-04 19:06:06,281:INFO:Uploading results into container
2024-02-04 19:06:06,281:INFO:Uploading model into container now
2024-02-04 19:06:06,281:INFO:_master_model_container: 7
2024-02-04 19:06:06,281:INFO:_display_container: 2
2024-02-04 19:06:06,285:INFO:OrthogonalMatchingPursuit()
2024-02-04 19:06:06,285:INFO:create_model() successfully completed......................................
2024-02-04 19:06:06,379:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:06,379:INFO:Creating metrics dataframe
2024-02-04 19:06:06,379:INFO:Initializing Bayesian Ridge
2024-02-04 19:06:06,379:INFO:Total runtime is 0.11848035256067913 minutes
2024-02-04 19:06:06,379:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:06,379:INFO:Initializing create_model()
2024-02-04 19:06:06,379:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=br, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:06,379:INFO:Checking exceptions
2024-02-04 19:06:06,379:INFO:Importing libraries
2024-02-04 19:06:06,379:INFO:Copying training dataset
2024-02-04 19:06:06,396:INFO:Defining folds
2024-02-04 19:06:06,396:INFO:Declaring metric variables
2024-02-04 19:06:06,396:INFO:Importing untrained model
2024-02-04 19:06:06,396:INFO:Bayesian Ridge Imported successfully
2024-02-04 19:06:06,396:INFO:Starting cross validation
2024-02-04 19:06:06,410:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:06,759:INFO:Calculating mean and std
2024-02-04 19:06:06,759:INFO:Creating metrics dataframe
2024-02-04 19:06:06,762:INFO:Uploading results into container
2024-02-04 19:06:06,762:INFO:Uploading model into container now
2024-02-04 19:06:06,762:INFO:_master_model_container: 8
2024-02-04 19:06:06,762:INFO:_display_container: 2
2024-02-04 19:06:06,764:INFO:BayesianRidge()
2024-02-04 19:06:06,764:INFO:create_model() successfully completed......................................
2024-02-04 19:06:06,849:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:06,859:INFO:Creating metrics dataframe
2024-02-04 19:06:06,867:INFO:Initializing Passive Aggressive Regressor
2024-02-04 19:06:06,867:INFO:Total runtime is 0.12661214272181193 minutes
2024-02-04 19:06:06,867:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:06,867:INFO:Initializing create_model()
2024-02-04 19:06:06,867:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=par, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:06,867:INFO:Checking exceptions
2024-02-04 19:06:06,867:INFO:Importing libraries
2024-02-04 19:06:06,867:INFO:Copying training dataset
2024-02-04 19:06:06,875:INFO:Defining folds
2024-02-04 19:06:06,875:INFO:Declaring metric variables
2024-02-04 19:06:06,880:INFO:Importing untrained model
2024-02-04 19:06:06,884:INFO:Passive Aggressive Regressor Imported successfully
2024-02-04 19:06:06,884:INFO:Starting cross validation
2024-02-04 19:06:06,884:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:07,194:INFO:Calculating mean and std
2024-02-04 19:06:07,194:INFO:Creating metrics dataframe
2024-02-04 19:06:07,199:INFO:Uploading results into container
2024-02-04 19:06:07,199:INFO:Uploading model into container now
2024-02-04 19:06:07,199:INFO:_master_model_container: 9
2024-02-04 19:06:07,199:INFO:_display_container: 2
2024-02-04 19:06:07,199:INFO:PassiveAggressiveRegressor(random_state=123)
2024-02-04 19:06:07,199:INFO:create_model() successfully completed......................................
2024-02-04 19:06:07,279:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:07,279:INFO:Creating metrics dataframe
2024-02-04 19:06:07,298:INFO:Initializing Huber Regressor
2024-02-04 19:06:07,300:INFO:Total runtime is 0.13379119237263998 minutes
2024-02-04 19:06:07,300:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:07,300:INFO:Initializing create_model()
2024-02-04 19:06:07,300:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=huber, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:07,300:INFO:Checking exceptions
2024-02-04 19:06:07,300:INFO:Importing libraries
2024-02-04 19:06:07,300:INFO:Copying training dataset
2024-02-04 19:06:07,300:INFO:Defining folds
2024-02-04 19:06:07,300:INFO:Declaring metric variables
2024-02-04 19:06:07,300:INFO:Importing untrained model
2024-02-04 19:06:07,313:INFO:Huber Regressor Imported successfully
2024-02-04 19:06:07,317:INFO:Starting cross validation
2024-02-04 19:06:07,317:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:07,603:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:06:07,616:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:06:07,618:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:06:07,634:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:06:07,651:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:06:07,728:INFO:Calculating mean and std
2024-02-04 19:06:07,731:INFO:Creating metrics dataframe
2024-02-04 19:06:07,734:INFO:Uploading results into container
2024-02-04 19:06:07,735:INFO:Uploading model into container now
2024-02-04 19:06:07,735:INFO:_master_model_container: 10
2024-02-04 19:06:07,735:INFO:_display_container: 2
2024-02-04 19:06:07,735:INFO:HuberRegressor()
2024-02-04 19:06:07,735:INFO:create_model() successfully completed......................................
2024-02-04 19:06:07,834:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:07,834:INFO:Creating metrics dataframe
2024-02-04 19:06:07,834:INFO:Initializing K Neighbors Regressor
2024-02-04 19:06:07,834:INFO:Total runtime is 0.1427184502283732 minutes
2024-02-04 19:06:07,834:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:07,844:INFO:Initializing create_model()
2024-02-04 19:06:07,844:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=knn, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:07,844:INFO:Checking exceptions
2024-02-04 19:06:07,844:INFO:Importing libraries
2024-02-04 19:06:07,844:INFO:Copying training dataset
2024-02-04 19:06:07,851:INFO:Defining folds
2024-02-04 19:06:07,851:INFO:Declaring metric variables
2024-02-04 19:06:07,851:INFO:Importing untrained model
2024-02-04 19:06:07,857:INFO:K Neighbors Regressor Imported successfully
2024-02-04 19:06:07,864:INFO:Starting cross validation
2024-02-04 19:06:07,864:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:08,291:INFO:Calculating mean and std
2024-02-04 19:06:08,291:INFO:Creating metrics dataframe
2024-02-04 19:06:08,291:INFO:Uploading results into container
2024-02-04 19:06:08,291:INFO:Uploading model into container now
2024-02-04 19:06:08,291:INFO:_master_model_container: 11
2024-02-04 19:06:08,291:INFO:_display_container: 2
2024-02-04 19:06:08,296:INFO:KNeighborsRegressor(n_jobs=-1)
2024-02-04 19:06:08,296:INFO:create_model() successfully completed......................................
2024-02-04 19:06:08,379:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:08,379:INFO:Creating metrics dataframe
2024-02-04 19:06:08,392:INFO:Initializing Decision Tree Regressor
2024-02-04 19:06:08,396:INFO:Total runtime is 0.1520889639854431 minutes
2024-02-04 19:06:08,399:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:08,399:INFO:Initializing create_model()
2024-02-04 19:06:08,399:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=dt, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:08,399:INFO:Checking exceptions
2024-02-04 19:06:08,399:INFO:Importing libraries
2024-02-04 19:06:08,399:INFO:Copying training dataset
2024-02-04 19:06:08,399:INFO:Defining folds
2024-02-04 19:06:08,399:INFO:Declaring metric variables
2024-02-04 19:06:08,399:INFO:Importing untrained model
2024-02-04 19:06:08,410:INFO:Decision Tree Regressor Imported successfully
2024-02-04 19:06:08,416:INFO:Starting cross validation
2024-02-04 19:06:08,416:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:08,777:INFO:Calculating mean and std
2024-02-04 19:06:08,777:INFO:Creating metrics dataframe
2024-02-04 19:06:08,779:INFO:Uploading results into container
2024-02-04 19:06:08,779:INFO:Uploading model into container now
2024-02-04 19:06:08,779:INFO:_master_model_container: 12
2024-02-04 19:06:08,779:INFO:_display_container: 2
2024-02-04 19:06:08,779:INFO:DecisionTreeRegressor(random_state=123)
2024-02-04 19:06:08,779:INFO:create_model() successfully completed......................................
2024-02-04 19:06:08,865:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:08,865:INFO:Creating metrics dataframe
2024-02-04 19:06:08,865:INFO:Initializing Random Forest Regressor
2024-02-04 19:06:08,865:INFO:Total runtime is 0.1599098483721415 minutes
2024-02-04 19:06:08,881:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:08,881:INFO:Initializing create_model()
2024-02-04 19:06:08,881:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=rf, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:08,881:INFO:Checking exceptions
2024-02-04 19:06:08,881:INFO:Importing libraries
2024-02-04 19:06:08,881:INFO:Copying training dataset
2024-02-04 19:06:08,881:INFO:Defining folds
2024-02-04 19:06:08,881:INFO:Declaring metric variables
2024-02-04 19:06:08,881:INFO:Importing untrained model
2024-02-04 19:06:08,892:INFO:Random Forest Regressor Imported successfully
2024-02-04 19:06:08,898:INFO:Starting cross validation
2024-02-04 19:06:08,903:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:10,654:INFO:Calculating mean and std
2024-02-04 19:06:10,654:INFO:Creating metrics dataframe
2024-02-04 19:06:10,657:INFO:Uploading results into container
2024-02-04 19:06:10,657:INFO:Uploading model into container now
2024-02-04 19:06:10,657:INFO:_master_model_container: 13
2024-02-04 19:06:10,657:INFO:_display_container: 2
2024-02-04 19:06:10,657:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:06:10,657:INFO:create_model() successfully completed......................................
2024-02-04 19:06:10,750:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:10,750:INFO:Creating metrics dataframe
2024-02-04 19:06:10,750:INFO:Initializing Extra Trees Regressor
2024-02-04 19:06:10,750:INFO:Total runtime is 0.19132637580235798 minutes
2024-02-04 19:06:10,763:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:10,763:INFO:Initializing create_model()
2024-02-04 19:06:10,763:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=et, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:10,763:INFO:Checking exceptions
2024-02-04 19:06:10,763:INFO:Importing libraries
2024-02-04 19:06:10,763:INFO:Copying training dataset
2024-02-04 19:06:10,768:INFO:Defining folds
2024-02-04 19:06:10,768:INFO:Declaring metric variables
2024-02-04 19:06:10,768:INFO:Importing untrained model
2024-02-04 19:06:10,774:INFO:Extra Trees Regressor Imported successfully
2024-02-04 19:06:10,782:INFO:Starting cross validation
2024-02-04 19:06:10,782:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:12,025:INFO:Calculating mean and std
2024-02-04 19:06:12,025:INFO:Creating metrics dataframe
2024-02-04 19:06:12,028:INFO:Uploading results into container
2024-02-04 19:06:12,029:INFO:Uploading model into container now
2024-02-04 19:06:12,029:INFO:_master_model_container: 14
2024-02-04 19:06:12,029:INFO:_display_container: 2
2024-02-04 19:06:12,029:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:06:12,029:INFO:create_model() successfully completed......................................
2024-02-04 19:06:12,117:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:12,117:INFO:Creating metrics dataframe
2024-02-04 19:06:12,120:INFO:Initializing AdaBoost Regressor
2024-02-04 19:06:12,120:INFO:Total runtime is 0.21416183312733966 minutes
2024-02-04 19:06:12,120:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:12,120:INFO:Initializing create_model()
2024-02-04 19:06:12,120:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=ada, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:12,120:INFO:Checking exceptions
2024-02-04 19:06:12,120:INFO:Importing libraries
2024-02-04 19:06:12,129:INFO:Copying training dataset
2024-02-04 19:06:12,129:INFO:Defining folds
2024-02-04 19:06:12,129:INFO:Declaring metric variables
2024-02-04 19:06:12,137:INFO:Importing untrained model
2024-02-04 19:06:12,140:INFO:AdaBoost Regressor Imported successfully
2024-02-04 19:06:12,146:INFO:Starting cross validation
2024-02-04 19:06:12,149:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:12,675:INFO:Calculating mean and std
2024-02-04 19:06:12,675:INFO:Creating metrics dataframe
2024-02-04 19:06:12,675:INFO:Uploading results into container
2024-02-04 19:06:12,675:INFO:Uploading model into container now
2024-02-04 19:06:12,675:INFO:_master_model_container: 15
2024-02-04 19:06:12,675:INFO:_display_container: 2
2024-02-04 19:06:12,675:INFO:AdaBoostRegressor(random_state=123)
2024-02-04 19:06:12,675:INFO:create_model() successfully completed......................................
2024-02-04 19:06:12,764:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:12,764:INFO:Creating metrics dataframe
2024-02-04 19:06:12,772:INFO:Initializing Gradient Boosting Regressor
2024-02-04 19:06:12,772:INFO:Total runtime is 0.22501918474833169 minutes
2024-02-04 19:06:12,779:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:12,782:INFO:Initializing create_model()
2024-02-04 19:06:12,782:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=gbr, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:12,782:INFO:Checking exceptions
2024-02-04 19:06:12,782:INFO:Importing libraries
2024-02-04 19:06:12,782:INFO:Copying training dataset
2024-02-04 19:06:12,782:INFO:Defining folds
2024-02-04 19:06:12,782:INFO:Declaring metric variables
2024-02-04 19:06:12,787:INFO:Importing untrained model
2024-02-04 19:06:12,791:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:06:12,796:INFO:Starting cross validation
2024-02-04 19:06:12,801:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:13,558:INFO:Calculating mean and std
2024-02-04 19:06:13,558:INFO:Creating metrics dataframe
2024-02-04 19:06:13,558:INFO:Uploading results into container
2024-02-04 19:06:13,558:INFO:Uploading model into container now
2024-02-04 19:06:13,558:INFO:_master_model_container: 16
2024-02-04 19:06:13,558:INFO:_display_container: 2
2024-02-04 19:06:13,558:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:06:13,558:INFO:create_model() successfully completed......................................
2024-02-04 19:06:13,644:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:13,644:INFO:Creating metrics dataframe
2024-02-04 19:06:13,664:INFO:Initializing Light Gradient Boosting Machine
2024-02-04 19:06:13,664:INFO:Total runtime is 0.23988552888234455 minutes
2024-02-04 19:06:13,666:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:13,666:INFO:Initializing create_model()
2024-02-04 19:06:13,666:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lightgbm, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:13,666:INFO:Checking exceptions
2024-02-04 19:06:13,666:INFO:Importing libraries
2024-02-04 19:06:13,666:INFO:Copying training dataset
2024-02-04 19:06:13,666:INFO:Defining folds
2024-02-04 19:06:13,666:INFO:Declaring metric variables
2024-02-04 19:06:13,675:INFO:Importing untrained model
2024-02-04 19:06:13,679:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-04 19:06:13,683:INFO:Starting cross validation
2024-02-04 19:06:13,683:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:14,462:INFO:Calculating mean and std
2024-02-04 19:06:14,462:INFO:Creating metrics dataframe
2024-02-04 19:06:14,468:INFO:Uploading results into container
2024-02-04 19:06:14,468:INFO:Uploading model into container now
2024-02-04 19:06:14,468:INFO:_master_model_container: 17
2024-02-04 19:06:14,468:INFO:_display_container: 2
2024-02-04 19:06:14,468:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:06:14,468:INFO:create_model() successfully completed......................................
2024-02-04 19:06:14,584:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:14,584:INFO:Creating metrics dataframe
2024-02-04 19:06:14,584:INFO:Initializing Dummy Regressor
2024-02-04 19:06:14,584:INFO:Total runtime is 0.25522610743840535 minutes
2024-02-04 19:06:14,584:INFO:SubProcess create_model() called ==================================
2024-02-04 19:06:14,584:INFO:Initializing create_model()
2024-02-04 19:06:14,584:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=dummy, fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA74630A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:14,584:INFO:Checking exceptions
2024-02-04 19:06:14,596:INFO:Importing libraries
2024-02-04 19:06:14,596:INFO:Copying training dataset
2024-02-04 19:06:14,601:INFO:Defining folds
2024-02-04 19:06:14,601:INFO:Declaring metric variables
2024-02-04 19:06:14,606:INFO:Importing untrained model
2024-02-04 19:06:14,606:INFO:Dummy Regressor Imported successfully
2024-02-04 19:06:14,615:INFO:Starting cross validation
2024-02-04 19:06:14,615:INFO:Cross validating with KFold(n_splits=5, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:14,928:INFO:Calculating mean and std
2024-02-04 19:06:14,930:INFO:Creating metrics dataframe
2024-02-04 19:06:14,933:INFO:Uploading results into container
2024-02-04 19:06:14,934:INFO:Uploading model into container now
2024-02-04 19:06:14,934:INFO:_master_model_container: 18
2024-02-04 19:06:14,934:INFO:_display_container: 2
2024-02-04 19:06:14,934:INFO:DummyRegressor()
2024-02-04 19:06:14,934:INFO:create_model() successfully completed......................................
2024-02-04 19:06:15,024:INFO:SubProcess create_model() end ==================================
2024-02-04 19:06:15,024:INFO:Creating metrics dataframe
2024-02-04 19:06:15,048:INFO:Initializing create_model()
2024-02-04 19:06:15,048:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=5, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:15,048:INFO:Checking exceptions
2024-02-04 19:06:15,050:INFO:Importing libraries
2024-02-04 19:06:15,050:INFO:Copying training dataset
2024-02-04 19:06:15,050:INFO:Defining folds
2024-02-04 19:06:15,050:INFO:Declaring metric variables
2024-02-04 19:06:15,050:INFO:Importing untrained model
2024-02-04 19:06:15,050:INFO:Declaring custom model
2024-02-04 19:06:15,050:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:06:15,050:INFO:Cross validation set to False
2024-02-04 19:06:15,050:INFO:Fitting Model
2024-02-04 19:06:15,610:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:06:15,610:INFO:create_model() successfully completed......................................
2024-02-04 19:06:15,712:INFO:Creating Dashboard logs
2024-02-04 19:06:15,715:INFO:Model: Gradient Boosting Regressor
2024-02-04 19:06:15,734:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-04 19:06:15,804:INFO:Initializing predict_model()
2024-02-04 19:06:15,804:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECCF0FC040>)
2024-02-04 19:06:15,804:INFO:Checking exceptions
2024-02-04 19:06:15,804:INFO:Preloading libraries
2024-02-04 19:06:17,026:INFO:Creating Dashboard logs
2024-02-04 19:06:17,042:INFO:Model: Light Gradient Boosting Machine
2024-02-04 19:06:17,058:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2024-02-04 19:06:17,229:INFO:Creating Dashboard logs
2024-02-04 19:06:17,229:INFO:Model: Extra Trees Regressor
2024-02-04 19:06:17,256:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-02-04 19:06:17,426:INFO:Creating Dashboard logs
2024-02-04 19:06:17,426:INFO:Model: Random Forest Regressor
2024-02-04 19:06:17,450:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-02-04 19:06:17,613:INFO:Creating Dashboard logs
2024-02-04 19:06:17,613:INFO:Model: Ridge Regression
2024-02-04 19:06:17,628:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}
2024-02-04 19:06:17,813:INFO:Creating Dashboard logs
2024-02-04 19:06:17,813:INFO:Model: Lasso Least Angle Regression
2024-02-04 19:06:17,829:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'max_iter': 500, 'normalize': 'deprecated', 'positive': False, 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2024-02-04 19:06:17,996:INFO:Creating Dashboard logs
2024-02-04 19:06:18,001:INFO:Model: Lasso Regression
2024-02-04 19:06:18,022:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2024-02-04 19:06:18,194:INFO:Creating Dashboard logs
2024-02-04 19:06:18,197:INFO:Model: Elastic Net
2024-02-04 19:06:18,210:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2024-02-04 19:06:18,397:INFO:Creating Dashboard logs
2024-02-04 19:06:18,397:INFO:Model: Bayesian Ridge
2024-02-04 19:06:18,419:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'n_iter': 300, 'tol': 0.001, 'verbose': False}
2024-02-04 19:06:18,571:INFO:Creating Dashboard logs
2024-02-04 19:06:18,571:INFO:Model: AdaBoost Regressor
2024-02-04 19:06:18,599:INFO:Logged params: {'base_estimator': 'deprecated', 'estimator': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50, 'random_state': 123}
2024-02-04 19:06:18,762:INFO:Creating Dashboard logs
2024-02-04 19:06:18,770:INFO:Model: Orthogonal Matching Pursuit
2024-02-04 19:06:18,789:INFO:Logged params: {'fit_intercept': True, 'n_nonzero_coefs': None, 'normalize': 'deprecated', 'precompute': 'auto', 'tol': None}
2024-02-04 19:06:18,944:INFO:Creating Dashboard logs
2024-02-04 19:06:18,944:INFO:Model: Decision Tree Regressor
2024-02-04 19:06:18,976:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 123, 'splitter': 'best'}
2024-02-04 19:06:19,141:INFO:Creating Dashboard logs
2024-02-04 19:06:19,141:INFO:Model: Huber Regressor
2024-02-04 19:06:19,157:INFO:Logged params: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': True, 'max_iter': 100, 'tol': 1e-05, 'warm_start': False}
2024-02-04 19:06:19,308:INFO:Creating Dashboard logs
2024-02-04 19:06:19,324:INFO:Model: K Neighbors Regressor
2024-02-04 19:06:19,333:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2024-02-04 19:06:19,507:INFO:Creating Dashboard logs
2024-02-04 19:06:19,507:INFO:Model: Passive Aggressive Regressor
2024-02-04 19:06:19,523:INFO:Logged params: {'C': 1.0, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'fit_intercept': True, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'n_iter_no_change': 5, 'random_state': 123, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-04 19:06:19,694:INFO:Creating Dashboard logs
2024-02-04 19:06:19,711:INFO:Model: Dummy Regressor
2024-02-04 19:06:19,729:INFO:Logged params: {'constant': None, 'quantile': None, 'strategy': 'mean'}
2024-02-04 19:06:19,881:INFO:Creating Dashboard logs
2024-02-04 19:06:19,881:INFO:Model: Linear Regression
2024-02-04 19:06:19,893:INFO:Logged params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'positive': False}
2024-02-04 19:06:20,060:INFO:Creating Dashboard logs
2024-02-04 19:06:20,066:INFO:Model: Least Angle Regression
2024-02-04 19:06:20,076:INFO:Logged params: {'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'n_nonzero_coefs': 500, 'normalize': 'deprecated', 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2024-02-04 19:06:20,246:INFO:_master_model_container: 18
2024-02-04 19:06:20,246:INFO:_display_container: 2
2024-02-04 19:06:20,246:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:06:20,246:INFO:compare_models() successfully completed......................................
2024-02-04 19:06:32,022:INFO:Initializing plot_model()
2024-02-04 19:06:32,022:INFO:plot_model(plot=residuals, fold=None, verbose=True, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, system=True)
2024-02-04 19:06:32,022:INFO:Checking exceptions
2024-02-04 19:06:32,026:INFO:Preloading libraries
2024-02-04 19:06:32,034:INFO:Copying training dataset
2024-02-04 19:06:32,034:INFO:Plot type: residuals
2024-02-04 19:06:32,428:INFO:Fitting Model
2024-02-04 19:06:32,428:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names
  warnings.warn(

2024-02-04 19:06:32,455:INFO:Scoring test/hold-out set
2024-02-04 19:06:32,755:INFO:Visual Rendered Successfully
2024-02-04 19:06:32,855:INFO:plot_model() successfully completed......................................
2024-02-04 19:06:35,517:INFO:Initializing plot_model()
2024-02-04 19:06:35,517:INFO:plot_model(plot=error, fold=None, verbose=True, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, system=True)
2024-02-04 19:06:35,517:INFO:Checking exceptions
2024-02-04 19:06:35,517:INFO:Preloading libraries
2024-02-04 19:06:35,523:INFO:Copying training dataset
2024-02-04 19:06:35,523:INFO:Plot type: error
2024-02-04 19:06:35,906:INFO:Fitting Model
2024-02-04 19:06:35,906:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\base.py:439: UserWarning: X does not have valid feature names, but GradientBoostingRegressor was fitted with feature names
  warnings.warn(

2024-02-04 19:06:35,906:INFO:Scoring test/hold-out set
2024-02-04 19:06:36,055:INFO:Visual Rendered Successfully
2024-02-04 19:06:36,156:INFO:plot_model() successfully completed......................................
2024-02-04 19:06:38,457:INFO:Initializing plot_model()
2024-02-04 19:06:38,457:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=GradientBoostingRegressor(random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, system=True)
2024-02-04 19:06:38,457:INFO:Checking exceptions
2024-02-04 19:06:38,457:INFO:Preloading libraries
2024-02-04 19:06:38,465:INFO:Copying training dataset
2024-02-04 19:06:38,465:INFO:Plot type: feature
2024-02-04 19:06:38,465:WARNING:No coef_ found. Trying feature_importances_
2024-02-04 19:06:38,609:INFO:Visual Rendered Successfully
2024-02-04 19:06:38,706:INFO:plot_model() successfully completed......................................
2024-02-04 19:06:48,738:INFO:Initializing create_model()
2024-02-04 19:06:48,745:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:48,746:INFO:Checking exceptions
2024-02-04 19:06:48,754:INFO:Importing libraries
2024-02-04 19:06:48,754:INFO:Copying training dataset
2024-02-04 19:06:48,763:INFO:Defining folds
2024-02-04 19:06:48,763:INFO:Declaring metric variables
2024-02-04 19:06:48,763:INFO:Importing untrained model
2024-02-04 19:06:48,763:INFO:Declaring custom model
2024-02-04 19:06:48,763:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:06:48,779:INFO:Starting cross validation
2024-02-04 19:06:48,779:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:50,626:INFO:Calculating mean and std
2024-02-04 19:06:50,626:INFO:Creating metrics dataframe
2024-02-04 19:06:50,629:INFO:Finalizing model
2024-02-04 19:06:51,183:INFO:Creating Dashboard logs
2024-02-04 19:06:51,183:INFO:Model: Gradient Boosting Regressor
2024-02-04 19:06:51,212:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-04 19:06:51,261:INFO:Initializing predict_model()
2024-02-04 19:06:51,261:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECCED87040>)
2024-02-04 19:06:51,261:INFO:Checking exceptions
2024-02-04 19:06:51,261:INFO:Preloading libraries
2024-02-04 19:06:51,655:INFO:Uploading results into container
2024-02-04 19:06:51,655:INFO:Uploading model into container now
2024-02-04 19:06:51,664:INFO:_master_model_container: 19
2024-02-04 19:06:51,664:INFO:_display_container: 3
2024-02-04 19:06:51,664:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:06:51,664:INFO:create_model() successfully completed......................................
2024-02-04 19:06:55,874:INFO:Initializing create_model()
2024-02-04 19:06:55,874:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lightgbm, fold=3, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:06:55,874:INFO:Checking exceptions
2024-02-04 19:06:55,883:INFO:Importing libraries
2024-02-04 19:06:55,883:INFO:Copying training dataset
2024-02-04 19:06:55,883:INFO:Defining folds
2024-02-04 19:06:55,883:INFO:Declaring metric variables
2024-02-04 19:06:55,883:INFO:Importing untrained model
2024-02-04 19:06:55,898:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-04 19:06:55,900:INFO:Starting cross validation
2024-02-04 19:06:55,900:INFO:Cross validating with KFold(n_splits=3, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:06:56,497:INFO:Calculating mean and std
2024-02-04 19:06:56,497:INFO:Creating metrics dataframe
2024-02-04 19:06:56,499:INFO:Finalizing model
2024-02-04 19:06:56,662:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-02-04 19:06:56,662:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000441 seconds.
2024-02-04 19:06:56,662:INFO:You can set `force_row_wise=true` to remove the overhead.
2024-02-04 19:06:56,662:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2024-02-04 19:06:56,662:INFO:[LightGBM] [Info] Total Bins 2881
2024-02-04 19:06:56,662:INFO:[LightGBM] [Info] Number of data points in the train set: 1019, number of used features: 93
2024-02-04 19:06:56,662:INFO:[LightGBM] [Info] Start training from score 180679.625123
2024-02-04 19:06:56,736:INFO:Creating Dashboard logs
2024-02-04 19:06:56,753:INFO:Model: Light Gradient Boosting Machine
2024-02-04 19:06:56,781:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2024-02-04 19:06:56,852:INFO:Initializing predict_model()
2024-02-04 19:06:56,852:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECCED87040>)
2024-02-04 19:06:56,852:INFO:Checking exceptions
2024-02-04 19:06:56,852:INFO:Preloading libraries
2024-02-04 19:06:57,312:INFO:Uploading results into container
2024-02-04 19:06:57,316:INFO:Uploading model into container now
2024-02-04 19:06:57,317:INFO:_master_model_container: 20
2024-02-04 19:06:57,317:INFO:_display_container: 4
2024-02-04 19:06:57,317:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:06:57,317:INFO:create_model() successfully completed......................................
2024-02-04 19:07:02,697:INFO:Initializing compare_models()
2024-02-04 19:07:02,697:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, include=['lightgbm', 'gbr', 'br'], fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, 'include': ['lightgbm', 'gbr', 'br'], 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2024-02-04 19:07:02,697:INFO:Checking exceptions
2024-02-04 19:07:02,705:INFO:Preparing display monitor
2024-02-04 19:07:02,721:INFO:Initializing Light Gradient Boosting Machine
2024-02-04 19:07:02,721:INFO:Total runtime is 0.0 minutes
2024-02-04 19:07:02,721:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:02,729:INFO:Initializing create_model()
2024-02-04 19:07:02,729:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECC8890700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:02,729:INFO:Checking exceptions
2024-02-04 19:07:02,729:INFO:Importing libraries
2024-02-04 19:07:02,729:INFO:Copying training dataset
2024-02-04 19:07:02,729:INFO:Defining folds
2024-02-04 19:07:02,729:INFO:Declaring metric variables
2024-02-04 19:07:02,729:INFO:Importing untrained model
2024-02-04 19:07:02,738:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-04 19:07:02,745:INFO:Starting cross validation
2024-02-04 19:07:02,745:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:04,499:INFO:Calculating mean and std
2024-02-04 19:07:04,499:INFO:Creating metrics dataframe
2024-02-04 19:07:04,505:INFO:Uploading results into container
2024-02-04 19:07:04,505:INFO:Uploading model into container now
2024-02-04 19:07:04,505:INFO:_master_model_container: 21
2024-02-04 19:07:04,505:INFO:_display_container: 5
2024-02-04 19:07:04,505:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:07:04,505:INFO:create_model() successfully completed......................................
2024-02-04 19:07:04,606:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:04,606:INFO:Creating metrics dataframe
2024-02-04 19:07:04,617:INFO:Initializing Gradient Boosting Regressor
2024-02-04 19:07:04,617:INFO:Total runtime is 0.03159751097361247 minutes
2024-02-04 19:07:04,617:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:04,617:INFO:Initializing create_model()
2024-02-04 19:07:04,617:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECC8890700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:04,617:INFO:Checking exceptions
2024-02-04 19:07:04,617:INFO:Importing libraries
2024-02-04 19:07:04,617:INFO:Copying training dataset
2024-02-04 19:07:04,617:INFO:Defining folds
2024-02-04 19:07:04,617:INFO:Declaring metric variables
2024-02-04 19:07:04,617:INFO:Importing untrained model
2024-02-04 19:07:04,635:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:07:04,643:INFO:Starting cross validation
2024-02-04 19:07:04,643:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:06,468:INFO:Calculating mean and std
2024-02-04 19:07:06,468:INFO:Creating metrics dataframe
2024-02-04 19:07:06,472:INFO:Uploading results into container
2024-02-04 19:07:06,472:INFO:Uploading model into container now
2024-02-04 19:07:06,472:INFO:_master_model_container: 22
2024-02-04 19:07:06,472:INFO:_display_container: 5
2024-02-04 19:07:06,472:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:07:06,472:INFO:create_model() successfully completed......................................
2024-02-04 19:07:06,561:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:06,561:INFO:Creating metrics dataframe
2024-02-04 19:07:06,580:INFO:Initializing Bayesian Ridge
2024-02-04 19:07:06,580:INFO:Total runtime is 0.0643048405647278 minutes
2024-02-04 19:07:06,580:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:06,580:INFO:Initializing create_model()
2024-02-04 19:07:06,580:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECC8890700>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:06,580:INFO:Checking exceptions
2024-02-04 19:07:06,580:INFO:Importing libraries
2024-02-04 19:07:06,580:INFO:Copying training dataset
2024-02-04 19:07:06,588:INFO:Defining folds
2024-02-04 19:07:06,588:INFO:Declaring metric variables
2024-02-04 19:07:06,594:INFO:Importing untrained model
2024-02-04 19:07:06,594:INFO:Bayesian Ridge Imported successfully
2024-02-04 19:07:06,594:INFO:Starting cross validation
2024-02-04 19:07:06,605:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:07,271:INFO:Calculating mean and std
2024-02-04 19:07:07,271:INFO:Creating metrics dataframe
2024-02-04 19:07:07,271:INFO:Uploading results into container
2024-02-04 19:07:07,271:INFO:Uploading model into container now
2024-02-04 19:07:07,271:INFO:_master_model_container: 23
2024-02-04 19:07:07,271:INFO:_display_container: 5
2024-02-04 19:07:07,271:INFO:BayesianRidge()
2024-02-04 19:07:07,271:INFO:create_model() successfully completed......................................
2024-02-04 19:07:07,362:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:07,362:INFO:Creating metrics dataframe
2024-02-04 19:07:07,368:INFO:Initializing create_model()
2024-02-04 19:07:07,368:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:07,368:INFO:Checking exceptions
2024-02-04 19:07:07,368:INFO:Importing libraries
2024-02-04 19:07:07,368:INFO:Copying training dataset
2024-02-04 19:07:07,384:INFO:Defining folds
2024-02-04 19:07:07,384:INFO:Declaring metric variables
2024-02-04 19:07:07,384:INFO:Importing untrained model
2024-02-04 19:07:07,384:INFO:Declaring custom model
2024-02-04 19:07:07,384:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:07:07,384:INFO:Cross validation set to False
2024-02-04 19:07:07,384:INFO:Fitting Model
2024-02-04 19:07:07,918:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:07:07,918:INFO:create_model() successfully completed......................................
2024-02-04 19:07:08,012:INFO:Creating Dashboard logs
2024-02-04 19:07:08,014:INFO:Model: Gradient Boosting Regressor
2024-02-04 19:07:08,038:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-04 19:07:08,095:INFO:Initializing predict_model()
2024-02-04 19:07:08,095:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECCED87430>)
2024-02-04 19:07:08,095:INFO:Checking exceptions
2024-02-04 19:07:08,095:INFO:Preloading libraries
2024-02-04 19:07:08,475:INFO:Creating Dashboard logs
2024-02-04 19:07:08,475:INFO:Model: Light Gradient Boosting Machine
2024-02-04 19:07:08,506:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2024-02-04 19:07:08,664:INFO:Creating Dashboard logs
2024-02-04 19:07:08,664:INFO:Model: Bayesian Ridge
2024-02-04 19:07:08,693:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'n_iter': 300, 'tol': 0.001, 'verbose': False}
2024-02-04 19:07:08,870:INFO:_master_model_container: 23
2024-02-04 19:07:08,870:INFO:_display_container: 5
2024-02-04 19:07:08,870:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:07:08,870:INFO:compare_models() successfully completed......................................
2024-02-04 19:07:19,861:INFO:Initializing compare_models()
2024-02-04 19:07:19,861:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, include=None, fold=None, round=4, cross_validation=True, sort=MAE, n_select=3, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MAE', 'n_select': 3, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2024-02-04 19:07:19,861:INFO:Checking exceptions
2024-02-04 19:07:19,865:INFO:Preparing display monitor
2024-02-04 19:07:19,884:INFO:Initializing Linear Regression
2024-02-04 19:07:19,884:INFO:Total runtime is 0.0 minutes
2024-02-04 19:07:19,884:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:19,884:INFO:Initializing create_model()
2024-02-04 19:07:19,884:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:19,884:INFO:Checking exceptions
2024-02-04 19:07:19,884:INFO:Importing libraries
2024-02-04 19:07:19,884:INFO:Copying training dataset
2024-02-04 19:07:19,889:INFO:Defining folds
2024-02-04 19:07:19,889:INFO:Declaring metric variables
2024-02-04 19:07:19,889:INFO:Importing untrained model
2024-02-04 19:07:19,896:INFO:Linear Regression Imported successfully
2024-02-04 19:07:19,905:INFO:Starting cross validation
2024-02-04 19:07:19,908:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:20,588:INFO:Calculating mean and std
2024-02-04 19:07:20,588:INFO:Creating metrics dataframe
2024-02-04 19:07:20,590:INFO:Uploading results into container
2024-02-04 19:07:20,590:INFO:Uploading model into container now
2024-02-04 19:07:20,590:INFO:_master_model_container: 24
2024-02-04 19:07:20,590:INFO:_display_container: 6
2024-02-04 19:07:20,590:INFO:LinearRegression(n_jobs=-1)
2024-02-04 19:07:20,590:INFO:create_model() successfully completed......................................
2024-02-04 19:07:20,697:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:20,697:INFO:Creating metrics dataframe
2024-02-04 19:07:20,697:INFO:Initializing Lasso Regression
2024-02-04 19:07:20,697:INFO:Total runtime is 0.013545691967010498 minutes
2024-02-04 19:07:20,706:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:20,707:INFO:Initializing create_model()
2024-02-04 19:07:20,707:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:20,707:INFO:Checking exceptions
2024-02-04 19:07:20,707:INFO:Importing libraries
2024-02-04 19:07:20,707:INFO:Copying training dataset
2024-02-04 19:07:20,707:INFO:Defining folds
2024-02-04 19:07:20,707:INFO:Declaring metric variables
2024-02-04 19:07:20,707:INFO:Importing untrained model
2024-02-04 19:07:20,707:INFO:Lasso Regression Imported successfully
2024-02-04 19:07:20,723:INFO:Starting cross validation
2024-02-04 19:07:20,723:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:21,123:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.799e+11, tolerance: 5.738e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,140:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.569e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,156:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.692e+11, tolerance: 5.599e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,157:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.241e+11, tolerance: 5.445e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,157:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.563e+11, tolerance: 5.378e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,186:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.631e+11, tolerance: 5.696e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,186:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.657e+11, tolerance: 5.662e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,208:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.375e+11, tolerance: 5.336e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,423:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.674e+11, tolerance: 5.647e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,423:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.671e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:21,473:INFO:Calculating mean and std
2024-02-04 19:07:21,473:INFO:Creating metrics dataframe
2024-02-04 19:07:21,473:INFO:Uploading results into container
2024-02-04 19:07:21,473:INFO:Uploading model into container now
2024-02-04 19:07:21,473:INFO:_master_model_container: 25
2024-02-04 19:07:21,473:INFO:_display_container: 6
2024-02-04 19:07:21,473:INFO:Lasso(random_state=123)
2024-02-04 19:07:21,473:INFO:create_model() successfully completed......................................
2024-02-04 19:07:21,557:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:21,557:INFO:Creating metrics dataframe
2024-02-04 19:07:21,576:INFO:Initializing Ridge Regression
2024-02-04 19:07:21,576:INFO:Total runtime is 0.02819267908732096 minutes
2024-02-04 19:07:21,576:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:21,576:INFO:Initializing create_model()
2024-02-04 19:07:21,576:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:21,576:INFO:Checking exceptions
2024-02-04 19:07:21,576:INFO:Importing libraries
2024-02-04 19:07:21,576:INFO:Copying training dataset
2024-02-04 19:07:21,576:INFO:Defining folds
2024-02-04 19:07:21,576:INFO:Declaring metric variables
2024-02-04 19:07:21,586:INFO:Importing untrained model
2024-02-04 19:07:21,590:INFO:Ridge Regression Imported successfully
2024-02-04 19:07:21,592:INFO:Starting cross validation
2024-02-04 19:07:21,592:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:22,169:INFO:Calculating mean and std
2024-02-04 19:07:22,169:INFO:Creating metrics dataframe
2024-02-04 19:07:22,173:INFO:Uploading results into container
2024-02-04 19:07:22,173:INFO:Uploading model into container now
2024-02-04 19:07:22,173:INFO:_master_model_container: 26
2024-02-04 19:07:22,173:INFO:_display_container: 6
2024-02-04 19:07:22,173:INFO:Ridge(random_state=123)
2024-02-04 19:07:22,173:INFO:create_model() successfully completed......................................
2024-02-04 19:07:22,257:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:22,257:INFO:Creating metrics dataframe
2024-02-04 19:07:22,257:INFO:Initializing Elastic Net
2024-02-04 19:07:22,270:INFO:Total runtime is 0.03976488510767619 minutes
2024-02-04 19:07:22,273:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:22,273:INFO:Initializing create_model()
2024-02-04 19:07:22,273:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:22,273:INFO:Checking exceptions
2024-02-04 19:07:22,273:INFO:Importing libraries
2024-02-04 19:07:22,273:INFO:Copying training dataset
2024-02-04 19:07:22,279:INFO:Defining folds
2024-02-04 19:07:22,279:INFO:Declaring metric variables
2024-02-04 19:07:22,279:INFO:Importing untrained model
2024-02-04 19:07:22,287:INFO:Elastic Net Imported successfully
2024-02-04 19:07:22,289:INFO:Starting cross validation
2024-02-04 19:07:22,289:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:22,687:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.761e+11, tolerance: 5.445e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,702:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.858e+11, tolerance: 5.662e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,718:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.863e+11, tolerance: 5.599e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,718:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.013e+11, tolerance: 5.738e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,718:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.889e+11, tolerance: 5.696e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,718:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.602e+11, tolerance: 5.336e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,733:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.721e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,749:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.723e+11, tolerance: 5.378e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,987:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.964e+11, tolerance: 5.647e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:22,987:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.975e+11, tolerance: 5.718e+08
  model = cd_fast.enet_coordinate_descent(

2024-02-04 19:07:23,065:INFO:Calculating mean and std
2024-02-04 19:07:23,065:INFO:Creating metrics dataframe
2024-02-04 19:07:23,065:INFO:Uploading results into container
2024-02-04 19:07:23,065:INFO:Uploading model into container now
2024-02-04 19:07:23,065:INFO:_master_model_container: 27
2024-02-04 19:07:23,065:INFO:_display_container: 6
2024-02-04 19:07:23,065:INFO:ElasticNet(random_state=123)
2024-02-04 19:07:23,065:INFO:create_model() successfully completed......................................
2024-02-04 19:07:23,154:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:23,154:INFO:Creating metrics dataframe
2024-02-04 19:07:23,154:INFO:Initializing Least Angle Regression
2024-02-04 19:07:23,154:INFO:Total runtime is 0.05449879964192708 minutes
2024-02-04 19:07:23,167:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:23,167:INFO:Initializing create_model()
2024-02-04 19:07:23,167:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:23,167:INFO:Checking exceptions
2024-02-04 19:07:23,167:INFO:Importing libraries
2024-02-04 19:07:23,167:INFO:Copying training dataset
2024-02-04 19:07:23,167:INFO:Defining folds
2024-02-04 19:07:23,167:INFO:Declaring metric variables
2024-02-04 19:07:23,176:INFO:Importing untrained model
2024-02-04 19:07:23,176:INFO:Least Angle Regression Imported successfully
2024-02-04 19:07:23,176:INFO:Starting cross validation
2024-02-04 19:07:23,187:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:23,456:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.718e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,456:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.851e+02, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,467:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.911e+02, with an active set of 99 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,467:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=3.621e+02, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,467:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.149e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,467:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 157 iterations, i.e. alpha=1.924e+05, with an active set of 109 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,467:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 160 iterations, i.e. alpha=1.780e+05, with an active set of 112 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,467:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=1.157e+03, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,467:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=5.555e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,483:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=4.731e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,483:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=2.519e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=1.562e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=3.774e+05, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=3.759e+05, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.496e+09, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=8.153e+04, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=5.167e+07, with an active set of 72 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=3.422e+07, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 83 iterations, i.e. alpha=2.687e+07, with an active set of 74 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=2.606e+07, with an active set of 75 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=2.275e+07, with an active set of 76 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.908e+07, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=1.562e+07, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=3.936e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.264e+07, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.442e+07, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 92 iterations, i.e. alpha=1.173e+07, with an active set of 82 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=1.137e+07, with an active set of 83 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 94 iterations, i.e. alpha=1.059e+07, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 55 iterations, i.e. alpha=2.412e+02, with an active set of 50 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=1.029e+07, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=9.833e+06, with an active set of 87 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 99 iterations, i.e. alpha=9.103e+06, with an active set of 88 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=8.950e+06, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=7.986e+06, with an active set of 90 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=7.476e+06, with an active set of 91 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,487:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 102 iterations, i.e. alpha=7.475e+06, with an active set of 91 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.243e+06, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.242e+06, with an active set of 93 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=6.703e+06, with an active set of 94 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=5.425e+06, with an active set of 96 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=5.332e+06, with an active set of 97 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=4.831e+06, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=4.529e+06, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=3.694e+06, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=3.221e+06, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 114 iterations, i.e. alpha=2.801e+06, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 115 iterations, i.e. alpha=2.370e+06, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=7.244e+12, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 123 iterations, i.e. alpha=1.750e+11, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.543e+11, with an active set of 107 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.062e+11, with an active set of 107 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.027e+11, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=7.904e+10, with an active set of 108 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=7.308e+10, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=6.501e+10, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=5.857e+10, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=5.713e+10, with an active set of 111 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=5.810e+10, with an active set of 113 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=5.809e+10, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,499:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=3.100e+10, with an active set of 114 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,514:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=9.162e+13, with an active set of 131 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,514:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=7.581e+13, with an active set of 131 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,514:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.051e+02, with an active set of 72 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,530:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=2.116e+02, with an active set of 116 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,530:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=1.924e+02, with an active set of 116 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,539:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 146 iterations, i.e. alpha=1.674e+02, with an active set of 121 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,539:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=1.569e+02, with an active set of 124 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,544:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=1.388e+03, with an active set of 128 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,544:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=1.232e+03, with an active set of 128 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,544:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=1.900e+02, with an active set of 128 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,544:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 159 iterations, i.e. alpha=7.168e+00, with an active set of 128 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=3.745e+02, with an active set of 43 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.029e+02, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=5.739e+02, with an active set of 38 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=2.546e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.190e+02, with an active set of 73 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=6.923e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=5.772e+01, with an active set of 88 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=3.716e+01, with an active set of 97 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=5.669e+03, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=2.536e+03, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=1.472e+02, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=3.202e+01, with an active set of 98 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.832e+01, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=2.695e+01, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=1.943e+01, with an active set of 107 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=1.431e+01, with an active set of 108 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 122 iterations, i.e. alpha=1.773e+01, with an active set of 111 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=1.219e+01, with an active set of 116 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,720:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=1.102e+01, with an active set of 116 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,736:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 144 iterations, i.e. alpha=4.280e+01, with an active set of 125 regressors, and the smallest cholesky pivot element being 9.714e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,736:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 150 iterations, i.e. alpha=3.768e+02, with an active set of 128 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,736:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=1.298e+04, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,736:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=7.540e+01, with an active set of 129 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:23,792:INFO:Calculating mean and std
2024-02-04 19:07:23,792:INFO:Creating metrics dataframe
2024-02-04 19:07:23,792:INFO:Uploading results into container
2024-02-04 19:07:23,792:INFO:Uploading model into container now
2024-02-04 19:07:23,792:INFO:_master_model_container: 28
2024-02-04 19:07:23,792:INFO:_display_container: 6
2024-02-04 19:07:23,792:INFO:Lars(random_state=123)
2024-02-04 19:07:23,792:INFO:create_model() successfully completed......................................
2024-02-04 19:07:23,871:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:23,871:INFO:Creating metrics dataframe
2024-02-04 19:07:23,887:INFO:Initializing Lasso Least Angle Regression
2024-02-04 19:07:23,887:INFO:Total runtime is 0.0667116363843282 minutes
2024-02-04 19:07:23,895:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:23,895:INFO:Initializing create_model()
2024-02-04 19:07:23,895:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:23,895:INFO:Checking exceptions
2024-02-04 19:07:23,895:INFO:Importing libraries
2024-02-04 19:07:23,895:INFO:Copying training dataset
2024-02-04 19:07:23,895:INFO:Defining folds
2024-02-04 19:07:23,895:INFO:Declaring metric variables
2024-02-04 19:07:23,895:INFO:Importing untrained model
2024-02-04 19:07:23,907:INFO:Lasso Least Angle Regression Imported successfully
2024-02-04 19:07:23,912:INFO:Starting cross validation
2024-02-04 19:07:23,912:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:24,218:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=7.153e+03, with an active set of 18 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,218:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.274e+03, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,218:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=2.353e+03, previous alpha=1.302e+03, with an active set of 29 regressors.
  warnings.warn(

2024-02-04 19:07:24,234:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=3.340e+02, with an active set of 46 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,234:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=3.955e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,234:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=1.603e+02, with an active set of 69 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,234:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 70 iterations, alpha=2.179e+02, previous alpha=2.179e+02, with an active set of 57 regressors.
  warnings.warn(

2024-02-04 19:07:24,234:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 86 iterations, alpha=1.479e+02, previous alpha=1.479e+02, with an active set of 71 regressors.
  warnings.warn(

2024-02-04 19:07:24,260:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=4.102e+02, with an active set of 45 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,260:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=4.005e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,260:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=3.697e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,260:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.087e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,260:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=1.416e+02, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=4.274e+00, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 146 iterations, i.e. alpha=1.586e+00, with an active set of 116 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 147 iterations, i.e. alpha=1.586e+00, with an active set of 117 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 151 iterations, alpha=1.419e+00, previous alpha=1.419e+00, with an active set of 118 regressors.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 97 iterations, i.e. alpha=8.289e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=8.289e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 101 iterations, alpha=8.102e+01, previous alpha=8.102e+01, with an active set of 80 regressors.
  warnings.warn(

2024-02-04 19:07:24,274:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 111 iterations, alpha=7.889e+01, previous alpha=5.351e+01, with an active set of 88 regressors.
  warnings.warn(

2024-02-04 19:07:24,290:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=4.426e+02, with an active set of 42 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,290:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 90 iterations, i.e. alpha=1.234e+02, with an active set of 68 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,290:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.522e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,290:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.795e+02, with an active set of 63 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,306:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=6.685e+01, with an active set of 79 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,306:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=6.615e+01, with an active set of 80 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,306:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 103 iterations, alpha=5.927e+01, previous alpha=5.849e+01, with an active set of 82 regressors.
  warnings.warn(

2024-02-04 19:07:24,306:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 140 iterations, alpha=2.414e+01, previous alpha=2.414e+01, with an active set of 95 regressors.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=4.325e+02, with an active set of 41 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=2.167e+02, with an active set of 54 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=5.168e+02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=4.459e+02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.380e+02, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=9.583e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=9.092e+01, with an active set of 78 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 89 iterations, alpha=8.936e+01, previous alpha=8.854e+01, with an active set of 78 regressors.
  warnings.warn(

2024-02-04 19:07:24,476:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:648: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.956e+02, with an active set of 57 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2024-02-04 19:07:24,488:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_least_angle.py:678: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 129 iterations, alpha=4.342e+01, previous alpha=3.885e+01, with an active set of 96 regressors.
  warnings.warn(

2024-02-04 19:07:24,535:INFO:Calculating mean and std
2024-02-04 19:07:24,535:INFO:Creating metrics dataframe
2024-02-04 19:07:24,535:INFO:Uploading results into container
2024-02-04 19:07:24,535:INFO:Uploading model into container now
2024-02-04 19:07:24,535:INFO:_master_model_container: 29
2024-02-04 19:07:24,535:INFO:_display_container: 6
2024-02-04 19:07:24,535:INFO:LassoLars(random_state=123)
2024-02-04 19:07:24,535:INFO:create_model() successfully completed......................................
2024-02-04 19:07:24,622:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:24,622:INFO:Creating metrics dataframe
2024-02-04 19:07:24,632:INFO:Initializing Orthogonal Matching Pursuit
2024-02-04 19:07:24,632:INFO:Total runtime is 0.0791207194328308 minutes
2024-02-04 19:07:24,632:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:24,632:INFO:Initializing create_model()
2024-02-04 19:07:24,632:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:24,639:INFO:Checking exceptions
2024-02-04 19:07:24,639:INFO:Importing libraries
2024-02-04 19:07:24,639:INFO:Copying training dataset
2024-02-04 19:07:24,642:INFO:Defining folds
2024-02-04 19:07:24,642:INFO:Declaring metric variables
2024-02-04 19:07:24,642:INFO:Importing untrained model
2024-02-04 19:07:24,642:INFO:Orthogonal Matching Pursuit Imported successfully
2024-02-04 19:07:24,656:INFO:Starting cross validation
2024-02-04 19:07:24,656:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:25,232:INFO:Calculating mean and std
2024-02-04 19:07:25,232:INFO:Creating metrics dataframe
2024-02-04 19:07:25,232:INFO:Uploading results into container
2024-02-04 19:07:25,232:INFO:Uploading model into container now
2024-02-04 19:07:25,232:INFO:_master_model_container: 30
2024-02-04 19:07:25,232:INFO:_display_container: 6
2024-02-04 19:07:25,232:INFO:OrthogonalMatchingPursuit()
2024-02-04 19:07:25,232:INFO:create_model() successfully completed......................................
2024-02-04 19:07:25,319:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:25,319:INFO:Creating metrics dataframe
2024-02-04 19:07:25,329:INFO:Initializing Bayesian Ridge
2024-02-04 19:07:25,329:INFO:Total runtime is 0.09075031280517577 minutes
2024-02-04 19:07:25,329:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:25,329:INFO:Initializing create_model()
2024-02-04 19:07:25,329:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:25,329:INFO:Checking exceptions
2024-02-04 19:07:25,329:INFO:Importing libraries
2024-02-04 19:07:25,329:INFO:Copying training dataset
2024-02-04 19:07:25,342:INFO:Defining folds
2024-02-04 19:07:25,342:INFO:Declaring metric variables
2024-02-04 19:07:25,342:INFO:Importing untrained model
2024-02-04 19:07:25,342:INFO:Bayesian Ridge Imported successfully
2024-02-04 19:07:25,342:INFO:Starting cross validation
2024-02-04 19:07:25,356:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:25,975:INFO:Calculating mean and std
2024-02-04 19:07:25,975:INFO:Creating metrics dataframe
2024-02-04 19:07:25,975:INFO:Uploading results into container
2024-02-04 19:07:25,975:INFO:Uploading model into container now
2024-02-04 19:07:25,975:INFO:_master_model_container: 31
2024-02-04 19:07:25,975:INFO:_display_container: 6
2024-02-04 19:07:25,975:INFO:BayesianRidge()
2024-02-04 19:07:25,975:INFO:create_model() successfully completed......................................
2024-02-04 19:07:26,062:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:26,062:INFO:Creating metrics dataframe
2024-02-04 19:07:26,073:INFO:Initializing Passive Aggressive Regressor
2024-02-04 19:07:26,073:INFO:Total runtime is 0.10313917795817057 minutes
2024-02-04 19:07:26,081:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:26,081:INFO:Initializing create_model()
2024-02-04 19:07:26,081:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:26,081:INFO:Checking exceptions
2024-02-04 19:07:26,081:INFO:Importing libraries
2024-02-04 19:07:26,081:INFO:Copying training dataset
2024-02-04 19:07:26,081:INFO:Defining folds
2024-02-04 19:07:26,081:INFO:Declaring metric variables
2024-02-04 19:07:26,090:INFO:Importing untrained model
2024-02-04 19:07:26,091:INFO:Passive Aggressive Regressor Imported successfully
2024-02-04 19:07:26,091:INFO:Starting cross validation
2024-02-04 19:07:26,091:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:26,675:INFO:Calculating mean and std
2024-02-04 19:07:26,675:INFO:Creating metrics dataframe
2024-02-04 19:07:26,675:INFO:Uploading results into container
2024-02-04 19:07:26,675:INFO:Uploading model into container now
2024-02-04 19:07:26,675:INFO:_master_model_container: 32
2024-02-04 19:07:26,675:INFO:_display_container: 6
2024-02-04 19:07:26,675:INFO:PassiveAggressiveRegressor(random_state=123)
2024-02-04 19:07:26,675:INFO:create_model() successfully completed......................................
2024-02-04 19:07:26,759:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:26,759:INFO:Creating metrics dataframe
2024-02-04 19:07:26,778:INFO:Initializing Huber Regressor
2024-02-04 19:07:26,778:INFO:Total runtime is 0.11488995552062987 minutes
2024-02-04 19:07:26,780:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:26,780:INFO:Initializing create_model()
2024-02-04 19:07:26,780:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:26,780:INFO:Checking exceptions
2024-02-04 19:07:26,780:INFO:Importing libraries
2024-02-04 19:07:26,780:INFO:Copying training dataset
2024-02-04 19:07:26,780:INFO:Defining folds
2024-02-04 19:07:26,780:INFO:Declaring metric variables
2024-02-04 19:07:26,780:INFO:Importing untrained model
2024-02-04 19:07:26,792:INFO:Huber Regressor Imported successfully
2024-02-04 19:07:26,792:INFO:Starting cross validation
2024-02-04 19:07:26,792:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:27,152:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,173:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,207:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,223:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,223:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,238:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,254:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,254:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,466:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,466:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\sklearn\linear_model\_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2024-02-04 19:07:27,522:INFO:Calculating mean and std
2024-02-04 19:07:27,522:INFO:Creating metrics dataframe
2024-02-04 19:07:27,528:INFO:Uploading results into container
2024-02-04 19:07:27,528:INFO:Uploading model into container now
2024-02-04 19:07:27,528:INFO:_master_model_container: 33
2024-02-04 19:07:27,528:INFO:_display_container: 6
2024-02-04 19:07:27,528:INFO:HuberRegressor()
2024-02-04 19:07:27,528:INFO:create_model() successfully completed......................................
2024-02-04 19:07:27,618:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:27,618:INFO:Creating metrics dataframe
2024-02-04 19:07:27,618:INFO:Initializing K Neighbors Regressor
2024-02-04 19:07:27,618:INFO:Total runtime is 0.1288971940676371 minutes
2024-02-04 19:07:27,618:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:27,618:INFO:Initializing create_model()
2024-02-04 19:07:27,618:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:27,618:INFO:Checking exceptions
2024-02-04 19:07:27,618:INFO:Importing libraries
2024-02-04 19:07:27,618:INFO:Copying training dataset
2024-02-04 19:07:27,633:INFO:Defining folds
2024-02-04 19:07:27,633:INFO:Declaring metric variables
2024-02-04 19:07:27,637:INFO:Importing untrained model
2024-02-04 19:07:27,637:INFO:K Neighbors Regressor Imported successfully
2024-02-04 19:07:27,643:INFO:Starting cross validation
2024-02-04 19:07:27,643:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:28,230:INFO:Calculating mean and std
2024-02-04 19:07:28,230:INFO:Creating metrics dataframe
2024-02-04 19:07:28,230:INFO:Uploading results into container
2024-02-04 19:07:28,230:INFO:Uploading model into container now
2024-02-04 19:07:28,230:INFO:_master_model_container: 34
2024-02-04 19:07:28,230:INFO:_display_container: 6
2024-02-04 19:07:28,230:INFO:KNeighborsRegressor(n_jobs=-1)
2024-02-04 19:07:28,230:INFO:create_model() successfully completed......................................
2024-02-04 19:07:28,319:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:28,319:INFO:Creating metrics dataframe
2024-02-04 19:07:28,329:INFO:Initializing Decision Tree Regressor
2024-02-04 19:07:28,329:INFO:Total runtime is 0.14074541727701823 minutes
2024-02-04 19:07:28,329:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:28,329:INFO:Initializing create_model()
2024-02-04 19:07:28,329:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:28,329:INFO:Checking exceptions
2024-02-04 19:07:28,329:INFO:Importing libraries
2024-02-04 19:07:28,329:INFO:Copying training dataset
2024-02-04 19:07:28,344:INFO:Defining folds
2024-02-04 19:07:28,345:INFO:Declaring metric variables
2024-02-04 19:07:28,345:INFO:Importing untrained model
2024-02-04 19:07:28,345:INFO:Decision Tree Regressor Imported successfully
2024-02-04 19:07:28,356:INFO:Starting cross validation
2024-02-04 19:07:28,356:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:28,951:INFO:Calculating mean and std
2024-02-04 19:07:28,951:INFO:Creating metrics dataframe
2024-02-04 19:07:28,951:INFO:Uploading results into container
2024-02-04 19:07:28,951:INFO:Uploading model into container now
2024-02-04 19:07:28,951:INFO:_master_model_container: 35
2024-02-04 19:07:28,951:INFO:_display_container: 6
2024-02-04 19:07:28,951:INFO:DecisionTreeRegressor(random_state=123)
2024-02-04 19:07:28,951:INFO:create_model() successfully completed......................................
2024-02-04 19:07:29,038:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:29,038:INFO:Creating metrics dataframe
2024-02-04 19:07:29,038:INFO:Initializing Random Forest Regressor
2024-02-04 19:07:29,038:INFO:Total runtime is 0.15255572001139323 minutes
2024-02-04 19:07:29,055:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:29,055:INFO:Initializing create_model()
2024-02-04 19:07:29,055:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:29,055:INFO:Checking exceptions
2024-02-04 19:07:29,055:INFO:Importing libraries
2024-02-04 19:07:29,056:INFO:Copying training dataset
2024-02-04 19:07:29,062:INFO:Defining folds
2024-02-04 19:07:29,062:INFO:Declaring metric variables
2024-02-04 19:07:29,062:INFO:Importing untrained model
2024-02-04 19:07:29,062:INFO:Random Forest Regressor Imported successfully
2024-02-04 19:07:29,072:INFO:Starting cross validation
2024-02-04 19:07:29,076:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:32,585:INFO:Calculating mean and std
2024-02-04 19:07:32,585:INFO:Creating metrics dataframe
2024-02-04 19:07:32,585:INFO:Uploading results into container
2024-02-04 19:07:32,585:INFO:Uploading model into container now
2024-02-04 19:07:32,585:INFO:_master_model_container: 36
2024-02-04 19:07:32,585:INFO:_display_container: 6
2024-02-04 19:07:32,585:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:07:32,585:INFO:create_model() successfully completed......................................
2024-02-04 19:07:32,672:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:32,672:INFO:Creating metrics dataframe
2024-02-04 19:07:32,684:INFO:Initializing Extra Trees Regressor
2024-02-04 19:07:32,684:INFO:Total runtime is 0.21332013607025146 minutes
2024-02-04 19:07:32,693:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:32,693:INFO:Initializing create_model()
2024-02-04 19:07:32,693:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:32,693:INFO:Checking exceptions
2024-02-04 19:07:32,693:INFO:Importing libraries
2024-02-04 19:07:32,693:INFO:Copying training dataset
2024-02-04 19:07:32,695:INFO:Defining folds
2024-02-04 19:07:32,695:INFO:Declaring metric variables
2024-02-04 19:07:32,695:INFO:Importing untrained model
2024-02-04 19:07:32,695:INFO:Extra Trees Regressor Imported successfully
2024-02-04 19:07:32,711:INFO:Starting cross validation
2024-02-04 19:07:32,711:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:35,182:INFO:Calculating mean and std
2024-02-04 19:07:35,182:INFO:Creating metrics dataframe
2024-02-04 19:07:35,189:INFO:Uploading results into container
2024-02-04 19:07:35,190:INFO:Uploading model into container now
2024-02-04 19:07:35,190:INFO:_master_model_container: 37
2024-02-04 19:07:35,190:INFO:_display_container: 6
2024-02-04 19:07:35,190:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:07:35,190:INFO:create_model() successfully completed......................................
2024-02-04 19:07:35,269:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:35,269:INFO:Creating metrics dataframe
2024-02-04 19:07:35,284:INFO:Initializing AdaBoost Regressor
2024-02-04 19:07:35,284:INFO:Total runtime is 0.2566667358080546 minutes
2024-02-04 19:07:35,293:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:35,293:INFO:Initializing create_model()
2024-02-04 19:07:35,293:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:35,293:INFO:Checking exceptions
2024-02-04 19:07:35,293:INFO:Importing libraries
2024-02-04 19:07:35,293:INFO:Copying training dataset
2024-02-04 19:07:35,298:INFO:Defining folds
2024-02-04 19:07:35,298:INFO:Declaring metric variables
2024-02-04 19:07:35,298:INFO:Importing untrained model
2024-02-04 19:07:35,298:INFO:AdaBoost Regressor Imported successfully
2024-02-04 19:07:35,306:INFO:Starting cross validation
2024-02-04 19:07:35,315:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:36,452:INFO:Calculating mean and std
2024-02-04 19:07:36,452:INFO:Creating metrics dataframe
2024-02-04 19:07:36,452:INFO:Uploading results into container
2024-02-04 19:07:36,452:INFO:Uploading model into container now
2024-02-04 19:07:36,452:INFO:_master_model_container: 38
2024-02-04 19:07:36,452:INFO:_display_container: 6
2024-02-04 19:07:36,452:INFO:AdaBoostRegressor(random_state=123)
2024-02-04 19:07:36,452:INFO:create_model() successfully completed......................................
2024-02-04 19:07:36,537:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:36,537:INFO:Creating metrics dataframe
2024-02-04 19:07:36,550:INFO:Initializing Gradient Boosting Regressor
2024-02-04 19:07:36,550:INFO:Total runtime is 0.2777679204940796 minutes
2024-02-04 19:07:36,560:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:36,560:INFO:Initializing create_model()
2024-02-04 19:07:36,560:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:36,560:INFO:Checking exceptions
2024-02-04 19:07:36,560:INFO:Importing libraries
2024-02-04 19:07:36,560:INFO:Copying training dataset
2024-02-04 19:07:36,560:INFO:Defining folds
2024-02-04 19:07:36,560:INFO:Declaring metric variables
2024-02-04 19:07:36,560:INFO:Importing untrained model
2024-02-04 19:07:36,572:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:07:36,582:INFO:Starting cross validation
2024-02-04 19:07:36,587:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:38,301:INFO:Calculating mean and std
2024-02-04 19:07:38,301:INFO:Creating metrics dataframe
2024-02-04 19:07:38,301:INFO:Uploading results into container
2024-02-04 19:07:38,301:INFO:Uploading model into container now
2024-02-04 19:07:38,301:INFO:_master_model_container: 39
2024-02-04 19:07:38,301:INFO:_display_container: 6
2024-02-04 19:07:38,301:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:07:38,301:INFO:create_model() successfully completed......................................
2024-02-04 19:07:38,386:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:38,386:INFO:Creating metrics dataframe
2024-02-04 19:07:38,402:INFO:Initializing Light Gradient Boosting Machine
2024-02-04 19:07:38,402:INFO:Total runtime is 0.30862969160079956 minutes
2024-02-04 19:07:38,402:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:38,402:INFO:Initializing create_model()
2024-02-04 19:07:38,402:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:38,402:INFO:Checking exceptions
2024-02-04 19:07:38,402:INFO:Importing libraries
2024-02-04 19:07:38,402:INFO:Copying training dataset
2024-02-04 19:07:38,410:INFO:Defining folds
2024-02-04 19:07:38,410:INFO:Declaring metric variables
2024-02-04 19:07:38,410:INFO:Importing untrained model
2024-02-04 19:07:38,417:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-04 19:07:38,422:INFO:Starting cross validation
2024-02-04 19:07:38,422:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:39,867:INFO:Calculating mean and std
2024-02-04 19:07:39,867:INFO:Creating metrics dataframe
2024-02-04 19:07:39,872:INFO:Uploading results into container
2024-02-04 19:07:39,872:INFO:Uploading model into container now
2024-02-04 19:07:39,872:INFO:_master_model_container: 40
2024-02-04 19:07:39,872:INFO:_display_container: 6
2024-02-04 19:07:39,872:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:07:39,872:INFO:create_model() successfully completed......................................
2024-02-04 19:07:39,979:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:39,979:INFO:Creating metrics dataframe
2024-02-04 19:07:39,992:INFO:Initializing Dummy Regressor
2024-02-04 19:07:39,992:INFO:Total runtime is 0.33513397773106895 minutes
2024-02-04 19:07:39,992:INFO:SubProcess create_model() called ==================================
2024-02-04 19:07:39,992:INFO:Initializing create_model()
2024-02-04 19:07:39,992:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECD29A0BE0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:39,992:INFO:Checking exceptions
2024-02-04 19:07:39,992:INFO:Importing libraries
2024-02-04 19:07:39,992:INFO:Copying training dataset
2024-02-04 19:07:40,002:INFO:Defining folds
2024-02-04 19:07:40,002:INFO:Declaring metric variables
2024-02-04 19:07:40,002:INFO:Importing untrained model
2024-02-04 19:07:40,008:INFO:Dummy Regressor Imported successfully
2024-02-04 19:07:40,008:INFO:Starting cross validation
2024-02-04 19:07:40,008:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:07:40,586:INFO:Calculating mean and std
2024-02-04 19:07:40,586:INFO:Creating metrics dataframe
2024-02-04 19:07:40,586:INFO:Uploading results into container
2024-02-04 19:07:40,586:INFO:Uploading model into container now
2024-02-04 19:07:40,586:INFO:_master_model_container: 41
2024-02-04 19:07:40,586:INFO:_display_container: 6
2024-02-04 19:07:40,586:INFO:DummyRegressor()
2024-02-04 19:07:40,586:INFO:create_model() successfully completed......................................
2024-02-04 19:07:40,672:INFO:SubProcess create_model() end ==================================
2024-02-04 19:07:40,672:INFO:Creating metrics dataframe
2024-02-04 19:07:40,692:INFO:Initializing create_model()
2024-02-04 19:07:40,692:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:40,692:INFO:Checking exceptions
2024-02-04 19:07:40,692:INFO:Importing libraries
2024-02-04 19:07:40,692:INFO:Copying training dataset
2024-02-04 19:07:40,692:INFO:Defining folds
2024-02-04 19:07:40,692:INFO:Declaring metric variables
2024-02-04 19:07:40,692:INFO:Importing untrained model
2024-02-04 19:07:40,692:INFO:Declaring custom model
2024-02-04 19:07:40,692:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:07:40,703:INFO:Cross validation set to False
2024-02-04 19:07:40,703:INFO:Fitting Model
2024-02-04 19:07:41,234:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:07:41,234:INFO:create_model() successfully completed......................................
2024-02-04 19:07:41,324:INFO:Creating Dashboard logs
2024-02-04 19:07:41,324:INFO:Model: Gradient Boosting Regressor
2024-02-04 19:07:41,354:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-04 19:07:41,405:INFO:Initializing predict_model()
2024-02-04 19:07:41,405:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECCF1FD790>)
2024-02-04 19:07:41,405:INFO:Checking exceptions
2024-02-04 19:07:41,405:INFO:Preloading libraries
2024-02-04 19:07:41,785:INFO:Initializing create_model()
2024-02-04 19:07:41,785:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:41,785:INFO:Checking exceptions
2024-02-04 19:07:41,785:INFO:Importing libraries
2024-02-04 19:07:41,785:INFO:Copying training dataset
2024-02-04 19:07:41,800:INFO:Defining folds
2024-02-04 19:07:41,800:INFO:Declaring metric variables
2024-02-04 19:07:41,800:INFO:Importing untrained model
2024-02-04 19:07:41,800:INFO:Declaring custom model
2024-02-04 19:07:41,800:INFO:Light Gradient Boosting Machine Imported successfully
2024-02-04 19:07:41,805:INFO:Cross validation set to False
2024-02-04 19:07:41,805:INFO:Fitting Model
2024-02-04 19:07:41,923:INFO:[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines
2024-02-04 19:07:41,936:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000851 seconds.
2024-02-04 19:07:41,936:INFO:You can set `force_col_wise=true` to remove the overhead.
2024-02-04 19:07:41,936:INFO:[LightGBM] [Info] Total Bins 2881
2024-02-04 19:07:41,936:INFO:[LightGBM] [Info] Number of data points in the train set: 1019, number of used features: 93
2024-02-04 19:07:41,936:INFO:[LightGBM] [Info] Start training from score 180679.625123
2024-02-04 19:07:42,005:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:07:42,005:INFO:create_model() successfully completed......................................
2024-02-04 19:07:42,108:INFO:Creating Dashboard logs
2024-02-04 19:07:42,124:INFO:Model: Light Gradient Boosting Machine
2024-02-04 19:07:42,141:INFO:Logged params: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 123, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}
2024-02-04 19:07:42,188:INFO:Initializing predict_model()
2024-02-04 19:07:42,188:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECCF1FD790>)
2024-02-04 19:07:42,188:INFO:Checking exceptions
2024-02-04 19:07:42,188:INFO:Preloading libraries
2024-02-04 19:07:42,623:INFO:Initializing create_model()
2024-02-04 19:07:42,623:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:07:42,623:INFO:Checking exceptions
2024-02-04 19:07:42,623:INFO:Importing libraries
2024-02-04 19:07:42,623:INFO:Copying training dataset
2024-02-04 19:07:42,639:INFO:Defining folds
2024-02-04 19:07:42,639:INFO:Declaring metric variables
2024-02-04 19:07:42,639:INFO:Importing untrained model
2024-02-04 19:07:42,639:INFO:Declaring custom model
2024-02-04 19:07:42,639:INFO:Extra Trees Regressor Imported successfully
2024-02-04 19:07:42,639:INFO:Cross validation set to False
2024-02-04 19:07:42,639:INFO:Fitting Model
2024-02-04 19:07:43,004:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2024-02-04 19:07:43,004:INFO:create_model() successfully completed......................................
2024-02-04 19:07:43,083:INFO:Creating Dashboard logs
2024-02-04 19:07:43,101:INFO:Model: Extra Trees Regressor
2024-02-04 19:07:43,125:INFO:Logged params: {'bootstrap': False, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-02-04 19:07:43,179:INFO:Initializing predict_model()
2024-02-04 19:07:43,179:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECCF4783A0>)
2024-02-04 19:07:43,179:INFO:Checking exceptions
2024-02-04 19:07:43,179:INFO:Preloading libraries
2024-02-04 19:07:43,606:INFO:Creating Dashboard logs
2024-02-04 19:07:43,606:INFO:Model: Random Forest Regressor
2024-02-04 19:07:43,620:INFO:Logged params: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-02-04 19:07:43,785:INFO:Creating Dashboard logs
2024-02-04 19:07:43,785:INFO:Model: Bayesian Ridge
2024-02-04 19:07:43,817:INFO:Logged params: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'alpha_init': None, 'compute_score': False, 'copy_X': True, 'fit_intercept': True, 'lambda_1': 1e-06, 'lambda_2': 1e-06, 'lambda_init': None, 'n_iter': 300, 'tol': 0.001, 'verbose': False}
2024-02-04 19:07:43,973:INFO:Creating Dashboard logs
2024-02-04 19:07:43,973:INFO:Model: Lasso Least Angle Regression
2024-02-04 19:07:43,995:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'max_iter': 500, 'normalize': 'deprecated', 'positive': False, 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2024-02-04 19:07:44,206:INFO:Creating Dashboard logs
2024-02-04 19:07:44,208:INFO:Model: Ridge Regression
2024-02-04 19:07:44,222:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': None, 'positive': False, 'random_state': 123, 'solver': 'auto', 'tol': 0.0001}
2024-02-04 19:07:44,389:INFO:Creating Dashboard logs
2024-02-04 19:07:44,389:INFO:Model: Elastic Net
2024-02-04 19:07:44,410:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'l1_ratio': 0.5, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2024-02-04 19:07:44,567:INFO:Creating Dashboard logs
2024-02-04 19:07:44,567:INFO:Model: Lasso Regression
2024-02-04 19:07:44,597:INFO:Logged params: {'alpha': 1.0, 'copy_X': True, 'fit_intercept': True, 'max_iter': 1000, 'positive': False, 'precompute': False, 'random_state': 123, 'selection': 'cyclic', 'tol': 0.0001, 'warm_start': False}
2024-02-04 19:07:44,740:INFO:Creating Dashboard logs
2024-02-04 19:07:44,756:INFO:Model: AdaBoost Regressor
2024-02-04 19:07:44,772:INFO:Logged params: {'base_estimator': 'deprecated', 'estimator': None, 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 50, 'random_state': 123}
2024-02-04 19:07:44,929:INFO:Creating Dashboard logs
2024-02-04 19:07:44,929:INFO:Model: Orthogonal Matching Pursuit
2024-02-04 19:07:44,943:INFO:Logged params: {'fit_intercept': True, 'n_nonzero_coefs': None, 'normalize': 'deprecated', 'precompute': 'auto', 'tol': None}
2024-02-04 19:07:45,104:INFO:Creating Dashboard logs
2024-02-04 19:07:45,108:INFO:Model: Decision Tree Regressor
2024-02-04 19:07:45,120:INFO:Logged params: {'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 123, 'splitter': 'best'}
2024-02-04 19:07:45,285:INFO:Creating Dashboard logs
2024-02-04 19:07:45,285:INFO:Model: Huber Regressor
2024-02-04 19:07:45,308:INFO:Logged params: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': True, 'max_iter': 100, 'tol': 1e-05, 'warm_start': False}
2024-02-04 19:07:45,460:INFO:Creating Dashboard logs
2024-02-04 19:07:45,460:INFO:Model: K Neighbors Regressor
2024-02-04 19:07:45,489:INFO:Logged params: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': -1, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}
2024-02-04 19:07:45,638:INFO:Creating Dashboard logs
2024-02-04 19:07:45,638:INFO:Model: Passive Aggressive Regressor
2024-02-04 19:07:45,669:INFO:Logged params: {'C': 1.0, 'average': False, 'early_stopping': False, 'epsilon': 0.1, 'fit_intercept': True, 'loss': 'epsilon_insensitive', 'max_iter': 1000, 'n_iter_no_change': 5, 'random_state': 123, 'shuffle': True, 'tol': 0.001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-04 19:07:45,821:INFO:Creating Dashboard logs
2024-02-04 19:07:45,837:INFO:Model: Dummy Regressor
2024-02-04 19:07:45,852:INFO:Logged params: {'constant': None, 'quantile': None, 'strategy': 'mean'}
2024-02-04 19:07:46,009:INFO:Creating Dashboard logs
2024-02-04 19:07:46,009:INFO:Model: Linear Regression
2024-02-04 19:07:46,034:INFO:Logged params: {'copy_X': True, 'fit_intercept': True, 'n_jobs': -1, 'positive': False}
2024-02-04 19:07:46,194:INFO:Creating Dashboard logs
2024-02-04 19:07:46,194:INFO:Model: Least Angle Regression
2024-02-04 19:07:46,224:INFO:Logged params: {'copy_X': True, 'eps': 2.220446049250313e-16, 'fit_intercept': True, 'fit_path': True, 'jitter': None, 'n_nonzero_coefs': 500, 'normalize': 'deprecated', 'precompute': 'auto', 'random_state': 123, 'verbose': False}
2024-02-04 19:07:46,420:INFO:_master_model_container: 41
2024-02-04 19:07:46,423:INFO:_display_container: 6
2024-02-04 19:07:46,423:INFO:[GradientBoostingRegressor(random_state=123), LGBMRegressor(n_jobs=-1, random_state=123), ExtraTreesRegressor(n_jobs=-1, random_state=123)]
2024-02-04 19:07:46,423:INFO:compare_models() successfully completed......................................
2024-02-04 19:09:26,980:INFO:Initializing tune_model()
2024-02-04 19:09:26,980:INFO:tune_model(estimator=GradientBoostingRegressor(random_state=123), fold=None, round=4, n_iter=10, custom_grid=None, optimize=R2, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>)
2024-02-04 19:09:26,980:INFO:Checking exceptions
2024-02-04 19:09:26,996:INFO:Copying training dataset
2024-02-04 19:09:26,999:INFO:Checking base model
2024-02-04 19:09:26,999:INFO:Base model : Gradient Boosting Regressor
2024-02-04 19:09:27,004:INFO:Declaring metric variables
2024-02-04 19:09:27,004:INFO:Defining Hyperparameters
2024-02-04 19:09:27,127:INFO:Tuning with n_jobs=-1
2024-02-04 19:09:27,127:INFO:Initializing RandomizedSearchCV
2024-02-04 19:09:40,626:INFO:best_params: {'actual_estimator__subsample': 0.85, 'actual_estimator__n_estimators': 230, 'actual_estimator__min_samples_split': 5, 'actual_estimator__min_samples_leaf': 5, 'actual_estimator__min_impurity_decrease': 0.02, 'actual_estimator__max_features': 1.0, 'actual_estimator__max_depth': 7, 'actual_estimator__learning_rate': 0.15}
2024-02-04 19:09:40,626:INFO:Hyperparameter search completed
2024-02-04 19:09:40,626:INFO:SubProcess create_model() called ==================================
2024-02-04 19:09:40,626:INFO:Initializing create_model()
2024-02-04 19:09:40,626:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECCF4DDD30>, model_only=True, return_train_score=False, error_score=0.0, kwargs={'subsample': 0.85, 'n_estimators': 230, 'min_samples_split': 5, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.02, 'max_features': 1.0, 'max_depth': 7, 'learning_rate': 0.15})
2024-02-04 19:09:40,626:INFO:Checking exceptions
2024-02-04 19:09:40,626:INFO:Importing libraries
2024-02-04 19:09:40,626:INFO:Copying training dataset
2024-02-04 19:09:40,632:INFO:Defining folds
2024-02-04 19:09:40,632:INFO:Declaring metric variables
2024-02-04 19:09:40,636:INFO:Importing untrained model
2024-02-04 19:09:40,636:INFO:Declaring custom model
2024-02-04 19:09:40,636:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:09:40,636:INFO:Starting cross validation
2024-02-04 19:09:40,636:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:09:45,894:INFO:Calculating mean and std
2024-02-04 19:09:45,894:INFO:Creating metrics dataframe
2024-02-04 19:09:45,898:INFO:Finalizing model
2024-02-04 19:09:47,828:INFO:Uploading results into container
2024-02-04 19:09:47,838:INFO:Uploading model into container now
2024-02-04 19:09:47,838:INFO:_master_model_container: 42
2024-02-04 19:09:47,838:INFO:_display_container: 7
2024-02-04 19:09:47,840:INFO:GradientBoostingRegressor(learning_rate=0.15, max_depth=7, max_features=1.0,
                          min_impurity_decrease=0.02, min_samples_leaf=5,
                          min_samples_split=5, n_estimators=230,
                          random_state=123, subsample=0.85)
2024-02-04 19:09:47,840:INFO:create_model() successfully completed......................................
2024-02-04 19:09:47,933:INFO:SubProcess create_model() end ==================================
2024-02-04 19:09:47,933:INFO:choose_better activated
2024-02-04 19:09:47,933:INFO:SubProcess create_model() called ==================================
2024-02-04 19:09:47,945:INFO:Initializing create_model()
2024-02-04 19:09:47,945:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:09:47,945:INFO:Checking exceptions
2024-02-04 19:09:47,945:INFO:Importing libraries
2024-02-04 19:09:47,945:INFO:Copying training dataset
2024-02-04 19:09:47,945:INFO:Defining folds
2024-02-04 19:09:47,945:INFO:Declaring metric variables
2024-02-04 19:09:47,945:INFO:Importing untrained model
2024-02-04 19:09:47,945:INFO:Declaring custom model
2024-02-04 19:09:47,945:INFO:Gradient Boosting Regressor Imported successfully
2024-02-04 19:09:47,945:INFO:Starting cross validation
2024-02-04 19:09:47,945:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:09:49,546:INFO:Calculating mean and std
2024-02-04 19:09:49,546:INFO:Creating metrics dataframe
2024-02-04 19:09:49,546:INFO:Finalizing model
2024-02-04 19:09:50,064:INFO:Uploading results into container
2024-02-04 19:09:50,064:INFO:Uploading model into container now
2024-02-04 19:09:50,064:INFO:_master_model_container: 43
2024-02-04 19:09:50,064:INFO:_display_container: 8
2024-02-04 19:09:50,064:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:09:50,064:INFO:create_model() successfully completed......................................
2024-02-04 19:09:50,150:INFO:SubProcess create_model() end ==================================
2024-02-04 19:09:50,150:INFO:GradientBoostingRegressor(random_state=123) result for R2 is 0.8969
2024-02-04 19:09:50,150:INFO:GradientBoostingRegressor(learning_rate=0.15, max_depth=7, max_features=1.0,
                          min_impurity_decrease=0.02, min_samples_leaf=5,
                          min_samples_split=5, n_estimators=230,
                          random_state=123, subsample=0.85) result for R2 is 0.8931
2024-02-04 19:09:50,150:INFO:GradientBoostingRegressor(random_state=123) is best model
2024-02-04 19:09:50,150:INFO:choose_better completed
2024-02-04 19:09:50,150:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2024-02-04 19:09:50,150:INFO:Creating Dashboard logs
2024-02-04 19:09:50,150:INFO:Model: Gradient Boosting Regressor
2024-02-04 19:09:50,166:INFO:Logged params: {'alpha': 0.9, 'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 123, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}
2024-02-04 19:09:50,240:INFO:Initializing predict_model()
2024-02-04 19:09:50,240:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECD072F0D0>)
2024-02-04 19:09:50,240:INFO:Checking exceptions
2024-02-04 19:09:50,240:INFO:Preloading libraries
2024-02-04 19:09:50,637:INFO:_master_model_container: 43
2024-02-04 19:09:50,637:INFO:_display_container: 7
2024-02-04 19:09:50,637:INFO:GradientBoostingRegressor(random_state=123)
2024-02-04 19:09:50,637:INFO:tune_model() successfully completed......................................
2024-02-04 19:10:07,528:INFO:Initializing ensemble_model()
2024-02-04 19:10:07,528:INFO:ensemble_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), method=Bagging, fold=None, n_estimators=50, round=4, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-04 19:10:07,528:INFO:Checking exceptions
2024-02-04 19:10:07,544:INFO:Importing libraries
2024-02-04 19:10:07,544:INFO:Copying training dataset
2024-02-04 19:10:07,544:INFO:Checking base model
2024-02-04 19:10:07,544:INFO:Base model : Gradient Boosting Regressor
2024-02-04 19:10:07,549:INFO:Importing untrained ensembler
2024-02-04 19:10:07,549:INFO:Ensemble method set to Bagging
2024-02-04 19:10:07,549:INFO:SubProcess create_model() called ==================================
2024-02-04 19:10:07,549:INFO:Initializing create_model()
2024-02-04 19:10:07,549:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA7665970>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:10:07,549:INFO:Checking exceptions
2024-02-04 19:10:07,549:INFO:Importing libraries
2024-02-04 19:10:07,549:INFO:Copying training dataset
2024-02-04 19:10:07,549:INFO:Defining folds
2024-02-04 19:10:07,549:INFO:Declaring metric variables
2024-02-04 19:10:07,557:INFO:Importing untrained model
2024-02-04 19:10:07,557:INFO:Declaring custom model
2024-02-04 19:10:07,561:INFO:Bagging Regressor Imported successfully
2024-02-04 19:10:07,565:INFO:Starting cross validation
2024-02-04 19:10:07,565:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:10:46,935:INFO:Calculating mean and std
2024-02-04 19:10:46,936:INFO:Creating metrics dataframe
2024-02-04 19:10:46,940:INFO:Finalizing model
2024-02-04 19:11:01,603:INFO:Uploading results into container
2024-02-04 19:11:01,604:INFO:Uploading model into container now
2024-02-04 19:11:01,604:INFO:_master_model_container: 44
2024-02-04 19:11:01,605:INFO:_display_container: 8
2024-02-04 19:11:01,605:INFO:BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123)
2024-02-04 19:11:01,605:INFO:create_model() successfully completed......................................
2024-02-04 19:11:01,764:INFO:SubProcess create_model() end ==================================
2024-02-04 19:11:01,765:INFO:Creating Dashboard logs
2024-02-04 19:11:01,768:INFO:Model: Bagging Regressor
2024-02-04 19:11:01,790:INFO:Logged params: {'base_estimator': 'deprecated', 'bootstrap': True, 'bootstrap_features': False, 'estimator__alpha': 0.9, 'estimator__ccp_alpha': 0.0, 'estimator__criterion': 'friedman_mse', 'estimator__init': None, 'estimator__learning_rate': 0.1, 'estimator__loss': 'squared_error', 'estimator__max_depth': 3, 'estimator__max_features': None, 'estimator__max_leaf_nodes': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__n_estimators': 100, 'estimator__n_iter_no_change': None, 'estimator__random_state': 123, 'estimator__subsample': 1.0, 'estimator__tol': 0.0001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': GradientBoostingRegressor(random_state=123), 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 50, 'n_jobs': None, 'oob_score': False, 'random_state': 123, 'verbose': 0, 'warm_start': False}
2024-02-04 19:11:01,869:INFO:Initializing predict_model()
2024-02-04 19:11:01,869:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECD06CC1F0>)
2024-02-04 19:11:01,869:INFO:Checking exceptions
2024-02-04 19:11:01,869:INFO:Preloading libraries
2024-02-04 19:11:02,467:INFO:_master_model_container: 44
2024-02-04 19:11:02,467:INFO:_display_container: 8
2024-02-04 19:11:02,467:INFO:BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123)
2024-02-04 19:11:02,467:INFO:ensemble_model() successfully completed......................................
2024-02-04 19:11:02,627:INFO:Initializing ensemble_model()
2024-02-04 19:11:02,627:INFO:ensemble_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=GradientBoostingRegressor(random_state=123), method=Boosting, fold=None, n_estimators=10, round=4, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-04 19:11:02,627:INFO:Checking exceptions
2024-02-04 19:11:06,539:INFO:Importing libraries
2024-02-04 19:11:06,539:INFO:Copying training dataset
2024-02-04 19:11:06,539:INFO:Checking base model
2024-02-04 19:11:06,539:INFO:Base model : Gradient Boosting Regressor
2024-02-04 19:11:06,547:INFO:Importing untrained ensembler
2024-02-04 19:11:06,547:INFO:Ensemble method set to Boosting
2024-02-04 19:11:06,547:INFO:SubProcess create_model() called ==================================
2024-02-04 19:11:06,548:INFO:Initializing create_model()
2024-02-04 19:11:06,548:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=AdaBoostRegressor(estimator=GradientBoostingRegressor(random_state=123),
                  n_estimators=10, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECCEBFC9D0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:11:06,548:INFO:Checking exceptions
2024-02-04 19:11:06,548:INFO:Importing libraries
2024-02-04 19:11:06,548:INFO:Copying training dataset
2024-02-04 19:11:06,552:INFO:Defining folds
2024-02-04 19:11:06,552:INFO:Declaring metric variables
2024-02-04 19:11:06,555:INFO:Importing untrained model
2024-02-04 19:11:06,555:INFO:Declaring custom model
2024-02-04 19:11:06,559:INFO:AdaBoost Regressor Imported successfully
2024-02-04 19:11:06,567:INFO:Starting cross validation
2024-02-04 19:11:06,569:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:11:17,308:INFO:Calculating mean and std
2024-02-04 19:11:17,308:INFO:Creating metrics dataframe
2024-02-04 19:11:17,313:INFO:Finalizing model
2024-02-04 19:11:20,997:INFO:Uploading results into container
2024-02-04 19:11:20,997:INFO:Uploading model into container now
2024-02-04 19:11:20,997:INFO:_master_model_container: 45
2024-02-04 19:11:20,997:INFO:_display_container: 9
2024-02-04 19:11:20,998:INFO:AdaBoostRegressor(estimator=GradientBoostingRegressor(random_state=123),
                  n_estimators=10, random_state=123)
2024-02-04 19:11:20,998:INFO:create_model() successfully completed......................................
2024-02-04 19:11:21,091:INFO:SubProcess create_model() end ==================================
2024-02-04 19:11:21,091:INFO:Creating Dashboard logs
2024-02-04 19:11:21,091:INFO:Model: AdaBoost Regressor
2024-02-04 19:11:21,106:INFO:Logged params: {'base_estimator': 'deprecated', 'estimator__alpha': 0.9, 'estimator__ccp_alpha': 0.0, 'estimator__criterion': 'friedman_mse', 'estimator__init': None, 'estimator__learning_rate': 0.1, 'estimator__loss': 'squared_error', 'estimator__max_depth': 3, 'estimator__max_features': None, 'estimator__max_leaf_nodes': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__n_estimators': 100, 'estimator__n_iter_no_change': None, 'estimator__random_state': 123, 'estimator__subsample': 1.0, 'estimator__tol': 0.0001, 'estimator__validation_fraction': 0.1, 'estimator__verbose': 0, 'estimator__warm_start': False, 'estimator': GradientBoostingRegressor(random_state=123), 'learning_rate': 1.0, 'loss': 'linear', 'n_estimators': 10, 'random_state': 123}
2024-02-04 19:11:21,176:INFO:Initializing predict_model()
2024-02-04 19:11:21,176:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=AdaBoostRegressor(estimator=GradientBoostingRegressor(random_state=123),
                  n_estimators=10, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECD072A3A0>)
2024-02-04 19:11:21,176:INFO:Checking exceptions
2024-02-04 19:11:21,176:INFO:Preloading libraries
2024-02-04 19:11:21,578:INFO:_master_model_container: 45
2024-02-04 19:11:21,578:INFO:_display_container: 9
2024-02-04 19:11:21,578:INFO:AdaBoostRegressor(estimator=GradientBoostingRegressor(random_state=123),
                  n_estimators=10, random_state=123)
2024-02-04 19:11:21,578:INFO:ensemble_model() successfully completed......................................
2024-02-04 19:11:27,894:INFO:Initializing blend_models()
2024-02-04 19:11:27,894:INFO:blend_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator_list=[GradientBoostingRegressor(random_state=123), LGBMRegressor(n_jobs=-1, random_state=123), ExtraTreesRegressor(n_jobs=-1, random_state=123)], fold=None, round=4, choose_better=False, optimize=R2, method=auto, weights=None, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-04 19:11:27,894:INFO:Checking exceptions
2024-02-04 19:11:27,909:INFO:Importing libraries
2024-02-04 19:11:27,910:INFO:Copying training dataset
2024-02-04 19:11:27,910:INFO:Getting model names
2024-02-04 19:11:27,910:INFO:SubProcess create_model() called ==================================
2024-02-04 19:11:27,917:INFO:Initializing create_model()
2024-02-04 19:11:27,917:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=VotingRegressor(estimators=[('Gradient Boosting Regressor',
                             GradientBoostingRegressor(random_state=123)),
                            ('Light Gradient Boosting Machine',
                             LGBMRegressor(n_jobs=-1, random_state=123)),
                            ('Extra Trees Regressor',
                             ExtraTreesRegressor(n_jobs=-1, random_state=123))],
                n_jobs=-1), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECCF2D8880>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:11:27,917:INFO:Checking exceptions
2024-02-04 19:11:27,917:INFO:Importing libraries
2024-02-04 19:11:27,917:INFO:Copying training dataset
2024-02-04 19:11:27,917:INFO:Defining folds
2024-02-04 19:11:27,917:INFO:Declaring metric variables
2024-02-04 19:11:27,925:INFO:Importing untrained model
2024-02-04 19:11:27,925:INFO:Declaring custom model
2024-02-04 19:11:27,927:INFO:Voting Regressor Imported successfully
2024-02-04 19:11:27,936:INFO:Starting cross validation
2024-02-04 19:11:27,941:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:11:32,560:INFO:Calculating mean and std
2024-02-04 19:11:32,561:INFO:Creating metrics dataframe
2024-02-04 19:11:32,561:INFO:Finalizing model
2024-02-04 19:11:33,527:INFO:Uploading results into container
2024-02-04 19:11:33,527:INFO:Uploading model into container now
2024-02-04 19:11:33,527:INFO:_master_model_container: 46
2024-02-04 19:11:33,527:INFO:_display_container: 10
2024-02-04 19:11:33,530:INFO:VotingRegressor(estimators=[('Gradient Boosting Regressor',
                             GradientBoostingRegressor(random_state=123)),
                            ('Light Gradient Boosting Machine',
                             LGBMRegressor(n_jobs=-1, random_state=123)),
                            ('Extra Trees Regressor',
                             ExtraTreesRegressor(n_jobs=-1, random_state=123))],
                n_jobs=-1)
2024-02-04 19:11:33,530:INFO:create_model() successfully completed......................................
2024-02-04 19:11:33,627:INFO:SubProcess create_model() end ==================================
2024-02-04 19:11:33,627:INFO:Creating Dashboard logs
2024-02-04 19:11:33,627:INFO:Model: Voting Regressor
2024-02-04 19:11:33,656:INFO:Logged params: {'estimators': [('Gradient Boosting Regressor', GradientBoostingRegressor(random_state=123)), ('Light Gradient Boosting Machine', LGBMRegressor(n_jobs=-1, random_state=123)), ('Extra Trees Regressor', ExtraTreesRegressor(n_jobs=-1, random_state=123))], 'n_jobs': -1, 'verbose': False, 'weights': None, 'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=123), 'Light Gradient Boosting Machine': LGBMRegressor(n_jobs=-1, random_state=123), 'Extra Trees Regressor': ExtraTreesRegressor(n_jobs=-1, random_state=123), 'Gradient Boosting Regressor__alpha': 0.9, 'Gradient Boosting Regressor__ccp_alpha': 0.0, 'Gradient Boosting Regressor__criterion': 'friedman_mse', 'Gradient Boosting Regressor__init': None, 'Gradient Boosting Regressor__learning_rate': 0.1, 'Gradient Boosting Regressor__loss': 'squared_error', 'Gradient Boosting Regressor__max_depth': 3, 'Gradient Boosting Regressor__max_features': None, 'Gradient Boosting Regressor__max_leaf_nodes': None, 'Gradient Boosting Regressor__min_impurity_decrease': 0.0, 'Gradient Boosting Regressor__min_samples_leaf': 1, 'Gradient Boosting Regressor__min_samples_split': 2, 'Gradient Boosting Regressor__min_weight_fraction_leaf': 0.0, 'Gradient Boosting Regressor__n_estimators': 100, 'Gradient Boosting Regressor__n_iter_no_change': None, 'Gradient Boosting Regressor__random_state': 123, 'Gradient Boosting Regressor__subsample': 1.0, 'Gradient Boosting Regressor__tol': 0.0001, 'Gradient Boosting Regressor__validation_fraction': 0.1, 'Gradient Boosting Regressor__verbose': 0, 'Gradient Boosting Regressor__warm_start': False, 'Light Gradient Boosting Machine__boosting_type': 'gbdt', 'Light Gradient Boosting Machine__class_weight': None, 'Light Gradient Boosting Machine__colsample_bytree': 1.0, 'Light Gradient Boosting Machine__importance_type': 'split', 'Light Gradient Boosting Machine__learning_rate': 0.1, 'Light Gradient Boosting Machine__max_depth': -1, 'Light Gradient Boosting Machine__min_child_samples': 20, 'Light Gradient Boosting Machine__min_child_weight': 0.001, 'Light Gradient Boosting Machine__min_split_gain': 0.0, 'Light Gradient Boosting Machine__n_estimators': 100, 'Light Gradient Boosting Machine__n_jobs': -1, 'Light Gradient Boosting Machine__num_leaves': 31, 'Light Gradient Boosting Machine__objective': None, 'Light Gradient Boosting Machine__random_state': 123, 'Light Gradient Boosting Machine__reg_alpha': 0.0, 'Light Gradient Boosting Machine__reg_lambda': 0.0, 'Light Gradient Boosting Machine__subsample': 1.0, 'Light Gradient Boosting Machine__subsample_for_bin': 200000, 'Light Gradient Boosting Machine__subsample_freq': 0, 'Extra Trees Regressor__bootstrap': False, 'Extra Trees Regressor__ccp_alpha': 0.0, 'Extra Trees Regressor__criterion': 'squared_error', 'Extra Trees Regressor__max_depth': None, 'Extra Trees Regressor__max_features': 1.0, 'Extra Trees Regressor__max_leaf_nodes': None, 'Extra Trees Regressor__max_samples': None, 'Extra Trees Regressor__min_impurity_decrease': 0.0, 'Extra Trees Regressor__min_samples_leaf': 1, 'Extra Trees Regressor__min_samples_split': 2, 'Extra Trees Regressor__min_weight_fraction_leaf': 0.0, 'Extra Trees Regressor__n_estimators': 100, 'Extra Trees Regressor__n_jobs': -1, 'Extra Trees Regressor__oob_score': False, 'Extra Trees Regressor__random_state': 123, 'Extra Trees Regressor__verbose': 0, 'Extra Trees Regressor__warm_start': False}
2024-02-04 19:11:33,756:INFO:Initializing predict_model()
2024-02-04 19:11:33,756:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=VotingRegressor(estimators=[('Gradient Boosting Regressor',
                             GradientBoostingRegressor(random_state=123)),
                            ('Light Gradient Boosting Machine',
                             LGBMRegressor(n_jobs=-1, random_state=123)),
                            ('Extra Trees Regressor',
                             ExtraTreesRegressor(n_jobs=-1, random_state=123))],
                n_jobs=-1), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECD60269D0>)
2024-02-04 19:11:33,756:INFO:Checking exceptions
2024-02-04 19:11:33,756:INFO:Preloading libraries
2024-02-04 19:11:34,224:INFO:_master_model_container: 46
2024-02-04 19:11:34,224:INFO:_display_container: 10
2024-02-04 19:11:34,224:INFO:VotingRegressor(estimators=[('Gradient Boosting Regressor',
                             GradientBoostingRegressor(random_state=123)),
                            ('Light Gradient Boosting Machine',
                             LGBMRegressor(n_jobs=-1, random_state=123)),
                            ('Extra Trees Regressor',
                             ExtraTreesRegressor(n_jobs=-1, random_state=123))],
                n_jobs=-1)
2024-02-04 19:11:34,224:INFO:blend_models() successfully completed......................................
2024-02-04 19:11:41,710:INFO:Initializing stack_models()
2024-02-04 19:11:41,710:INFO:stack_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator_list=[GradientBoostingRegressor(random_state=123), LGBMRegressor(n_jobs=-1, random_state=123), ExtraTreesRegressor(n_jobs=-1, random_state=123)], meta_model=None, meta_model_fold=5, fold=None, round=4, method=auto, restack=True, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2024-02-04 19:11:41,710:INFO:Checking exceptions
2024-02-04 19:11:41,710:INFO:Defining meta model
2024-02-04 19:11:41,728:INFO:Getting model names
2024-02-04 19:11:41,728:INFO:[('Gradient Boosting Regressor', GradientBoostingRegressor(random_state=123)), ('Light Gradient Boosting Machine', LGBMRegressor(n_jobs=-1, random_state=123)), ('Extra Trees Regressor', ExtraTreesRegressor(n_jobs=-1, random_state=123))]
2024-02-04 19:11:41,728:INFO:SubProcess create_model() called ==================================
2024-02-04 19:11:41,737:INFO:Initializing create_model()
2024-02-04 19:11:41,737:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=StackingRegressor(cv=5,
                  estimators=[('Gradient Boosting Regressor',
                               GradientBoostingRegressor(random_state=123)),
                              ('Light Gradient Boosting Machine',
                               LGBMRegressor(n_jobs=-1, random_state=123)),
                              ('Extra Trees Regressor',
                               ExtraTreesRegressor(n_jobs=-1,
                                                   random_state=123))],
                  final_estimator=LinearRegression(n_jobs=-1), n_jobs=-1,
                  passthrough=True), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001ECA7636FA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:11:41,737:INFO:Checking exceptions
2024-02-04 19:11:41,737:INFO:Importing libraries
2024-02-04 19:11:41,737:INFO:Copying training dataset
2024-02-04 19:11:41,737:INFO:Defining folds
2024-02-04 19:11:41,737:INFO:Declaring metric variables
2024-02-04 19:11:41,744:INFO:Importing untrained model
2024-02-04 19:11:41,744:INFO:Declaring custom model
2024-02-04 19:11:41,747:INFO:Stacking Regressor Imported successfully
2024-02-04 19:11:41,755:INFO:Starting cross validation
2024-02-04 19:11:41,755:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-02-04 19:12:03,650:INFO:Calculating mean and std
2024-02-04 19:12:03,651:INFO:Creating metrics dataframe
2024-02-04 19:12:03,657:INFO:Finalizing model
2024-02-04 19:12:06,207:INFO:Uploading results into container
2024-02-04 19:12:06,207:INFO:Uploading model into container now
2024-02-04 19:12:06,208:INFO:_master_model_container: 47
2024-02-04 19:12:06,208:INFO:_display_container: 11
2024-02-04 19:12:06,213:INFO:StackingRegressor(cv=5,
                  estimators=[('Gradient Boosting Regressor',
                               GradientBoostingRegressor(random_state=123)),
                              ('Light Gradient Boosting Machine',
                               LGBMRegressor(n_jobs=-1, random_state=123)),
                              ('Extra Trees Regressor',
                               ExtraTreesRegressor(n_jobs=-1,
                                                   random_state=123))],
                  final_estimator=LinearRegression(n_jobs=-1), n_jobs=-1,
                  passthrough=True)
2024-02-04 19:12:06,213:INFO:create_model() successfully completed......................................
2024-02-04 19:12:06,346:INFO:SubProcess create_model() end ==================================
2024-02-04 19:12:06,346:INFO:Creating Dashboard logs
2024-02-04 19:12:06,346:INFO:Model: Stacking Regressor
2024-02-04 19:12:06,383:INFO:Logged params: {'cv': 5, 'estimators': [('Gradient Boosting Regressor', GradientBoostingRegressor(random_state=123)), ('Light Gradient Boosting Machine', LGBMRegressor(n_jobs=-1, random_state=123)), ('Extra Trees Regressor', ExtraTreesRegressor(n_jobs=-1, random_state=123))], 'final_estimator__copy_X': True, 'final_estimator__fit_intercept': True, 'final_estimator__n_jobs': -1, 'final_estimator__positive': False, 'final_estimator': LinearRegression(n_jobs=-1), 'n_jobs': -1, 'passthrough': True, 'verbose': 0, 'Gradient Boosting Regressor': GradientBoostingRegressor(random_state=123), 'Light Gradient Boosting Machine': LGBMRegressor(n_jobs=-1, random_state=123), 'Extra Trees Regressor': ExtraTreesRegressor(n_jobs=-1, random_state=123), 'Gradient Boosting Regressor__alpha': 0.9, 'Gradient Boosting Regressor__ccp_alpha': 0.0, 'Gradient Boosting Regressor__criterion': 'friedman_mse', 'Gradient Boosting Regressor__init': None, 'Gradient Boosting Regressor__learning_rate': 0.1, 'Gradient Boosting Regressor__loss': 'squared_error', 'Gradient Boosting Regressor__max_depth': 3, 'Gradient Boosting Regressor__max_features': None, 'Gradient Boosting Regressor__max_leaf_nodes': None, 'Gradient Boosting Regressor__min_impurity_decrease': 0.0, 'Gradient Boosting Regressor__min_samples_leaf': 1, 'Gradient Boosting Regressor__min_samples_split': 2, 'Gradient Boosting Regressor__min_weight_fraction_leaf': 0.0, 'Gradient Boosting Regressor__n_estimators': 100, 'Gradient Boosting Regressor__n_iter_no_change': None, 'Gradient Boosting Regressor__random_state': 123, 'Gradient Boosting Regressor__subsample': 1.0, 'Gradient Boosting Regressor__tol': 0.0001, 'Gradient Boosting Regressor__validation_fraction': 0.1, 'Gradient Boosting Regressor__verbose': 0, 'Gradient Boosting Regressor__warm_start': False, 'Light Gradient Boosting Machine__boosting_type': 'gbdt', 'Light Gradient Boosting Machine__class_weight': None, 'Light Gradient Boosting Machine__colsample_bytree': 1.0, 'Light Gradient Boosting Machine__importance_type': 'split', 'Light Gradient Boosting Machine__learning_rate': 0.1, 'Light Gradient Boosting Machine__max_depth': -1, 'Light Gradient Boosting Machine__min_child_samples': 20, 'Light Gradient Boosting Machine__min_child_weight': 0.001, 'Light Gradient Boosting Machine__min_split_gain': 0.0, 'Light Gradient Boosting Machine__n_estimators': 100, 'Light Gradient Boosting Machine__n_jobs': -1, 'Light Gradient Boosting Machine__num_leaves': 31, 'Light Gradient Boosting Machine__objective': None, 'Light Gradient Boosting Machine__random_state': 123, 'Light Gradient Boosting Machine__reg_alpha': 0.0, 'Light Gradient Boosting Machine__reg_lambda': 0.0, 'Light Gradient Boosting Machine__subsample': 1.0, 'Light Gradient Boosting Machine__subsample_for_bin': 200000, 'Light Gradient Boosting Machine__subsample_freq': 0, 'Extra Trees Regressor__bootstrap': False, 'Extra Trees Regressor__ccp_alpha': 0.0, 'Extra Trees Regressor__criterion': 'squared_error', 'Extra Trees Regressor__max_depth': None, 'Extra Trees Regressor__max_features': 1.0, 'Extra Trees Regressor__max_leaf_nodes': None, 'Extra Trees Regressor__max_samples': None, 'Extra Trees Regressor__min_impurity_decrease': 0.0, 'Extra Trees Regressor__min_samples_leaf': 1, 'Extra Trees Regressor__min_samples_split': 2, 'Extra Trees Regressor__min_weight_fraction_leaf': 0.0, 'Extra Trees Regressor__n_estimators': 100, 'Extra Trees Regressor__n_jobs': -1, 'Extra Trees Regressor__oob_score': False, 'Extra Trees Regressor__random_state': 123, 'Extra Trees Regressor__verbose': 0, 'Extra Trees Regressor__warm_start': False}
2024-02-04 19:12:06,490:INFO:Initializing predict_model()
2024-02-04 19:12:06,491:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=StackingRegressor(cv=5,
                  estimators=[('Gradient Boosting Regressor',
                               GradientBoostingRegressor(random_state=123)),
                              ('Light Gradient Boosting Machine',
                               LGBMRegressor(n_jobs=-1, random_state=123)),
                              ('Extra Trees Regressor',
                               ExtraTreesRegressor(n_jobs=-1,
                                                   random_state=123))],
                  final_estimator=LinearRegression(n_jobs=-1), n_jobs=-1,
                  passthrough=True), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=False, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECD6026160>)
2024-02-04 19:12:06,491:INFO:Checking exceptions
2024-02-04 19:12:06,491:INFO:Preloading libraries
2024-02-04 19:12:07,016:INFO:_master_model_container: 47
2024-02-04 19:12:07,016:INFO:_display_container: 11
2024-02-04 19:12:07,032:INFO:StackingRegressor(cv=5,
                  estimators=[('Gradient Boosting Regressor',
                               GradientBoostingRegressor(random_state=123)),
                              ('Light Gradient Boosting Machine',
                               LGBMRegressor(n_jobs=-1, random_state=123)),
                              ('Extra Trees Regressor',
                               ExtraTreesRegressor(n_jobs=-1,
                                                   random_state=123))],
                  final_estimator=LinearRegression(n_jobs=-1), n_jobs=-1,
                  passthrough=True)
2024-02-04 19:12:07,032:INFO:stack_models() successfully completed......................................
2024-02-04 19:15:25,084:INFO:Initializing interpret_model()
2024-02-04 19:15:25,084:INFO:interpret_model(estimator=LGBMRegressor(n_jobs=-1, random_state=123), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=summary, save=False, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>)
2024-02-04 19:15:25,084:INFO:Checking exceptions
2024-02-04 19:15:25,084:INFO:Soft dependency imported: shap: 0.44.1
2024-02-04 19:15:25,599:INFO:plot type: summary
2024-02-04 19:15:25,599:INFO:Creating TreeExplainer
2024-02-04 19:15:25,690:INFO:Compiling shap values
2024-02-04 19:15:26,155:INFO:Visual Rendered Successfully
2024-02-04 19:15:26,155:INFO:interpret_model() successfully completed......................................
2024-02-04 19:15:33,684:INFO:Initializing interpret_model()
2024-02-04 19:15:33,684:INFO:interpret_model(estimator=LGBMRegressor(n_jobs=-1, random_state=123), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=None, plot=correlation, save=False, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>)
2024-02-04 19:15:33,684:INFO:Checking exceptions
2024-02-04 19:15:33,685:INFO:Soft dependency imported: shap: 0.44.1
2024-02-04 19:15:33,775:INFO:plot type: correlation
2024-02-04 19:15:33,776:WARNING:No feature passed. Default value of feature used for correlation plot: MSSubClass_SPLIT FOYER
2024-02-04 19:15:33,776:INFO:Creating TreeExplainer
2024-02-04 19:15:33,847:INFO:Compiling shap values
2024-02-04 19:15:33,942:INFO:model type detected: type 2
2024-02-04 19:15:34,050:INFO:Visual Rendered Successfully
2024-02-04 19:15:34,050:INFO:interpret_model() successfully completed......................................
2024-02-04 19:15:50,452:INFO:Initializing interpret_model()
2024-02-04 19:15:50,452:INFO:interpret_model(estimator=LGBMRegressor(n_jobs=-1, random_state=123), use_train_data=False, X_new_sample=None, y_new_sample=None, feature=None, kwargs={}, observation=12, plot=reason, save=False, self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>)
2024-02-04 19:15:50,452:INFO:Checking exceptions
2024-02-04 19:15:50,452:INFO:Soft dependency imported: shap: 0.44.1
2024-02-04 19:15:50,547:INFO:plot type: reason
2024-02-04 19:15:50,547:INFO:model type detected: type 2
2024-02-04 19:15:50,547:INFO:Creating TreeExplainer
2024-02-04 19:15:50,620:INFO:Compiling shap values
2024-02-04 19:15:50,721:INFO:Visual Rendered Successfully
2024-02-04 19:15:50,721:INFO:interpret_model() successfully completed......................................
2024-02-04 19:17:38,721:INFO:Initializing automl()
2024-02-04 19:17:38,721:INFO:automl(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, optimize=R2, use_holdout=False, turbo=True, return_train_score=False)
2024-02-04 19:17:38,721:INFO:Model Selection Basis : CV Results on Training set
2024-02-04 19:17:38,721:INFO:Checking model 18
2024-02-04 19:17:38,721:INFO:Checking model 20
2024-02-04 19:17:38,721:INFO:Checking model 21
2024-02-04 19:17:38,721:INFO:Checking model 22
2024-02-04 19:17:38,721:INFO:Checking model 23
2024-02-04 19:17:38,721:INFO:Checking model 24
2024-02-04 19:17:38,721:INFO:Checking model 25
2024-02-04 19:17:38,721:INFO:Checking model 26
2024-02-04 19:17:38,721:INFO:Checking model 27
2024-02-04 19:17:38,721:INFO:Checking model 28
2024-02-04 19:17:38,721:INFO:Checking model 29
2024-02-04 19:17:38,721:INFO:Checking model 30
2024-02-04 19:17:38,721:INFO:Checking model 31
2024-02-04 19:17:38,721:INFO:Checking model 32
2024-02-04 19:17:38,721:INFO:Checking model 33
2024-02-04 19:17:38,721:INFO:Checking model 34
2024-02-04 19:17:38,721:INFO:Checking model 35
2024-02-04 19:17:38,721:INFO:Checking model 36
2024-02-04 19:17:38,721:INFO:Checking model 37
2024-02-04 19:17:38,721:INFO:Checking model 38
2024-02-04 19:17:38,721:INFO:Checking model 39
2024-02-04 19:17:38,721:INFO:Checking model 40
2024-02-04 19:17:38,721:INFO:Checking model 41
2024-02-04 19:17:38,721:INFO:Checking model 42
2024-02-04 19:17:38,721:INFO:Checking model 43
2024-02-04 19:17:38,721:INFO:Checking model 44
2024-02-04 19:17:38,721:INFO:Checking model 45
2024-02-04 19:17:38,728:INFO:Checking model 46
2024-02-04 19:17:38,728:INFO:Initializing create_model()
2024-02-04 19:17:38,728:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-02-04 19:17:38,728:INFO:Checking exceptions
2024-02-04 19:17:38,730:INFO:Importing libraries
2024-02-04 19:17:38,730:INFO:Copying training dataset
2024-02-04 19:17:38,734:INFO:Defining folds
2024-02-04 19:17:38,734:INFO:Declaring metric variables
2024-02-04 19:17:38,734:INFO:Importing untrained model
2024-02-04 19:17:38,734:INFO:Declaring custom model
2024-02-04 19:17:38,735:INFO:Bagging Regressor Imported successfully
2024-02-04 19:17:38,737:INFO:Cross validation set to False
2024-02-04 19:17:38,737:INFO:Fitting Model
2024-02-04 19:17:55,347:INFO:BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123)
2024-02-04 19:17:55,347:INFO:create_model() successfully completed......................................
2024-02-04 19:17:55,620:INFO:BaggingRegressor(estimator=GradientBoostingRegressor(random_state=123),
                 n_estimators=50, random_state=123)
2024-02-04 19:17:55,622:INFO:automl() successfully completed......................................
2024-02-04 19:18:09,500:INFO:Soft dependency imported: explainerdashboard: 0.4.5
2024-02-04 19:18:10,071:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\dash\dash.py:538: UserWarning:

JupyterDash is deprecated, use Dash instead.
See https://dash.plotly.com/dash-in-jupyter for more details.


2024-02-04 19:20:04,047:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,053:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,053:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,053:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,053:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,067:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,067:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,067:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,072:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,079:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,081:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,086:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,087:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,093:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,094:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,095:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,100:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,101:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,101:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,101:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,101:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,101:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,101:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,101:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,105:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,105:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,109:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,109:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,109:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:04,109:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,583:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,599:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,599:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,599:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,599:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,599:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,599:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,599:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,700:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,700:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,701:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,701:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,701:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,702:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,702:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,702:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,703:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,710:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,710:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,710:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:20:50,711:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\explainerdashboard\explainer_methods.py:421: PerformanceWarning:

DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`


2024-02-04 19:22:50,397:INFO:Initializing predict_model()
2024-02-04 19:22:50,397:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000001ECB85D5DF0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001ECDB347B80>)
2024-02-04 19:22:50,397:INFO:Checking exceptions
2024-02-04 19:22:50,397:INFO:Preloading libraries
2024-02-04 19:23:13,722:INFO:Soft dependency imported: gradio: 3.50.0
2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,732:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,739:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,740:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,741:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,741:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,742:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,742:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,742:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,743:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,743:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,744:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,744:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,744:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,744:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,744:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,747:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,748:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,753:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,759:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,759:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,760:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,760:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,760:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,762:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,762:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,763:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,763:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5445: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,764:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components


2024-02-04 19:23:13,764:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`optional` parameter is deprecated, and it has no effect


2024-02-04 19:23:13,764:WARNING:c:\Users\joseg\AppData\Local\Programs\Python\Python39\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:5447: GradioDeprecationWarning:

`numeric` parameter is deprecated, and it has no effect


2024-02-04 19:23:28,127:INFO:Soft dependency imported: fastapi: 0.109.1
2024-02-04 19:23:28,127:INFO:Soft dependency imported: uvicorn: 0.27.0.post1
2024-02-04 19:23:28,132:INFO:Soft dependency imported: pydantic: 1.10.12
2024-02-04 19:23:28,150:INFO:Initializing save_model()
2024-02-04 19:23:28,150:INFO:save_model(model=GradientBoostingRegressor(random_state=123), model_name=housing_price_api, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\joseg\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFin...
                                             'BldgType', 'HouseStyle',
                                             'RoofStyle', 'RoofMatl',
                                             'MasVnrType', 'Foundation',
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True)))]), verbose=False, use_case=MLUsecase.REGRESSION, kwargs={})
2024-02-04 19:23:28,150:INFO:Adding model into prep_pipe
2024-02-04 19:23:28,165:INFO:housing_price_api.pkl saved in current working directory
2024-02-04 19:23:28,190:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['LotFrontage', 'LotArea',
                                             'LandSlope', 'OverallQual',
                                             'OverallCond', 'YearBuilt',
                                             'YearRemodAdd', 'MasVnrArea',
                                             'ExterQual', 'ExterCond',
                                             'BsmtQual', 'BsmtCond',
                                             'BsmtExposure', 'BsmtFinType1',
                                             'BsmtFinSF1', 'BsmtFinType2',
                                             'BsmtFinSF2', 'BsmtUnfSF',
                                             'TotalBsmtSF', 'HeatingQ...
                                             'Heating', 'GarageType',
                                             'MiscFeature', 'MoSold'],
                                    transformer=OneHotEncoder(cols=['MSSubClass',
                                                                    'MSZoning',
                                                                    'LandContour',
                                                                    'LotConfig',
                                                                    'BldgType',
                                                                    'HouseStyle',
                                                                    'RoofStyle',
                                                                    'RoofMatl',
                                                                    'MasVnrType',
                                                                    'Foundation',
                                                                    'Heating',
                                                                    'GarageType',
                                                                    'MiscFeature',
                                                                    'MoSold'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('trained_model', GradientBoostingRegressor(random_state=123))])
2024-02-04 19:23:28,190:INFO:save_model() successfully completed......................................
2024-02-04 19:24:06,592:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:24:06,592:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:24:06,592:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:24:06,592:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:24:06,777:INFO:Initializing load_model()
2024-02-04 19:24:06,777:INFO:load_model(model_name=housing_price_api, platform=None, authentication=None, verbose=True)
2024-02-04 19:25:16,918:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:25:16,918:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:25:16,918:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:25:16,918:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-02-04 19:25:17,103:INFO:Initializing load_model()
2024-02-04 19:25:17,103:INFO:load_model(model_name=housing_price_api, platform=None, authentication=None, verbose=True)
